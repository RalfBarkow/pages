{
  "title": "Hidden Risks of Counting 9s",
  "story": [
    {
      "id": "656e4000",
      "type": "paragraph",
      "text": "A counterintuitive experience report from measuring availability."
    },
    {
      "id": "53d72000",
      "type": "paragraph",
      "text": "I joined a team at an internet scale company whose job was to manage an incident chat bot and related incident database. The bot would record the start time and end time of any incident along with any time severity changed over the duration of the incident."
    },
    {
      "id": "110cb000",
      "type": "paragraph",
      "text": "After an incident, teams were expected to estimate the customer impact for each of the major products and for each of the severity timespans over the duration of the incident."
    },
    {
      "id": "ab8f800",
      "type": "paragraph",
      "text": "One of the expected outcomes from an incident retro was to identify which team owned the impact for the incident."
    },
    {
      "id": "e69c000",
      "type": "paragraph",
      "text": "From this data, we would generate reports of availability expressed as number of nines, adjusted by the percentage of customer impact. So 99.5, 99.7, 99.8, whatever was happening for a specific group."
    },
    {
      "id": "4a418000",
      "type": "paragraph",
      "text": "These were broken down by both team and product and grouped over the past three months, alongside the past 30 day rolling window. The cells were colored green, yellow, or red according to team-specific or product-specific objectives for availability. Reports were delivered in a weekly email to pretty well everybody in the engineering organization. "
    },
    {
      "id": "796af000",
      "type": "paragraph",
      "text": "This is the best calculator of availability that I’ve seen in any of my own experience and better than most that I’ve even heard of from comparing notes with other engineers. It’s the kind of tool I think most companies think they want."
    },
    {
      "id": "6f6f0400",
      "type": "paragraph",
      "text": "But I’m telling you this story in the month of October as a horror story and a cautionary tale."
    },
    {
      "id": "18a7e000",
      "type": "paragraph",
      "text": "One example of an unexpected outcome of having this system is that the longer incident ran, the more difficult was the data entry. Long running incidents tended to include a number of ups and downs in customer impact or places where symptoms would cascade from one product area to another. Where those cascades started or ended were hard to identify and didn’t correlate cleanly with the changes in severity."
    },
    {
      "id": "5f5d6000",
      "type": "paragraph",
      "text": "The more complex incidents would involve many teams and impact many products. So retros would often fixate on who owned the impact or reassessing the customer impact instead of discovering mechanisms of failure, communication breakdowns, or the kinds of things that would actually improve our incident response. Instead, we were fighting over who had to own it."
    },
    {
      "id": "60da3000",
      "type": "paragraph",
      "text": "Probably the most popular feature request that we got on the team was to allow incidents to share ownership between the teams involved. This was also the hardest thing for us to implement based on the database schema we had and would’ve doubled the complexity of an already difficult and costly data entry in the first place."
    },
    {
      "id": "2c8a6000",
      "type": "paragraph",
      "text": "So time passed. We had a pretty rough couple of months over one August and September. Here it is an anniversary. It was in October when leadership decided to implement a new policy: a kind of targeted code freeze."
    },
    {
      "id": "5d266000",
      "type": "paragraph",
      "text": "If teams entered the red, they were expected to stop feature development, develop a plan that focused on reliability engineering. The plan had to be signed off by their VP and would include specific exit criteria that would enable them to resume work on their existing roadmap."
    },
    {
      "id": "53c3a000",
      "type": "paragraph",
      "text": "As teams encountered the new policy, it became universally hated. This memory is particularly acute for me because not long after the venom started flowing, I had written an impassioned defense of the new process. Teams have an accumulation of technical debt. We know there are areas that get neglected. And the purpose of the policy was to create organizational cover, to buy time for teams to be able to invest in the kinds of cleanup that we know were neglected."
    },
    {
      "id": "3194000",
      "type": "paragraph",
      "text": "What I learned in the backlash that ensued from my blog post is that leadership were not universally aligned on the new policy. It turned out that the organizational pressures to make our deadlines to keep to our roadmaps was much higher than the pressure to preserve reliability of the features under development."
    },
    {
      "id": "24b30000",
      "type": "paragraph",
      "text": "So few leaders would adjust their schedule when they entered the code freeze. Most kept to their expected deadlines."
    },
    {
      "id": "1ba78000",
      "type": "paragraph",
      "text": "The result was perhaps the worst of policy outcomes. Teams who were already most exhausted from recent incidents were now getting double the demands of their time. Instead of us creating cover, the policy was doubling the workload on the teams already collapsed from overload."
    },
    {
      "id": "52e91000",
      "type": "paragraph",
      "text": "I further learned that the report itself had a subtle effect of shaming teams by publicly drawing attention to their team in red. This had the effect of suppressing the reported severity of incidents. Low severity incidents could skip the extra data entry and accounting visibility."
    },
    {
      "id": "239aa000",
      "type": "paragraph",
      "text": "These features had the combined effect of converting the reliability work into a kind of punishment."
    },
    {
      "id": "6b479000",
      "type": "paragraph",
      "text": "But there’s more. As I looked more closely at the data that was in our database relative to the incidents that I witnessed, I recognized that every piece of data we had was being negotiated during the incidents."
    },
    {
      "id": "c6cc000",
      "type": "paragraph",
      "text": "They weren’t crisp measurable points. They were all judgment calls. Every one of them."
    },
    {
      "id": "60ae2000",
      "type": "paragraph",
      "text": "What’s more, There were existing company processes related to customer root cause analysis documents our team was involved in that further negotiated the customer impact reported to customers. When a customer would demand a report, say after a bad month, our job was to identify the incidents over the span of that report that would have affected the customer based on what we had, for which products were affected, and which products that customer was paying for."
    },
    {
      "id": "25dd3800",
      "type": "paragraph",
      "text": "So a great deal of effort was spent on our part to clean the data and double-check with teams who had maybe not finished their data entry on the customer impact to ensure that that customer’s impact based on the incidents over the period was focused on only those things that could have affected them."
    },
    {
      "id": "850000",
      "type": "paragraph",
      "text": "And I don’t want to suggest that the work we were doing for the RCAs was in any way deceptive. I think it was appropriate. But what I do want to make clear is that it was very expensive."
    },
    {
      "id": "2de0f000",
      "type": "paragraph",
      "text": "Only my teammates and I could actually see how much it was costing the company to collect the data. It was spread thinly across every single team, hidden in ordinary day-to-day work. The resulting numbers were based on sloppy and hasty estimates and judgments. The numbers didn’t actually help teams prioritize paying down their technical debt or other efforts to improve reliability."
    },
    {
      "id": "1a522000",
      "type": "paragraph",
      "text": "The costs to morale across the company were substantial. And all of it further undermined the quality of what the company learned from incidents cuz we were too busy fighting over who had the impact."
    },
    {
      "id": "5ff31000",
      "type": "paragraph",
      "text": "The last thing I wanna leave you with is what to choose instead. We were migrating away from of these availability numbers and recommending teams develop more focused service level indicators and objectives. Now, SLOs are not a silver bullet either. What’s really going to change your reliability is when teams build the muscles to actually balance the time you spend on reliability versus features."
    },
    {
      "id": "18277800",
      "type": "paragraph",
      "text": "Reminiscing with co-workers uncovered these experiences, also confirmed by a few of them:"
    },
    {
      "id": "2931f000",
      "type": "paragraph",
      "text": "One thing that I witnessed during this time frame was managers wrangling with each other over who would “own” the incident and be forced into [the code freeze]. Rather than doing what was best globally, they were both trying to optimize locally for their team. And, it led to misleading ownership that was assigned not for good reason, but so that managers could save their own SLAs and push things on to other teams who hadn’t used up their budgets yet. So, in essence, the game became “how to not be forced into [code freeze]” rather than “how to most effectively fix our overall system.”"
    },
    {
      "id": "64cd4000",
      "type": "paragraph",
      "text": "I remember feeling pretty defensive (which is, like, the least useful emotion to have ever) and yes, it became more about “getting my team out of [code freeze]” in addition to fixing the underlying problems. Because it felt like the focus was more on “Here are the hoops the team needs to jump through to get out of [code freeze]” rather than (but, to be fair, in addition to) “here’s how we get better as a company”. We ... really didn’t need that split focus, IMO. We didn’t need hoops to jump through, or “reliability training wheels”. We had enough engineering excellence gravity that was already pulling us toward Doing the Right Thing. [Code freeze] was just noise on our end. Needless friction."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Hidden Risks of Counting 9s",
        "story": [
          {
            "id": "656e4000",
            "type": "paragraph",
            "text": "A counterintuitive experience report from measuring availability."
          },
          {
            "id": "53d72000",
            "type": "paragraph",
            "text": "I joined a team at an internet scale company whose job was to manage an incident chat bot and related incident database. The bot would record the start time and end time of any incident along with any time severity changed over the duration of the incident."
          },
          {
            "id": "110cb000",
            "type": "paragraph",
            "text": "After an incident, teams were expected to estimate the customer impact for each of the major products and for each of the severity timespans over the duration of the incident."
          },
          {
            "id": "ab8f800",
            "type": "paragraph",
            "text": "One of the expected outcomes from an incident retro was to identify which team owned the impact for the incident."
          },
          {
            "id": "e69c000",
            "type": "paragraph",
            "text": "From this data, we would generate reports of availability expressed as number of nines, adjusted by the percentage of customer impact. So 99.5, 99.7, 99.8, whatever was happening for a specific group."
          },
          {
            "id": "4a418000",
            "type": "paragraph",
            "text": "These were broken down by both team and product and grouped over the past three months, alongside the past 30 day rolling window. The cells were colored green, yellow, or red according to team-specific or product-specific objectives for availability. Reports were delivered in a weekly email to pretty well everybody in the engineering organization. "
          },
          {
            "id": "796af000",
            "type": "paragraph",
            "text": "This is the best calculator of availability that I’ve seen in any of my own experience and better than most that I’ve even heard of from comparing notes with other engineers. It’s the kind of tool I think most companies think they want."
          },
          {
            "id": "6f6f0400",
            "type": "paragraph",
            "text": "But I’m telling you this story in the month of October as a horror story and a cautionary tale."
          },
          {
            "id": "18a7e000",
            "type": "paragraph",
            "text": "One example of an unexpected outcome of having this system is that the longer incident ran, the more difficult was the data entry. Long running incidents tended to include a number of ups and downs in customer impact or places where symptoms would cascade from one product area to another. Where those cascades started or ended were hard to identify and didn’t correlate cleanly with the changes in severity."
          },
          {
            "id": "5f5d6000",
            "type": "paragraph",
            "text": "The more complex incidents would involve many teams and impact many products. So retros would often fixate on who owned the impact or reassessing the customer impact instead of discovering mechanisms of failure, communication breakdowns, or the kinds of things that would actually improve our incident response. Instead, we were fighting over who had to own it."
          },
          {
            "id": "60da3000",
            "type": "paragraph",
            "text": "Probably the most popular feature request that we got on the team was to allow incidents to share ownership between the teams involved. This was also the hardest thing for us to implement based on the database schema we had and would’ve doubled the complexity of an already difficult and costly data entry in the first place."
          },
          {
            "id": "2c8a6000",
            "type": "paragraph",
            "text": "So time passed. We had a pretty rough couple of months over one August and September. Here it is an anniversary. It was in October when leadership decided to implement a new policy: a kind of targeted code freeze."
          },
          {
            "id": "5d266000",
            "type": "paragraph",
            "text": "If teams entered the red, they were expected to stop feature development, develop a plan that focused on reliability engineering. The plan had to be signed off by their VP and would include specific exit criteria that would enable them to resume work on their existing roadmap."
          },
          {
            "id": "53c3a000",
            "type": "paragraph",
            "text": "As teams encountered the new policy, it became universally hated. This memory is particularly acute for me because not long after the venom started flowing, I had written an impassioned defense of the new process. Teams have an accumulation of technical debt. We know there are areas that get neglected. And the purpose of the policy was to create organizational cover, to buy time for teams to be able to invest in the kinds of cleanup that we know were neglected."
          },
          {
            "id": "3194000",
            "type": "paragraph",
            "text": "What I learned in the backlash that ensued from my blog post is that leadership were not universally aligned on the new policy. It turned out that the organizational pressures to make our deadlines to keep to our roadmaps was much higher than the pressure to preserve reliability of the features under development."
          },
          {
            "id": "24b30000",
            "type": "paragraph",
            "text": "So few leaders would adjust their schedule when they entered the code freeze. Most kept to their expected deadlines."
          },
          {
            "id": "1ba78000",
            "type": "paragraph",
            "text": "The result was perhaps the worst of policy outcomes. Teams who were already most exhausted from recent incidents were now getting double the demands of their time. Instead of us creating cover, the policy was doubling the workload on the teams already collapsed from overload."
          },
          {
            "id": "52e91000",
            "type": "paragraph",
            "text": "I further learned that the report itself had a subtle effect of shaming teams by publicly drawing attention to their team in red. This had the effect of suppressing the reported severity of incidents. Low severity incidents could skip the extra data entry and accounting visibility."
          },
          {
            "id": "239aa000",
            "type": "paragraph",
            "text": "These features had the combined effect of converting the reliability work into a kind of punishment."
          },
          {
            "id": "6b479000",
            "type": "paragraph",
            "text": "But there’s more. As I looked more closely at the data that was in our database relative to the incidents that I witnessed, I recognized that every piece of data we had was being negotiated during the incidents."
          },
          {
            "id": "c6cc000",
            "type": "paragraph",
            "text": "They weren’t crisp measurable points. They were all judgment calls. Every one of them."
          },
          {
            "id": "60ae2000",
            "type": "paragraph",
            "text": "What’s more, There were existing company processes related to customer root cause analysis documents our team was involved in that further negotiated the customer impact reported to customers. When a customer would demand a report, say after a bad month, our job was to identify the incidents over the span of that report that would have affected the customer based on what we had, for which products were affected, and which products that customer was paying for."
          },
          {
            "id": "25dd3800",
            "type": "paragraph",
            "text": "So a great deal of effort was spent on our part to clean the data and double-check with teams who had maybe not finished their data entry on the customer impact to ensure that that customer’s impact based on the incidents over the period was focused on only those things that could have affected them."
          },
          {
            "id": "850000",
            "type": "paragraph",
            "text": "And I don’t want to suggest that the work we were doing for the RCAs was in any way deceptive. I think it was appropriate. But what I do want to make clear is that it was very expensive."
          },
          {
            "id": "2de0f000",
            "type": "paragraph",
            "text": "Only my teammates and I could actually see how much it was costing the company to collect the data. It was spread thinly across every single team, hidden in ordinary day-to-day work. The resulting numbers were based on sloppy and hasty estimates and judgments. The numbers didn’t actually help teams prioritize paying down their technical debt or other efforts to improve reliability."
          },
          {
            "id": "1a522000",
            "type": "paragraph",
            "text": "The costs to morale across the company were substantial. And all of it further undermined the quality of what the company learned from incidents cuz we were too busy fighting over who had the impact."
          },
          {
            "id": "5ff31000",
            "type": "paragraph",
            "text": "The last thing I wanna leave you with is what to choose instead. We were migrating away from of these availability numbers and recommending teams develop more focused service level indicators and objectives. Now, SLOs are not a silver bullet either. What’s really going to change your reliability is when teams build the muscles to actually balance the time you spend on reliability versus features."
          },
          {
            "id": "18277800",
            "type": "paragraph",
            "text": "Reminiscing with co-workers uncovered these experiences, also confirmed by a few of them:"
          },
          {
            "id": "2931f000",
            "type": "paragraph",
            "text": "One thing that I witnessed during this time frame was managers wrangling with each other over who would “own” the incident and be forced into [the code freeze]. Rather than doing what was best globally, they were both trying to optimize locally for their team. And, it led to misleading ownership that was assigned not for good reason, but so that managers could save their own SLAs and push things on to other teams who hadn’t used up their budgets yet. So, in essence, the game became “how to not be forced into [code freeze]” rather than “how to most effectively fix our overall system.”"
          },
          {
            "id": "64cd4000",
            "type": "paragraph",
            "text": "I remember feeling pretty defensive (which is, like, the least useful emotion to have ever) and yes, it became more about “getting my team out of [code freeze]” in addition to fixing the underlying problems. Because it felt like the focus was more on “Here are the hoops the team needs to jump through to get out of [code freeze]” rather than (but, to be fair, in addition to) “here’s how we get better as a company”. We ... really didn’t need that split focus, IMO. We didn’t need hoops to jump through, or “reliability training wheels”. We had enough engineering excellence gravity that was already pulling us toward Doing the Right Thing. [Code freeze] was just noise on our end. Needless friction."
          }
        ]
      },
      "date": 1665173245921
    },
    {
      "type": "fork",
      "date": 1665173284250
    },
    {
      "type": "fork",
      "site": "lfi.wiki.dbbs.co",
      "date": 1665226581191
    }
  ]
}