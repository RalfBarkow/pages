{
  "title": "Denoising Autoencoder Self-Organizing Map",
  "story": [
    {
      "type": "paragraph",
      "id": "ce834b0cfc5c13d6",
      "text": "In this report, we address the question of combining nonlinearities of neurons into networks for modeling increasingly varying and progressively more complex functions. "
    },
    {
      "type": "paragraph",
      "id": "73c9b3809f6b8a06",
      "text": "A fundamental approach is the use of higher-level representations devised by restricted Boltzmann machines and (denoising) autoencoders. "
    },
    {
      "type": "paragraph",
      "id": "050ffb5c7a22f1b0",
      "text": "We present the Denoising Autoencoder Self-Organizing Map (DASOM) that integrates the latter into a hierarchically organized hybrid model where the front-end component is a grid of topologically ordered neurons. The approach is to interpose a layer of hidden representations between the [[Input Space]] and the neural lattice of the self-organizing map. In so doing the parameters are adjusted by the proposed unsupervised learning algorithm. The model therefore maintains the clustering properties of its predecessor, whereas by extending and enhancing its visualization capacity enables an inclusion and an analysis of the intermediate representation space. A comprehensive series of experiments comprising optical recognition of text and images, and cancer type clustering and categorization is used to demonstrate DASOM’s efficiency, performance and projection capabilities."
    },
    {
      "type": "pagefold",
      "id": "5e4462bc365dc730",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "e31f917275b3cbb7",
      "text": "\nFERLES, Christos, PAPANIKOLAOU, Yannis and NAIDOO, Kevin J., 2018. Denoising Autoencoder Self-Organizing Map (DASOM). Neural Networks. 1 September 2018. Vol. 105, p. 112–131. DOI 10.1016/j.neunet.2018.04.016. \n"
    },
    {
      "type": "paragraph",
      "id": "91f1bb56e352aea6",
      "text": "The structural and functional nature of biological neural systems provides a developmental template for artificial neural network algorithms. Structurally networks of topographically ordered neurons mimic nerve nets of the [[Visual Cortex]] (Von der Malsburg, 1973) and deep models (H. Lee, Ekanadham, & Ng, 2008) capture and store higher representations extracted from inherent regularities and structure in data (Ito & Komatsu, 2004). "
    },
    {
      "type": "paragraph",
      "id": "a0131ec6d8957110",
      "text": "Functionally layered hierarchical architectures imitate sensory signal propagation and preprocessing/processing in regions of the brain (T. S. Lee & Mumford, 2003; T. S. Lee, Mumford, Romero, & Lamme, 1998). "
    },
    {
      "type": "paragraph",
      "id": "ac8faa0f27b7b0ef",
      "text": "Such findings and advances in neuroscience have fueled numerous aspects of machine learning research, and have paved the way for designing distributed information representation-processing models based on complex layered architectures."
    },
    {
      "type": "paragraph",
      "id": "73ff9ddbbd0260bb",
      "text": "The extensively studied and widely applied clustering paradigm (Jain, 2010; Jain, Murty, & Flynn, 1999; Xu & Wunsch, 2005) forms a substantial part of the unsupervised learning backbone. Cluster analysis lends itself to varying approaches where principally the theme of grouping (separating) data elements based on their similarity/closeness (difference/distance) is common to much of these unsupervised classification algorithms. In a clustering method, partitioning the data into subsets subject to the criteria that the intra-cluster's internal homogeneity is greater than the inter-cluster's external heterogeneity, is the norm. Despite these fundamental commonalities there remains a diverse range of clustering approaches, for addressing generic problems that are enriched by a comprehensive array of problem-specific clustering algorithms."
    },
    {
      "type": "paragraph",
      "id": "72070bbac9a3e866",
      "text": "The [[Self-Organizing Map]] (SOM) (Kohonen, 2001, 2013) is a specific type of unsupervised learning algorithm that represents the distribution-characteristics of input samples on planes of topographically ordered nodes and in doing so achieves clustering through dimensionality reduction. The intrinsic nonlinear mapping capabilities of the SOM on its low-dimensional neural surface distinguish it from most of the techniques that fall within the wider class of clustering algorithms. This advantage over more typical clustering approaches has led to the SOM methodology being as a means of visualizing nonlinear relations of data, topology-based cluster analysis, [[Vector Quantization]] and projection of multi-dimensional data. Because of the versatility of the SOM the scope of applications to which it has been applied is vast ranging from pattern recognition, image-text processing, mining, genomics, medical diagnostics, robotics and economics."
    },
    {
      "type": "paragraph",
      "id": "b6664ad22bf74dfd",
      "text": "A difficulty that arises during pattern recognition from unlabeled data is the presence of large numbers of elements that make no contribution, or contribute marginally, to the data representation (features) of the dataset. The performance of machine learning methods therefore depends on the choice of the data features on which they are applied. "
    },
    {
      "type": "paragraph",
      "id": "1b535d4d5e561d89",
      "text": "The introduction of the autoencoder has been a major innovation in unsupervised learning where through backpropagation key features in the data are discovered. The learning structure is borrowed from the neurological process underpinning global learning and intelligent behavior in hominids. This is where biochemical events respond to input data and induce synaptic changes that are, through self-organization, coordinated to learn from the input data. Biologically this defines what we consider to be perception and ingenuity. "
    },
    {
      "type": "paragraph",
      "id": "c8bacbc231918e12",
      "text": "Autoencoder algorithms mimic this biology by transforming an input vector into a hidden representation (the [[Encoder]]) using a deterministic function that is parameterized by a [[Weighting Matrix]] and a bias vector. Through a placement of constraints on the network, such as limiting the number of hidden units or deactivating certain of its parts, structure in the data that is of interest can be discovered. The hidden representation is then mapped back to a reconstruction vector that has the same deterministic functional form as before and so comprises a counterpart to its own weighting matrix and bias vector (the [[Decoder]]). The parameters are optimized by minimizing the (average) reconstruction error."
    },
    {
      "type": "pagefold",
      "id": "46379d7a097d9ce6",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "ce1bb4fb3e818b33",
      "text": "ELM JSON encoder/decoder"
    },
    {
      "type": "pagefold",
      "id": "616b430c91713824",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "0d99f0720fe9dff7",
      "text": "During the past decade, deep learning (Bengio, 2009; Hinton, Osindero, & Teh, 2006; Schmidhuber, 2015) has been at the forefront of the development of methods that shift the focus from feature engineering by experts to meaningful representation discovery by algorithms. "
    },
    {
      "type": "paragraph",
      "id": "b687bbcb2b2fcb67",
      "text": "The discovered distributed layered representations, which build upon lower-level invariant partial features, reveal higher-level abstract concepts and aspects of the data. The induced responses, from discovered correlations within data, depend on the connectivity of the neurons. In an algorithm this is implemented as multiple sequential causative compute events wherein each event transforms (often in a nonlinear way) the aggregate response of the network (Schmidhuber, 2015). Deep learning within this context refers to the accurate adjustment of parameters (weights and bias vectors) across such events."
    },
    {
      "type": "paragraph",
      "id": "6f6251f3441cf3b4",
      "text": "Denoising Autoencoders (DAs) (Bengio, 2009; Vincent, Larochelle, Bengio, & Manzagol, 2008; Vincent, Larochelle, Lajoie, Bengio, & Manzagol, 2010) introduce an alternative to deep belief networks by incorporating stochastic corruption during the training process. The premise of introducing the denoising functions is that a good representation is impervious to disruptions of and perturbations to the input data. At the same time these types of representations capture the characteristic aspects and correlations that underpin the distribution of the input data. A procedure of cancelling out the artificial corruption leads to devised representations that are closer to the stable structures and invariant features present in the input data space."
    },
    {
      "type": "paragraph",
      "id": "8c10b05b4009d6ed",
      "text": "Supervised learning tasks, i.e. regression and classification, are the primary objectives of deep learning methodologies. However, a modular approach where unsupervised pre-training replaces the usual initialization schemes during the development of (semi)supervised deep learning algorithms has been shown to be effective and is now the standard approach in many developments (Bengio, Lamblin, Popovici, & Larochelle, 2007; Erhan, et al., 2010; Erhan, Manzagol, Bengio, Bengio, & Vincent, 2009; Hinton, et al., 2006). "
    },
    {
      "type": "paragraph",
      "id": "287fd434b15caba0",
      "text": "In the deep learning paradigm, the unsupervised process contributes to the tuning of each layer. This intervention at each layer adds up to a cascade yielding an improved abstraction of higher-level distributed representations as progress is made bottom-up along the layered hierarchy. In this implementation, unsupervised learning effectively reduces the redundancy by progressively gathering and consolidating the information existent in the original data."
    },
    {
      "type": "paragraph",
      "id": "d907322059955412",
      "text": "While drawing inspiration from deep learning here we propose a model that deviates from the above principle of layer by layer contribution, instead we hybridize a pure unsupervised learning algorithm (viz. the SOM) with the unsupervised learning module of a deep learning architecture (viz. the DA). The Denoising Autoencoder Self-Organizing Map (DASOM) structurally combines, architecturally stacks and algorithmically fuses its two unsupervised learning components. In the following section we present the basic framework and characteristics of the DASOM model that:"
    },
    {
      "type": "paragraph",
      "id": "fc3163543d8de053",
      "text": "1. Captures and projects the relations between higher representations extracted from input data, and simultaneously maps these onto two-dimensional manifolds (e.g. planes). Throughout the procedure, data clusters and their principal correlations are visually observed and analyzed."
    },
    {
      "type": "paragraph",
      "id": "2f2b6bcb2736aaa4",
      "text": "2. By effectively incorporating the use of component planes the influence and the importance of each nonlinear combination of input features are visualized on the neural map. Building upon a mechanism to gain insight into the produced representations, a feature selection scheme is realized based on the highest contributing attributes and inputs of specified representations."
    },
    {
      "type": "paragraph",
      "id": "7a0b6d255db4f6c7",
      "text": "The remainder of this paper is organized into five sections. Section 2 presents in detail the DASOM both architecturally and algorithmically, and subsequently, analyzes the corresponding learning phases and adaptation procedures. Section 3 contains qualitative and quantitative experimental results, performance evaluations and comparisons with different algorithms. A systematic evaluation of the data clustering, visualizations and low-dimensional projections is given in Section 4. In section 5 a summary is given and conclusions are drawn. We include an Appendix that details the analytical equations for the DASOM learning algorithm as well as the derivation thereof."
    },
    {
      "type": "pagefold",
      "id": "ab6aaaab23632dbf",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "1b082a25349b339c",
      "text": "Kohonen, T. (2001). [[Self-Organizing Maps]], vol. 30 of Springer Series in Information Sciences. ed:SpringerBerlin. "
    },
    {
      "type": "paragraph",
      "id": "0f50d518121a279c",
      "text": "Kohonen, T. (2013). [[Essentials of the Self-Organizing Map]]. NeuralNetworks,37, 52-65. "
    },
    {
      "type": "paragraph",
      "id": "1dda4e57fe58f6c0",
      "text": "Kohonen, T. (2014). MATLAB implementations and applications of the self-organizing map. UnigrafiaOy,Helsinki,Finland, 11-23."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Denoising Autoencoder Self-Organizing Map",
        "story": []
      },
      "date": 1674575371319
    },
    {
      "id": "ce834b0cfc5c13d6",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "ce834b0cfc5c13d6",
        "text": "In this report, we address the question of combining nonlinearities of neurons into networks for modeling increasingly varying and progressively more complex functions. A fundamental approach is the use of higher-level representations devised by restricted Boltzmann machines and (denoising) autoencoders. We present the Denoising Autoencoder Self-Organizing Map (DASOM) that integrates the latter into a hierarchically organized hybrid model where the front-end component is a grid of topologically ordered neurons. The approach is to interpose a layer of hidden representations between the input space and the neural lattice of the self-organizing map. In so doing the parameters are adjusted by the proposed unsupervised learning algorithm. The model therefore maintains the clustering properties of its predecessor, whereas by extending and enhancing its visualization capacity enables an inclusion and an analysis of the intermediate representation space. A comprehensive series of experiments comprising optical recognition of text and images, and cancer type clustering and categorization is used to demonstrate DASOM’s efficiency, performance and projection capabilities."
      },
      "date": 1674575374921
    },
    {
      "item": {
        "type": "factory",
        "id": "5e4462bc365dc730"
      },
      "id": "5e4462bc365dc730",
      "type": "add",
      "after": "ce834b0cfc5c13d6",
      "date": 1674575376960
    },
    {
      "type": "edit",
      "id": "5e4462bc365dc730",
      "item": {
        "type": "pagefold",
        "id": "5e4462bc365dc730",
        "text": "~"
      },
      "date": 1674575380530
    },
    {
      "item": {
        "type": "factory",
        "id": "e31f917275b3cbb7"
      },
      "id": "e31f917275b3cbb7",
      "type": "add",
      "after": "5e4462bc365dc730",
      "date": 1674575389431
    },
    {
      "type": "edit",
      "id": "e31f917275b3cbb7",
      "item": {
        "type": "paragraph",
        "id": "e31f917275b3cbb7",
        "text": "\nFERLES, Christos, PAPANIKOLAOU, Yannis and NAIDOO, Kevin J., 2018. Denoising Autoencoder Self-Organizing Map (DASOM). Neural Networks. 1 September 2018. Vol. 105, p. 112–131. DOI 10.1016/j.neunet.2018.04.016. \n"
      },
      "date": 1674575402090
    },
    {
      "type": "edit",
      "id": "ce834b0cfc5c13d6",
      "item": {
        "type": "paragraph",
        "id": "ce834b0cfc5c13d6",
        "text": "In this report, we address the question of combining nonlinearities of neurons into networks for modeling increasingly varying and progressively more complex functions. "
      },
      "date": 1674575416292
    },
    {
      "type": "add",
      "id": "73c9b3809f6b8a06",
      "item": {
        "type": "paragraph",
        "id": "73c9b3809f6b8a06",
        "text": "A fundamental approach is the use of higher-level representations devised by restricted Boltzmann machines and (denoising) autoencoders. "
      },
      "after": "ce834b0cfc5c13d6",
      "date": 1674575418796
    },
    {
      "type": "add",
      "id": "050ffb5c7a22f1b0",
      "item": {
        "type": "paragraph",
        "id": "050ffb5c7a22f1b0",
        "text": "We present the Denoising Autoencoder Self-Organizing Map (DASOM) that integrates the latter into a hierarchically organized hybrid model where the front-end component is a grid of topologically ordered neurons. The approach is to interpose a layer of hidden representations between the input space and the neural lattice of the self-organizing map. In so doing the parameters are adjusted by the proposed unsupervised learning algorithm. The model therefore maintains the clustering properties of its predecessor, whereas by extending and enhancing its visualization capacity enables an inclusion and an analysis of the intermediate representation space. A comprehensive series of experiments comprising optical recognition of text and images, and cancer type clustering and categorization is used to demonstrate DASOM’s efficiency, performance and projection capabilities."
      },
      "after": "73c9b3809f6b8a06",
      "date": 1674575419224
    },
    {
      "type": "edit",
      "id": "050ffb5c7a22f1b0",
      "item": {
        "type": "paragraph",
        "id": "050ffb5c7a22f1b0",
        "text": "We present the Denoising Autoencoder Self-Organizing Map (DASOM) that integrates the latter into a hierarchically organized hybrid model where the front-end component is a grid of topologically ordered neurons. The approach is to interpose a layer of hidden representations between the [[Input Space]] and the neural lattice of the self-organizing map. In so doing the parameters are adjusted by the proposed unsupervised learning algorithm. The model therefore maintains the clustering properties of its predecessor, whereas by extending and enhancing its visualization capacity enables an inclusion and an analysis of the intermediate representation space. A comprehensive series of experiments comprising optical recognition of text and images, and cancer type clustering and categorization is used to demonstrate DASOM’s efficiency, performance and projection capabilities."
      },
      "date": 1674575461130
    },
    {
      "item": {
        "type": "factory",
        "id": "91f1bb56e352aea6"
      },
      "id": "91f1bb56e352aea6",
      "type": "add",
      "after": "e31f917275b3cbb7",
      "date": 1674575534191
    },
    {
      "type": "edit",
      "id": "91f1bb56e352aea6",
      "item": {
        "type": "paragraph",
        "id": "91f1bb56e352aea6",
        "text": "The structural and functional nature of biological neural systems provides a developmental template for artificial neural network algorithms. Structurally networks of topographically ordered neurons mimic nerve nets of the visual cortex (Von der Malsburg, 1973) and deep models (H. Lee, Ekanadham, & Ng, 2008) capture and store higher representations extracted from inherent regularities and structure in data (Ito & Komatsu, 2004). Functionally layered hierarchical architectures imitate sensory signal propagation and preprocessing/processing in regions of the brain (T. S. Lee & Mumford, 2003; T. S. Lee, Mumford, Romero, & Lamme, 1998). Such findings and advances in neuroscience have fueled numerous aspects of machine learning research, and have paved the way for designing distributed information representationprocessing models based on complex layered architectures."
      },
      "date": 1674575535794
    },
    {
      "type": "edit",
      "id": "91f1bb56e352aea6",
      "item": {
        "type": "paragraph",
        "id": "91f1bb56e352aea6",
        "text": "The structural and functional nature of biological neural systems provides a developmental template for artificial neural network algorithms. Structurally networks of topographically ordered neurons mimic nerve nets of the visual cortex (Von der Malsburg, 1973) and deep models (H. Lee, Ekanadham, & Ng, 2008) capture and store higher representations extracted from inherent regularities and structure in data (Ito & Komatsu, 2004). "
      },
      "date": 1674575561322
    },
    {
      "type": "add",
      "id": "a0131ec6d8957110",
      "item": {
        "type": "paragraph",
        "id": "a0131ec6d8957110",
        "text": "Functionally layered hierarchical architectures imitate sensory signal propagation and preprocessing/processing in regions of the brain (T. S. Lee & Mumford, 2003; T. S. Lee, Mumford, Romero, & Lamme, 1998). Such findings and advances in neuroscience have fueled numerous aspects of machine learning research, and have paved the way for designing distributed information representationprocessing models based on complex layered architectures."
      },
      "after": "91f1bb56e352aea6",
      "date": 1674575562759
    },
    {
      "type": "edit",
      "id": "a0131ec6d8957110",
      "item": {
        "type": "paragraph",
        "id": "a0131ec6d8957110",
        "text": "Functionally layered hierarchical architectures imitate sensory signal propagation and preprocessing/processing in regions of the brain (T. S. Lee & Mumford, 2003; T. S. Lee, Mumford, Romero, & Lamme, 1998). "
      },
      "date": 1674575588398
    },
    {
      "type": "add",
      "id": "ac8faa0f27b7b0ef",
      "item": {
        "type": "paragraph",
        "id": "ac8faa0f27b7b0ef",
        "text": "Such findings and advances in neuroscience have fueled numerous aspects of machine learning research, and have paved the way for designing distributed information representationprocessing models based on complex layered architectures."
      },
      "after": "a0131ec6d8957110",
      "date": 1674575589174
    },
    {
      "type": "edit",
      "id": "ac8faa0f27b7b0ef",
      "item": {
        "type": "paragraph",
        "id": "ac8faa0f27b7b0ef",
        "text": "Such findings and advances in neuroscience have fueled numerous aspects of machine learning research, and have paved the way for designing distributed information representation-processing models based on complex layered architectures."
      },
      "date": 1674575603476
    },
    {
      "item": {
        "type": "factory",
        "id": "3f7883aa820f5d2c"
      },
      "id": "3f7883aa820f5d2c",
      "type": "add",
      "after": "ac8faa0f27b7b0ef",
      "date": 1674575616176
    },
    {
      "type": "remove",
      "id": "3f7883aa820f5d2c",
      "date": 1674575624799
    },
    {
      "item": {
        "type": "factory",
        "id": "73ff9ddbbd0260bb"
      },
      "id": "73ff9ddbbd0260bb",
      "type": "add",
      "after": "ac8faa0f27b7b0ef",
      "date": 1674575644524
    },
    {
      "type": "edit",
      "id": "73ff9ddbbd0260bb",
      "item": {
        "type": "paragraph",
        "id": "73ff9ddbbd0260bb",
        "text": "The extensively studied and widely applied clustering paradigm (Jain, 2010; Jain, Murty, & Flynn, 1999; Xu & Wunsch, 2005) forms a substantial part of the unsupervised learning backbone. Cluster analysis lends itself to varying approaches where principally the theme of grouping (separating) data elements based on their similarity/closeness (difference/distance) is common to much of these unsupervised classification algorithms. In a clustering method, partitioning the data into subsets subject to the criteria that the intra-\n\n, is the norm. Despite these fundamental commonalities there remains a diverse range of clustering approaches, for addressing generic problems that are enriched by a comprehensive array of problem-specific clustering algorithms."
      },
      "date": 1674575647832
    },
    {
      "type": "edit",
      "id": "73ff9ddbbd0260bb",
      "item": {
        "type": "paragraph",
        "id": "73ff9ddbbd0260bb",
        "text": "The extensively studied and widely applied clustering paradigm (Jain, 2010; Jain, Murty, & Flynn, 1999; Xu & Wunsch, 2005) forms a substantial part of the unsupervised learning backbone. Cluster analysis lends itself to varying approaches where principally the theme of grouping (separating) data elements based on their similarity/closeness (difference/distance) is common to much of these unsupervised classification algorithms. In a clustering method, partitioning the data into subsets subject to the criteria that the intra-cluster's internal homogeneity is greater than the inter-cluster's external heterogeneity, is the norm. Despite these fundamental commonalities there remains a diverse range of clustering approaches, for addressing generic problems that are enriched by a comprehensive array of problem-specific clustering algorithms."
      },
      "date": 1674575749997
    },
    {
      "item": {
        "type": "factory",
        "id": "72070bbac9a3e866"
      },
      "id": "72070bbac9a3e866",
      "type": "add",
      "after": "73ff9ddbbd0260bb",
      "date": 1674575811863
    },
    {
      "type": "edit",
      "id": "72070bbac9a3e866",
      "item": {
        "type": "paragraph",
        "id": "72070bbac9a3e866",
        "text": "The Self-Organizing Map (SOM) (Kohonen, 2001, 2013) is a specific type of unsupervised learning algorithm that represents the distribution-characteristics of input samples on planes of topographically ordered nodes and in doing so achieves clustering through dimensionality reduction. The intrinsic nonlinear mapping capabilities of the SOM on its low-dimensional neural surface distinguish it from most of the techniques that fall within the wider class of clustering algorithms. This advantage over more typical clustering approaches has led to the SOM methodology being as a means of visualizing nonlinear relations of data, topology-based cluster analysis, vector quantization and projection of multi-dimensional data. Because of the versatility of the SOM the scope of applications to which it has been applied is vast ranging from pattern recognition, image-text processing, mining, genomics, medical diagnostics, robotics and economics."
      },
      "date": 1674575814412
    },
    {
      "type": "edit",
      "id": "72070bbac9a3e866",
      "item": {
        "type": "paragraph",
        "id": "72070bbac9a3e866",
        "text": "The [[Self-Organizing Map]] (SOM) (Kohonen, 2001, 2013) is a specific type of unsupervised learning algorithm that represents the distribution-characteristics of input samples on planes of topographically ordered nodes and in doing so achieves clustering through dimensionality reduction. The intrinsic nonlinear mapping capabilities of the SOM on its low-dimensional neural surface distinguish it from most of the techniques that fall within the wider class of clustering algorithms. This advantage over more typical clustering approaches has led to the SOM methodology being as a means of visualizing nonlinear relations of data, topology-based cluster analysis, vector quantization and projection of multi-dimensional data. Because of the versatility of the SOM the scope of applications to which it has been applied is vast ranging from pattern recognition, image-text processing, mining, genomics, medical diagnostics, robotics and economics."
      },
      "date": 1674575824813
    },
    {
      "type": "edit",
      "id": "72070bbac9a3e866",
      "item": {
        "type": "paragraph",
        "id": "72070bbac9a3e866",
        "text": "The [[Self-Organizing Map]] (SOM) (Kohonen, 2001, 2013) is a specific type of unsupervised learning algorithm that represents the distribution-characteristics of input samples on planes of topographically ordered nodes and in doing so achieves clustering through dimensionality reduction. The intrinsic nonlinear mapping capabilities of the SOM on its low-dimensional neural surface distinguish it from most of the techniques that fall within the wider class of clustering algorithms. This advantage over more typical clustering approaches has led to the SOM methodology being as a means of visualizing nonlinear relations of data, topology-based cluster analysis, [[Vector Quantization]] and projection of multi-dimensional data. Because of the versatility of the SOM the scope of applications to which it has been applied is vast ranging from pattern recognition, image-text processing, mining, genomics, medical diagnostics, robotics and economics."
      },
      "date": 1674575890580
    },
    {
      "item": {
        "type": "factory",
        "id": "b6664ad22bf74dfd"
      },
      "id": "b6664ad22bf74dfd",
      "type": "add",
      "after": "72070bbac9a3e866",
      "date": 1674575933731
    },
    {
      "type": "edit",
      "id": "b6664ad22bf74dfd",
      "item": {
        "type": "paragraph",
        "id": "b6664ad22bf74dfd",
        "text": "A difficulty that arises during pattern recognition from unlabeled data is the presence of large numbers of elements that make no contribution, or contribute marginally, to the data representation (features) of the dataset. The performance of machine learning methods therefore depends on the choice of the data features on which they are applied. The introduction of the autoencoder has been a major innovation in unsupervised learning where through backpropagation key features in the data are discovered. The learning structure is borrowed from the neurological process underpinning global learning and intelligent behavior in hominids. This is where biochemical events respond to input data and induce synaptic changes that are, through self-organization, coordinated to learn from the input data. Biologically this defines what we consider to be perception and ingenuity. Autoencoder algorithms mimic this biology by transforming an input vector into a hidden representation (the encoder) using a deterministic function that is parameterized by a weighting matrix and a bias vector. Through a placement of constraints on the network, such as limiting the number of hidden units or deactivating certain of its parts, structure in the data that is of interest can be discovered. The hidden representation is then mapped back to a reconstruction vector that has the same deterministic functional form as before and so comprises a counterpart to its own weighting matrix and bias vector (the decoder). The parameters are optimized by minimizing the (average) reconstruction error."
      },
      "date": 1674575935407
    },
    {
      "type": "edit",
      "id": "b6664ad22bf74dfd",
      "item": {
        "type": "paragraph",
        "id": "b6664ad22bf74dfd",
        "text": "A difficulty that arises during pattern recognition from unlabeled data is the presence of large numbers of elements that make no contribution, or contribute marginally, to the data representation (features) of the dataset. The performance of machine learning methods therefore depends on the choice of the data features on which they are applied. "
      },
      "date": 1674575964475
    },
    {
      "type": "add",
      "id": "1b535d4d5e561d89",
      "item": {
        "type": "paragraph",
        "id": "1b535d4d5e561d89",
        "text": "The introduction of the autoencoder has been a major innovation in unsupervised learning where through backpropagation key features in the data are discovered. The learning structure is borrowed from the neurological process underpinning global learning and intelligent behavior in hominids. This is where biochemical events respond to input data and induce synaptic changes that are, through self-organization, coordinated to learn from the input data. Biologically this defines what we consider to be perception and ingenuity. Autoencoder algorithms mimic this biology by transforming an input vector into a hidden representation (the encoder) using a deterministic function that is parameterized by a weighting matrix and a bias vector. Through a placement of constraints on the network, such as limiting the number of hidden units or deactivating certain of its parts, structure in the data that is of interest can be discovered. The hidden representation is then mapped back to a reconstruction vector that has the same deterministic functional form as before and so comprises a counterpart to its own weighting matrix and bias vector (the decoder). The parameters are optimized by minimizing the (average) reconstruction error."
      },
      "after": "b6664ad22bf74dfd",
      "date": 1674575965094
    },
    {
      "type": "edit",
      "id": "1b535d4d5e561d89",
      "item": {
        "type": "paragraph",
        "id": "1b535d4d5e561d89",
        "text": "The introduction of the autoencoder has been a major innovation in unsupervised learning where through backpropagation key features in the data are discovered. The learning structure is borrowed from the neurological process underpinning global learning and intelligent behavior in hominids. This is where biochemical events respond to input data and induce synaptic changes that are, through self-organization, coordinated to learn from the input data. Biologically this defines what we consider to be perception and ingenuity. "
      },
      "date": 1674576019066
    },
    {
      "type": "add",
      "id": "c8bacbc231918e12",
      "item": {
        "type": "paragraph",
        "id": "c8bacbc231918e12",
        "text": "Autoencoder algorithms mimic this biology by transforming an input vector into a hidden representation (the encoder) using a deterministic function that is parameterized by a weighting matrix and a bias vector. Through a placement of constraints on the network, such as limiting the number of hidden units or deactivating certain of its parts, structure in the data that is of interest can be discovered. The hidden representation is then mapped back to a reconstruction vector that has the same deterministic functional form as before and so comprises a counterpart to its own weighting matrix and bias vector (the decoder). The parameters are optimized by minimizing the (average) reconstruction error."
      },
      "after": "1b535d4d5e561d89",
      "date": 1674576019878
    },
    {
      "item": {
        "type": "factory",
        "id": "46379d7a097d9ce6"
      },
      "id": "46379d7a097d9ce6",
      "type": "add",
      "after": "c8bacbc231918e12",
      "date": 1674576046957
    },
    {
      "type": "edit",
      "id": "46379d7a097d9ce6",
      "item": {
        "type": "pagefold",
        "id": "46379d7a097d9ce6",
        "text": "~"
      },
      "date": 1674576050363
    },
    {
      "item": {
        "type": "factory",
        "id": "ce1bb4fb3e818b33"
      },
      "id": "ce1bb4fb3e818b33",
      "type": "add",
      "after": "46379d7a097d9ce6",
      "date": 1674576052325
    },
    {
      "type": "edit",
      "id": "ce1bb4fb3e818b33",
      "item": {
        "type": "paragraph",
        "id": "ce1bb4fb3e818b33",
        "text": "ELM JSON encoder/decoder"
      },
      "date": 1674576062233
    },
    {
      "type": "edit",
      "id": "c8bacbc231918e12",
      "item": {
        "type": "paragraph",
        "id": "c8bacbc231918e12",
        "text": "Autoencoder algorithms mimic this biology by transforming an input vector into a hidden representation (the [[Encoder]]) using a deterministic function that is parameterized by a weighting matrix and a bias vector. Through a placement of constraints on the network, such as limiting the number of hidden units or deactivating certain of its parts, structure in the data that is of interest can be discovered. The hidden representation is then mapped back to a reconstruction vector that has the same deterministic functional form as before and so comprises a counterpart to its own weighting matrix and bias vector (the decoder). The parameters are optimized by minimizing the (average) reconstruction error."
      },
      "date": 1674576077149
    },
    {
      "type": "edit",
      "id": "c8bacbc231918e12",
      "item": {
        "type": "paragraph",
        "id": "c8bacbc231918e12",
        "text": "Autoencoder algorithms mimic this biology by transforming an input vector into a hidden representation (the [[Encoder]]) using a deterministic function that is parameterized by a [[Weighting Matrix]] and a bias vector. Through a placement of constraints on the network, such as limiting the number of hidden units or deactivating certain of its parts, structure in the data that is of interest can be discovered. The hidden representation is then mapped back to a reconstruction vector that has the same deterministic functional form as before and so comprises a counterpart to its own weighting matrix and bias vector (the decoder). The parameters are optimized by minimizing the (average) reconstruction error."
      },
      "date": 1674576116832
    },
    {
      "type": "edit",
      "id": "c8bacbc231918e12",
      "item": {
        "type": "paragraph",
        "id": "c8bacbc231918e12",
        "text": "Autoencoder algorithms mimic this biology by transforming an input vector into a hidden representation (the [[Encoder]]) using a deterministic function that is parameterized by a [[Weighting Matrix]] and a bias vector. Through a placement of constraints on the network, such as limiting the number of hidden units or deactivating certain of its parts, structure in the data that is of interest can be discovered. The hidden representation is then mapped back to a reconstruction vector that has the same deterministic functional form as before and so comprises a counterpart to its own weighting matrix and bias vector (the [[Decoder]]). The parameters are optimized by minimizing the (average) reconstruction error."
      },
      "date": 1674576155523
    },
    {
      "item": {
        "type": "factory",
        "id": "616b430c91713824"
      },
      "id": "616b430c91713824",
      "type": "add",
      "after": "ce1bb4fb3e818b33",
      "date": 1674576222010
    },
    {
      "type": "edit",
      "id": "616b430c91713824",
      "item": {
        "type": "pagefold",
        "id": "616b430c91713824",
        "text": "~"
      },
      "date": 1674576227392
    },
    {
      "item": {
        "type": "factory",
        "id": "0d99f0720fe9dff7"
      },
      "id": "0d99f0720fe9dff7",
      "type": "add",
      "after": "616b430c91713824",
      "date": 1674576229398
    },
    {
      "type": "edit",
      "id": "0d99f0720fe9dff7",
      "item": {
        "type": "paragraph",
        "id": "0d99f0720fe9dff7",
        "text": "During the past decade, deep learning (Bengio, 2009; Hinton, Osindero, & Teh, 2006; Schmidhuber, 2015) has been at the forefront of the development of methods that shift the focus from feature engineering by experts to meaningful representation discovery by algorithms. The discovered distributed layered representations, which build upon lower-level invariant partial features, reveal higher-level abstract concepts and aspects of the data. The induced responses, from discovered correlations within data, depend on the connectivity of the neurons. In an algorithm this is implemented as multiple sequential causative compute events wherein each event transforms (often in a nonlinear way) the aggregate response of the network (Schmidhuber, 2015). Deep learning within this context refers to the accurate adjustment of parameters (weights and bias vectors) across such events."
      },
      "date": 1674576236326
    },
    {
      "type": "edit",
      "id": "0d99f0720fe9dff7",
      "item": {
        "type": "paragraph",
        "id": "0d99f0720fe9dff7",
        "text": "During the past decade, deep learning (Bengio, 2009; Hinton, Osindero, & Teh, 2006; Schmidhuber, 2015) has been at the forefront of the development of methods that shift the focus from feature engineering by experts to meaningful representation discovery by algorithms. "
      },
      "date": 1674576259631
    },
    {
      "type": "add",
      "id": "b687bbcb2b2fcb67",
      "item": {
        "type": "paragraph",
        "id": "b687bbcb2b2fcb67",
        "text": "The discovered distributed layered representations, which build upon lower-level invariant partial features, reveal higher-level abstract concepts and aspects of the data. The induced responses, from discovered correlations within data, depend on the connectivity of the neurons. In an algorithm this is implemented as multiple sequential causative compute events wherein each event transforms (often in a nonlinear way) the aggregate response of the network (Schmidhuber, 2015). Deep learning within this context refers to the accurate adjustment of parameters (weights and bias vectors) across such events."
      },
      "after": "0d99f0720fe9dff7",
      "date": 1674576260267
    },
    {
      "item": {
        "type": "factory",
        "id": "6f6251f3441cf3b4"
      },
      "id": "6f6251f3441cf3b4",
      "type": "add",
      "after": "b687bbcb2b2fcb67",
      "date": 1674576303048
    },
    {
      "type": "edit",
      "id": "6f6251f3441cf3b4",
      "item": {
        "type": "paragraph",
        "id": "6f6251f3441cf3b4",
        "text": "Denoising Autoencoders (DAs) (Bengio, 2009; Vincent, Larochelle, Bengio, & Manzagol, 2008; Vincent, Larochelle, Lajoie, Bengio, & Manzagol, 2010) introduce an alternative to deep belief networks by incorporating stochastic corruption during the training process. The premise of introducing the denoising functions is that a good representation is impervious to disruptions of and perturbations to the input data. At the same time these types of representations capture the characteristic aspects and correlations that underpin the distribution of the input data. A procedure of cancelling out the artificial corruption leads to devised representations that are closer to the stable structures and invariant features present in the input data space."
      },
      "date": 1674576304764
    },
    {
      "type": "add",
      "id": "8c10b05b4009d6ed",
      "item": {
        "type": "paragraph",
        "id": "8c10b05b4009d6ed",
        "text": "Supervised learning tasks, i.e. regression and classification, are the primary objectives of deep learning methodologies. However, a modular approach where unsupervised pre-training replaces the usual initialization schemes during the development of (semi)supervised deep learning algorithms has been shown to be effective and is now the standard approach in many developments (Bengio, Lamblin, Popovici, & Larochelle, 2007; Erhan, et al., 2010; Erhan, Manzagol, Bengio, Bengio, & Vincent, 2009; Hinton, et al., 2006). In the deep learning paradigm, the unsupervised process contributes to the tuning of each layer. This intervention at each layer adds up to a cascade yielding an improved abstraction of higher-level distributed representations as progress is made bottom-up along the layered hierarchy. In this implementation, unsupervised learning effectively reduces the redundancy by progressively gathering and consolidating the information existent in the original data."
      },
      "after": "6f6251f3441cf3b4",
      "date": 1674576356889
    },
    {
      "type": "edit",
      "id": "8c10b05b4009d6ed",
      "item": {
        "type": "paragraph",
        "id": "8c10b05b4009d6ed",
        "text": "Supervised learning tasks, i.e. regression and classification, are the primary objectives of deep learning methodologies. However, a modular approach where unsupervised pre-training replaces the usual initialization schemes during the development of (semi)supervised deep learning algorithms has been shown to be effective and is now the standard approach in many developments (Bengio, Lamblin, Popovici, & Larochelle, 2007; Erhan, et al., 2010; Erhan, Manzagol, Bengio, Bengio, & Vincent, 2009; Hinton, et al., 2006). "
      },
      "date": 1674576379715
    },
    {
      "type": "add",
      "id": "287fd434b15caba0",
      "item": {
        "type": "paragraph",
        "id": "287fd434b15caba0",
        "text": "In the deep learning paradigm, the unsupervised process contributes to the tuning of each layer. This intervention at each layer adds up to a cascade yielding an improved abstraction of higher-level distributed representations as progress is made bottom-up along the layered hierarchy. In this implementation, unsupervised learning effectively reduces the redundancy by progressively gathering and consolidating the information existent in the original data."
      },
      "after": "8c10b05b4009d6ed",
      "date": 1674576380398
    },
    {
      "type": "add",
      "id": "d907322059955412",
      "item": {
        "type": "paragraph",
        "id": "d907322059955412",
        "text": "While drawing inspiration from deep learning here we propose a model that deviates from the above principle of layer by layer contribution, instead we hybridize a pure unsupervised learning algorithm (viz. the SOM) with the unsupervised learning module of a deep learning architecture (viz. the DA). The Denoising Autoencoder Self-Organizing Map (DASOM) structurally combines, architecturally stacks and algorithmically fuses its two unsupervised learning components. In the following section we present the basic framework and characteristics of the DASOM model that:"
      },
      "after": "287fd434b15caba0",
      "date": 1674576412450
    },
    {
      "type": "add",
      "id": "fc3163543d8de053",
      "item": {
        "type": "paragraph",
        "id": "fc3163543d8de053",
        "text": "1. Captures and projects the relations between higher representations extracted from input data, and simultaneously maps these onto two-dimensional manifolds (e.g. planes). Throughout the procedure, data clusters and their principal correlations are visually observed and analyzed."
      },
      "after": "d907322059955412",
      "date": 1674576449264
    },
    {
      "type": "add",
      "id": "2f2b6bcb2736aaa4",
      "item": {
        "type": "paragraph",
        "id": "2f2b6bcb2736aaa4",
        "text": "2. By effectively incorporating the use of component planes the influence and the importance of each nonlinear combination of input features are visualized on the neural map. Building upon a mechanism to gain insight into the produced representations, a feature selection scheme is realized based on the highest contributing attributes and inputs of specified representations."
      },
      "after": "fc3163543d8de053",
      "date": 1674576470959
    },
    {
      "type": "add",
      "id": "7a0b6d255db4f6c7",
      "item": {
        "type": "paragraph",
        "id": "7a0b6d255db4f6c7",
        "text": "The remainder of this paper is organized into five sections. Section 2 presents in detail the DASOM both architecturally and algorithmically, and subsequently, analyzes the corresponding learning phases and adaptation procedures. Section 3 contains qualitative and quantitative experimental results, performance evaluations and comparisons with different algorithms. A systematic evaluation of the data clustering, visualizations and low-dimensional projections is given in Section 4. In section 5 a summary is given and conclusions are drawn. We include an Appendix that details the analytical equations for the DASOM learning algorithm as well as the derivation thereof."
      },
      "after": "2f2b6bcb2736aaa4",
      "date": 1674576499676
    },
    {
      "item": {
        "type": "factory",
        "id": "ab6aaaab23632dbf"
      },
      "id": "ab6aaaab23632dbf",
      "type": "add",
      "after": "7a0b6d255db4f6c7",
      "date": 1674576691303
    },
    {
      "type": "edit",
      "id": "ab6aaaab23632dbf",
      "item": {
        "type": "pagefold",
        "id": "ab6aaaab23632dbf",
        "text": "~"
      },
      "date": 1674576694298
    },
    {
      "item": {
        "type": "factory",
        "id": "1b082a25349b339c"
      },
      "id": "1b082a25349b339c",
      "type": "add",
      "after": "ab6aaaab23632dbf",
      "date": 1674576697149
    },
    {
      "type": "edit",
      "id": "1b082a25349b339c",
      "item": {
        "type": "paragraph",
        "id": "1b082a25349b339c",
        "text": "Kohonen, T. (2001). Self-organizing maps, vol. 30 of Springer Series in Information Sciences. ed:SpringerBerlin. "
      },
      "date": 1674576702089
    },
    {
      "type": "add",
      "id": "0f50d518121a279c",
      "item": {
        "type": "paragraph",
        "id": "0f50d518121a279c",
        "text": "Kohonen, T. (2013). Essentials of the self-organizing map. NeuralNetworks,37, 52-65. "
      },
      "after": "1b082a25349b339c",
      "date": 1674576704734
    },
    {
      "type": "add",
      "id": "1dda4e57fe58f6c0",
      "item": {
        "type": "paragraph",
        "id": "1dda4e57fe58f6c0",
        "text": "Kohonen, T. (2014). MATLAB implementations and applications of the self-organizing map. UnigrafiaOy,Helsinki,Finland, 11-23."
      },
      "after": "0f50d518121a279c",
      "date": 1674576705353
    },
    {
      "type": "edit",
      "id": "91f1bb56e352aea6",
      "item": {
        "type": "paragraph",
        "id": "91f1bb56e352aea6",
        "text": "The structural and functional nature of biological neural systems provides a developmental template for artificial neural network algorithms. Structurally networks of topographically ordered neurons mimic nerve nets of the [[Visual Cortex]] (Von der Malsburg, 1973) and deep models (H. Lee, Ekanadham, & Ng, 2008) capture and store higher representations extracted from inherent regularities and structure in data (Ito & Komatsu, 2004). "
      },
      "date": 1674576830316
    },
    {
      "type": "edit",
      "id": "0f50d518121a279c",
      "item": {
        "type": "paragraph",
        "id": "0f50d518121a279c",
        "text": "Kohonen, T. (2013). [[Essentials of the Self-Organizing Map]]. NeuralNetworks,37, 52-65. "
      },
      "date": 1674577128527
    },
    {
      "type": "edit",
      "id": "1b082a25349b339c",
      "item": {
        "type": "paragraph",
        "id": "1b082a25349b339c",
        "text": "Kohonen, T. (2001). [[Self-organizing maps]], vol. 30 of Springer Series in Information Sciences. ed:SpringerBerlin. "
      },
      "date": 1674577141438
    },
    {
      "type": "edit",
      "id": "1b082a25349b339c",
      "item": {
        "type": "paragraph",
        "id": "1b082a25349b339c",
        "text": "Kohonen, T. (2001). [[Self-Organizing Maps]], vol. 30 of Springer Series in Information Sciences. ed:SpringerBerlin. "
      },
      "date": 1674577161310
    }
  ]
}