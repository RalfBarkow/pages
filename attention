{
  "title": "Attention",
  "story": [
    {
      "type": "reference",
      "id": "37433e841a354360",
      "site": "wiki.ralfbarkow.ch",
      "slug": "sparse-distributed-memory",
      "title": "Sparse Distributed Memory",
      "text": "While attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well."
    },
    {
      "type": "markdown",
      "id": "e3fc02baa1ae70c7",
      "text": "* Functional role of attention, see [[Predictive Brains]]\n* One considers what could come of other's forced attention, see [[Garden's Meet]], [[Event Wiki]]"
    },
    {
      "type": "paragraph",
      "id": "5ae603be0adbebac",
      "text": "When you use a tool, you are trying to achieve something. You have a [[Story]] in mind. For example, \"given all the expenses I had this year, I want to see what kind of holiday I can afford\" or \"given my running [[Emacs]], I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story [[molds]] the context into a result. It does it in a way that attracts the listener attention! (See [[Moldable Emacs]], README.org)"
    },
    {
      "type": "markdown",
      "id": "f411c0141e947075",
      "text": "> Since there is so much data around us, we could be telling fabulous stories. The problem is that there is too much information to read it all! We just cannot absorb it as text (or it would take too long). [[We]] need tools that help us telling our stories! They must do so by attracting our attention on what is important.\n"
    },
    {
      "type": "reference",
      "id": "cc2165363681a7d5",
      "site": "wiki.ralfbarkow.ch",
      "slug": "continuum",
      "title": "Continuum",
      "text": "between [[Synthesis]] and [[Analysis]]"
    },
    {
      "type": "pagefold",
      "id": "b775d4a983d09abe",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "6fee831a31ff6066",
      "text": "Aufmerksamkeit (engl.: Attention) ist ein 2017 von einem Team um Google Brain vorgeschlagenes Konzept, um diese Verständnishierarchie von Wörtern und Sätzen gar nicht erst aufbauen zu müssen. Sie ist die Schlüsselinnovation der Transformer-Modelle und damit auch der GPT-Familie. Aufmerksamkeit ist ein sehr mächtiges Werkzeug, etwas, das—so mutmasst Karpathy im Video—den Autoren zum Zeitpunkt der Publikation wahrscheinlich gar nicht bewusst gewesen sei. [https://dnip.ch/2023/01/30/wie-funktioniert-eigentlich-chatgpt/#erreicht_nanoGPT_durch_Aufmerksamkeit post]\n\nWie funktioniert diese Aufmerksamkeit? Sie versucht automatisch statistische Zusammenhänge zwischen den verwendeten Bigrammen und ihren direkten und indirekten Vorläufersymbolen herzustellen. Um das Beispiel Karpathys aufzugreifen: Ein Vokal an der aktuellen Position könnte sich für einige der vorangegangenen Konsonanten „interessieren“. Das Ziel ist also statt alle möglichen Kombinationen der letzten Zeichen abzuspeichern, nur einen spezifischen Kommunikationskanal mit dem relevantesten vorangegangen Zustand zu öffnen, über den Informationen ausgetauscht werden. Damit wird gegenüber dem naiven, unpraktikablen Ansatz mit riesigem Speicherverbrauch nun massiv (exponentiell) weniger Speicher benötigt, aber der Rechenaufwand zur Laufzeit wächst dafür etwas (linear) an.\n\nAttention kann man auch als dynamisches Publish-Subscribe-System zwischen den verschiedenen Zeichen im Text verstehen: Das aktuelle Zeichen „fragt“ alle Vorgänger über eine Query (einen Vektor mit verschiedenen Gewichten) an und diese antworten mit einem Key (deren Information). Durch (Skalar-)Multiplikation der Vektoren entstehen verschiedene Signale und das stärkste wird herausgefiltert, die mutmasslich hilfreichste Information, und fliesst in die Auswahl des nächsten auszugebenden Zeichens ein.\n\nAuch wenn es hilfreich ist, sich diese Kommunikation wie oben beschrieben als „Vokal sucht Konsonant“ vorzustellen, diese kommunizierten Informationen haben meist keine Entsprechung im menschlichen Sprachverständnis. Dies sind einfach irgendwelche Werte, welche statistisch bessere Ausgaben zu erzeugen helfen. Erstaunlich ist, dass es trotzdem funktioniert, obwohl keine Orthografie- oder Grammatikregeln dahinter stecken!\n\nNanoGPT nutzt die Aufmerksamkeit nur über die letzten 1-8 Zeichen und trotzdem scheint der resultierende Text inzwischen deutlich mehr aus Wörtern und nicht mehr nur Zeichen zu bestehen. […]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Attention",
        "story": []
      },
      "date": 1637901196539
    },
    {
      "item": {
        "type": "factory",
        "id": "e3fc02baa1ae70c7"
      },
      "id": "e3fc02baa1ae70c7",
      "type": "add",
      "date": 1637901233639
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "paragraph",
        "id": "e3fc02baa1ae70c7",
        "text": "* the functional role of attention"
      },
      "date": 1637901240358
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "markdown",
        "id": "e3fc02baa1ae70c7",
        "text": "* the functional role of attention"
      },
      "date": 1637901242101
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "markdown",
        "id": "e3fc02baa1ae70c7",
        "text": "* the functional role of attention, see [[Predictive Brains]]"
      },
      "date": 1637901269448
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "markdown",
        "id": "e3fc02baa1ae70c7",
        "text": "* Functional role of [[attention]], see [[Predictive Brains]]\n* One considers what could come of other's forced [[attention]], see "
      },
      "date": 1637901337593
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "markdown",
        "id": "e3fc02baa1ae70c7",
        "text": "* Functional role of [[attention]], see [[Predictive Brains]]\n* One considers what could come of other's forced [[attention]], see Garden's Meet"
      },
      "date": 1637901346290
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "markdown",
        "id": "e3fc02baa1ae70c7",
        "text": "* Functional role of [[attention]], see [[Predictive Brains]]\n* One considers what could come of other's forced [[attention]], see [[Garden's Meet]]"
      },
      "date": 1637901353832
    },
    {
      "item": {
        "type": "factory",
        "id": "5ae603be0adbebac"
      },
      "id": "5ae603be0adbebac",
      "type": "add",
      "after": "e3fc02baa1ae70c7",
      "date": 1638508402232
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a\nstory in mind. For example, \"given all the expenses I had this year, I\nwant to see what kind of holiday I can afford\" or \"given my running\nEmacs, I want to make sure I didn't introduce a memory bug by checking\nthe variable sizes (otherwise I have to review my code!)\". In all your\nstories you have some context and some result (finances -> holiday\nbudget). A good story molds the context into a result. It does it in a\nway that attracts the listener attention!\n"
      },
      "date": 1638508403935
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a\nstory in mind. For example, \"given all the expenses I had this year, I\nwant to see what kind of holiday I can afford\" or \"given my running Emacs, I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story molds the context into a result. It does it in a way that attracts the listener [[attention]]!\n"
      },
      "date": 1638508428910
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a [[story]] in mind. For example, \"given all the expenses I had this year, I want to see what kind of holiday I can afford\" or \"given my running Emacs, I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story molds the context into a result. It does it in a way that attracts the listener [[attention]]!\n"
      },
      "date": 1638508450383
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a [[story]] in mind. For example, \"given all the expenses I had this year, I want to see what kind of holiday I can afford\" or \"given my running [[Emacs]], I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story molds the context into a result. It does it in a way that attracts the listener [[attention]]!\n"
      },
      "date": 1638508471173
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a [[story]] in mind. For example, \"given all the expenses I had this year, I want to see what kind of holiday I can afford\" or \"given my running [[Emacs]], I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story molds the context into a result. It does it in a way that attracts the listener [[attention]]! (See [[Moldable Emacs]], "
      },
      "date": 1638508552349
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a [[story]] in mind. For example, \"given all the expenses I had this year, I want to see what kind of holiday I can afford\" or \"given my running [[Emacs]], I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story molds the context into a result. It does it in a way that attracts the listener [[attention]]! (See [[Moldable Emacs]], README.org)"
      },
      "date": 1638508562067
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "markdown",
        "id": "e3fc02baa1ae70c7",
        "text": "* Functional role of [[attention]], see [[Predictive Brains]]\n* One considers what could come of other's forced [[attention]], see [[Garden's Meet]], [[Event Wiki]]"
      },
      "date": 1638508917174
    },
    {
      "type": "add",
      "id": "f411c0141e947075",
      "item": {
        "type": "paragraph",
        "id": "f411c0141e947075",
        "text": "> Since there is so much data around us, we could be telling fabulous\nstories. The problem is that there is too much information to read it\nall! We just cannot absorb it as text (or it would take too long). We\nneed tools that help us telling our stories! They must do so by\nattracting our attention on what is important.\n"
      },
      "after": "5ae603be0adbebac",
      "date": 1638509054962
    },
    {
      "type": "edit",
      "id": "f411c0141e947075",
      "item": {
        "type": "markdown",
        "id": "f411c0141e947075",
        "text": "> Since there is so much data around us, we could be telling fabulous\nstories. The problem is that there is too much information to read it\nall! We just cannot absorb it as text (or it would take too long). We\nneed tools that help us telling our stories! They must do so by\nattracting our attention on what is important.\n"
      },
      "date": 1638509056698
    },
    {
      "type": "edit",
      "id": "f411c0141e947075",
      "item": {
        "type": "markdown",
        "id": "f411c0141e947075",
        "text": "> Since there is so much data around us, we could be telling fabulous stories. The problem is that there is too much information to read it all! We just cannot absorb it as text (or it would take too long). We\nneed tools that help us telling our stories! They must do so by attracting our attention on what is important.\n"
      },
      "date": 1638509068875
    },
    {
      "type": "edit",
      "id": "f411c0141e947075",
      "item": {
        "type": "markdown",
        "id": "f411c0141e947075",
        "text": "> Since there is so much data around us, we could be telling fabulous stories. The problem is that there is too much information to read it all! We just cannot absorb it as text (or it would take too long). [[We]] need tools that help us telling our stories! They must do so by attracting our attention on what is important.\n"
      },
      "date": 1638509079026
    },
    {
      "type": "edit",
      "id": "f411c0141e947075",
      "item": {
        "type": "markdown",
        "id": "f411c0141e947075",
        "text": "> Since there is so much data around us, we could be telling fabulous stories. The problem is that there is too much information to read it all! We just cannot absorb it as text (or it would take too long). [[We]] need tools that help us telling our stories! They must do so by attracting our [[attention]] on what is important.\n"
      },
      "date": 1638509198117
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a [[story]] in mind. For example, \"given all the expenses I had this year, I want to see what kind of holiday I can afford\" or \"given my running [[Emacs]], I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story [[molds]] the context into a result. It does it in a way that attracts the listener [[attention]]! (See [[Moldable Emacs]], README.org)"
      },
      "date": 1638510021764
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "markdown",
        "id": "e3fc02baa1ae70c7",
        "text": "* Functional role of attention, see [[Predictive Brains]]\n* One considers what could come of other's forced [[attention]], see [[Garden's Meet]], [[Event Wiki]]"
      },
      "date": 1673944060207
    },
    {
      "item": {
        "type": "factory",
        "id": "37433e841a354360"
      },
      "id": "37433e841a354360",
      "type": "add",
      "after": "f411c0141e947075",
      "date": 1673944134336
    },
    {
      "type": "edit",
      "id": "37433e841a354360",
      "item": {
        "type": "reference",
        "id": "37433e841a354360",
        "site": "wiki.ralfbarkow.ch",
        "slug": "sparse-distributed-memory",
        "title": "Sparse Distributed Memory",
        "text": "While [[Attention]] has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well."
      },
      "date": 1673944138665
    },
    {
      "id": "37433e841a354360",
      "type": "move",
      "order": [
        "37433e841a354360",
        "e3fc02baa1ae70c7",
        "5ae603be0adbebac",
        "f411c0141e947075"
      ],
      "date": 1673944141450
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a [[Story]] in mind. For example, \"given all the expenses I had this year, I want to see what kind of holiday I can afford\" or \"given my running [[Emacs]], I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story [[molds]] the context into a result. It does it in a way that attracts the listener [[attention]]! (See [[Moldable Emacs]], README.org)"
      },
      "date": 1673944155345
    },
    {
      "type": "edit",
      "id": "37433e841a354360",
      "item": {
        "type": "reference",
        "id": "37433e841a354360",
        "site": "wiki.ralfbarkow.ch",
        "slug": "sparse-distributed-memory",
        "title": "Sparse Distributed Memory",
        "text": "While attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well."
      },
      "date": 1673944169112
    },
    {
      "item": {
        "type": "factory",
        "id": "cc2165363681a7d5"
      },
      "id": "cc2165363681a7d5",
      "type": "add",
      "after": "f411c0141e947075",
      "date": 1673944906581
    },
    {
      "type": "edit",
      "id": "cc2165363681a7d5",
      "item": {
        "type": "reference",
        "id": "cc2165363681a7d5",
        "site": "wiki.ralfbarkow.ch",
        "slug": "continuum",
        "title": "Continuum",
        "text": "between [[Synthesis]] and [[Analysis]]"
      },
      "date": 1673944911051
    },
    {
      "type": "edit",
      "id": "f411c0141e947075",
      "item": {
        "type": "markdown",
        "id": "f411c0141e947075",
        "text": "> Since there is so much data around us, we could be telling fabulous stories. The problem is that there is too much information to read it all! We just cannot absorb it as text (or it would take too long). [[We]] need tools that help us telling our stories! They must do so by attracting our attention on what is important.\n"
      },
      "date": 1673944919509
    },
    {
      "type": "edit",
      "id": "e3fc02baa1ae70c7",
      "item": {
        "type": "markdown",
        "id": "e3fc02baa1ae70c7",
        "text": "* Functional role of attention, see [[Predictive Brains]]\n* One considers what could come of other's forced attention, see [[Garden's Meet]], [[Event Wiki]]"
      },
      "date": 1673944943465
    },
    {
      "type": "edit",
      "id": "5ae603be0adbebac",
      "item": {
        "type": "paragraph",
        "id": "5ae603be0adbebac",
        "text": "When you use a tool, you are trying to achieve something. You have a [[Story]] in mind. For example, \"given all the expenses I had this year, I want to see what kind of holiday I can afford\" or \"given my running [[Emacs]], I want to make sure I didn't introduce a memory bug by checking the variable sizes (otherwise I have to review my code!)\". In all your stories you have some context and some result (finances -> holiday budget). A good story [[molds]] the context into a result. It does it in a way that attracts the listener attention! (See [[Moldable Emacs]], README.org)"
      },
      "date": 1673945037779
    },
    {
      "item": {
        "type": "factory",
        "id": "b775d4a983d09abe"
      },
      "id": "b775d4a983d09abe",
      "type": "add",
      "after": "cc2165363681a7d5",
      "date": 1676202669241
    },
    {
      "type": "edit",
      "id": "b775d4a983d09abe",
      "item": {
        "type": "pagefold",
        "id": "b775d4a983d09abe",
        "text": "~"
      },
      "date": 1676202672722
    },
    {
      "item": {
        "type": "factory",
        "id": "6fee831a31ff6066"
      },
      "id": "6fee831a31ff6066",
      "type": "add",
      "after": "b775d4a983d09abe",
      "date": 1676202673920
    },
    {
      "type": "edit",
      "id": "6fee831a31ff6066",
      "item": {
        "type": "paragraph",
        "id": "6fee831a31ff6066",
        "text": "Aufmerksamkeit (engl.: Attention) ist ein 2017 von einem Team um Google Brain vorgeschlagenes Konzept, um diese Verständnishierarchie von Wörtern und Sätzen gar nicht erst aufbauen zu müssen. Sie ist die Schlüsselinnovation der Transformer-Modelle und damit auch der GPT-Familie. Aufmerksamkeit ist ein sehr mächtiges Werkzeug, etwas, das—so mutmasst Karpathy im Video—den Autoren zum Zeitpunkt der Publikation wahrscheinlich gar nicht bewusst gewesen sei.\n\nWie funktioniert diese Aufmerksamkeit? Sie versucht automatisch statistische Zusammenhänge zwischen den verwendeten Bigrammen und ihren direkten und indirekten Vorläufersymbolen herzustellen. Um das Beispiel Karpathys aufzugreifen: Ein Vokal an der aktuellen Position könnte sich für einige der vorangegangenen Konsonanten „interessieren“. Das Ziel ist also statt alle möglichen Kombinationen der letzten Zeichen abzuspeichern, nur einen spezifischen Kommunikationskanal mit dem relevantesten vorangegangen Zustand zu öffnen, über den Informationen ausgetauscht werden. Damit wird gegenüber dem naiven, unpraktikablen Ansatz mit riesigem Speicherverbrauch nun massiv (exponentiell) weniger Speicher benötigt, aber der Rechenaufwand zur Laufzeit wächst dafür etwas (linear) an.\n\nAttention kann man auch als dynamisches Publish-Subscribe-System zwischen den verschiedenen Zeichen im Text verstehen: Das aktuelle Zeichen „fragt“ alle Vorgänger über eine Query (einen Vektor mit verschiedenen Gewichten) an und diese antworten mit einem Key (deren Information). Durch (Skalar-)Multiplikation der Vektoren entstehen verschiedene Signale und das stärkste wird herausgefiltert, die mutmasslich hilfreichste Information, und fliesst in die Auswahl des nächsten auszugebenden Zeichens ein.\n\nAuch wenn es hilfreich ist, sich diese Kommunikation wie oben beschrieben als „Vokal sucht Konsonant“ vorzustellen, diese kommunizierten Informationen haben meist keine Entsprechung im menschlichen Sprachverständnis. Dies sind einfach irgendwelche Werte, welche statistisch bessere Ausgaben zu erzeugen helfen. Erstaunlich ist, dass es trotzdem funktioniert, obwohl keine Orthografie- oder Grammatikregeln dahinter stecken!\n\nNanoGPT nutzt die Aufmerksamkeit nur über die letzten 1-8 Zeichen und trotzdem scheint der resultierende Text inzwischen deutlich mehr aus Wörtern und nicht mehr nur Zeichen zu bestehen."
      },
      "date": 1676202676398
    },
    {
      "type": "edit",
      "id": "6fee831a31ff6066",
      "item": {
        "type": "paragraph",
        "id": "6fee831a31ff6066",
        "text": "Aufmerksamkeit (engl.: Attention) ist ein 2017 von einem Team um Google Brain vorgeschlagenes Konzept, um diese Verständnishierarchie von Wörtern und Sätzen gar nicht erst aufbauen zu müssen. Sie ist die Schlüsselinnovation der Transformer-Modelle und damit auch der GPT-Familie. Aufmerksamkeit ist ein sehr mächtiges Werkzeug, etwas, das—so mutmasst Karpathy im Video—den Autoren zum Zeitpunkt der Publikation wahrscheinlich gar nicht bewusst gewesen sei. [https://dnip.ch/2023/01/30/wie-funktioniert-eigentlich-chatgpt/#erreicht_nanoGPT_durch_Aufmerksamkeit post]\n\nWie funktioniert diese Aufmerksamkeit? Sie versucht automatisch statistische Zusammenhänge zwischen den verwendeten Bigrammen und ihren direkten und indirekten Vorläufersymbolen herzustellen. Um das Beispiel Karpathys aufzugreifen: Ein Vokal an der aktuellen Position könnte sich für einige der vorangegangenen Konsonanten „interessieren“. Das Ziel ist also statt alle möglichen Kombinationen der letzten Zeichen abzuspeichern, nur einen spezifischen Kommunikationskanal mit dem relevantesten vorangegangen Zustand zu öffnen, über den Informationen ausgetauscht werden. Damit wird gegenüber dem naiven, unpraktikablen Ansatz mit riesigem Speicherverbrauch nun massiv (exponentiell) weniger Speicher benötigt, aber der Rechenaufwand zur Laufzeit wächst dafür etwas (linear) an.\n\nAttention kann man auch als dynamisches Publish-Subscribe-System zwischen den verschiedenen Zeichen im Text verstehen: Das aktuelle Zeichen „fragt“ alle Vorgänger über eine Query (einen Vektor mit verschiedenen Gewichten) an und diese antworten mit einem Key (deren Information). Durch (Skalar-)Multiplikation der Vektoren entstehen verschiedene Signale und das stärkste wird herausgefiltert, die mutmasslich hilfreichste Information, und fliesst in die Auswahl des nächsten auszugebenden Zeichens ein.\n\nAuch wenn es hilfreich ist, sich diese Kommunikation wie oben beschrieben als „Vokal sucht Konsonant“ vorzustellen, diese kommunizierten Informationen haben meist keine Entsprechung im menschlichen Sprachverständnis. Dies sind einfach irgendwelche Werte, welche statistisch bessere Ausgaben zu erzeugen helfen. Erstaunlich ist, dass es trotzdem funktioniert, obwohl keine Orthografie- oder Grammatikregeln dahinter stecken!\n\nNanoGPT nutzt die Aufmerksamkeit nur über die letzten 1-8 Zeichen und trotzdem scheint der resultierende Text inzwischen deutlich mehr aus Wörtern und nicht mehr nur Zeichen zu bestehen."
      },
      "date": 1676202699121
    },
    {
      "type": "edit",
      "id": "6fee831a31ff6066",
      "item": {
        "type": "paragraph",
        "id": "6fee831a31ff6066",
        "text": "Aufmerksamkeit (engl.: Attention) ist ein 2017 von einem Team um Google Brain vorgeschlagenes Konzept, um diese Verständnishierarchie von Wörtern und Sätzen gar nicht erst aufbauen zu müssen. Sie ist die Schlüsselinnovation der Transformer-Modelle und damit auch der GPT-Familie. Aufmerksamkeit ist ein sehr mächtiges Werkzeug, etwas, das—so mutmasst Karpathy im Video—den Autoren zum Zeitpunkt der Publikation wahrscheinlich gar nicht bewusst gewesen sei. [https://dnip.ch/2023/01/30/wie-funktioniert-eigentlich-chatgpt/#erreicht_nanoGPT_durch_Aufmerksamkeit post]\n\nWie funktioniert diese Aufmerksamkeit? Sie versucht automatisch statistische Zusammenhänge zwischen den verwendeten Bigrammen und ihren direkten und indirekten Vorläufersymbolen herzustellen. Um das Beispiel Karpathys aufzugreifen: Ein Vokal an der aktuellen Position könnte sich für einige der vorangegangenen Konsonanten „interessieren“. Das Ziel ist also statt alle möglichen Kombinationen der letzten Zeichen abzuspeichern, nur einen spezifischen Kommunikationskanal mit dem relevantesten vorangegangen Zustand zu öffnen, über den Informationen ausgetauscht werden. Damit wird gegenüber dem naiven, unpraktikablen Ansatz mit riesigem Speicherverbrauch nun massiv (exponentiell) weniger Speicher benötigt, aber der Rechenaufwand zur Laufzeit wächst dafür etwas (linear) an.\n\nAttention kann man auch als dynamisches Publish-Subscribe-System zwischen den verschiedenen Zeichen im Text verstehen: Das aktuelle Zeichen „fragt“ alle Vorgänger über eine Query (einen Vektor mit verschiedenen Gewichten) an und diese antworten mit einem Key (deren Information). Durch (Skalar-)Multiplikation der Vektoren entstehen verschiedene Signale und das stärkste wird herausgefiltert, die mutmasslich hilfreichste Information, und fliesst in die Auswahl des nächsten auszugebenden Zeichens ein.\n\nAuch wenn es hilfreich ist, sich diese Kommunikation wie oben beschrieben als „Vokal sucht Konsonant“ vorzustellen, diese kommunizierten Informationen haben meist keine Entsprechung im menschlichen Sprachverständnis. Dies sind einfach irgendwelche Werte, welche statistisch bessere Ausgaben zu erzeugen helfen. Erstaunlich ist, dass es trotzdem funktioniert, obwohl keine Orthografie- oder Grammatikregeln dahinter stecken!\n\nNanoGPT nutzt die Aufmerksamkeit nur über die letzten 1-8 Zeichen und trotzdem scheint der resultierende Text inzwischen deutlich mehr aus Wörtern und nicht mehr nur Zeichen zu bestehen. […]"
      },
      "date": 1676202802097
    }
  ]
}