{
  "title": "What Aspects of Lisp Influenced Smalltalk?",
  "story": [
    {
      "type": "markdown",
      "id": "151c87f8e17041da",
      "text": "I (Alan Kay) have written about this variously elsewhere: including in “The Early History of Smalltalk”, and I’ve emphasized especially that “the idea of Lisp” pervades the underpinnings of Smalltalk as it was developed at Xerox Parc. [https://www.quora.com/What-aspects-of-Lisp-influenced-Smalltalk quora]"
    },
    {
      "type": "markdown",
      "id": "00e406dcb5a2a540",
      "text": "For me, it was spending a Sunday afternoon in the late 60s tracing through John McCarthy’s eval-apply for Lisp that rotated my perspective to thinking about computing “that way”. This included how late binding everywhere and finding a universal composable and embedable computing element could astoundingly reveal “hidden simplicities” in programming, its design, and implications for programming languages.\n\nThis got me to go back and really look at Peter Deutsch’s PDP-1 interactive Lisp in the Bobrow & Berkeley book “The Programming Language Lisp” — the entire PDP-1 assembly code was included in the paper, and Peter had realized that if you’ve got a really good live language, that you don’t need a separate OS. I was using a very similar approach for the Flex Machine personal computer, and the Flex language was derived from Euler, an extreme generalization of Algol by Wirth that came close to being a Lisp.\n\n(At Parc the several versions of Smalltalk that we did used internals that were variants of the internals of both Lisp and Euler.)\n\nThe peculiar organization of Lisp — once you got used to it — made deep fundamental things easier to think about than Euler (and the general Algolish approaches of the day).\n\nOne thing that stands out is that about half the tiny McCarthy interpreter is only there because he was thinking about functions that evaluate their arguments before the lambda form is “applied” to them. This requires forms of quoting to delay evaling for the “special forms” of QUOTE, COND, LAMBDA, etc. But you don’t need that, and the MIT Lisp 1.5 people had already introduced the idea of an FEXPR, which doesn’t eval its arguments at call time, but simply is handed the argument list. The FEXPRs can then eval parameters if needed and desired (this allows e.g. “COND” to just be a vanilla FEXPR).\n\nAlso, Lisp raised the sibling question in a thinkable way about passing a lambda with free (global) variables.\n\n* What does this mean in the new environment? Look for them dynamically in the new environment? Can be useful.\n* Bind the globals to their values in the calling environment and package the lambda with its own a-list? Very useful (and very like a Simula-style object).\n* Search for the values of the globals dynamically in the calling environment? Also very useful, and rather like the *thunk* ideas in Algol for “calling by name”.\n\nLisp also had a second set of interesting ideas that were “loosely” (meaning “not really”) integrated into the rest of the language: the property lists on atoms. If you were thinking “Sketchpad” or “Simula” thoughts, and looked at Lisp, you would see some of what you wanted in closures and some of what you wanted in property lists.\n\nThe term “object” in the 60s meant “a compound data structure with fields”, and this was another way to use property lists. Since you could stash procedures on the property list along with anything else you wanted, you could make a whole little world. If you had the procedures have a global variable that contained a pointer to the property list the procedure was part of, then quite a bit could be done.\n\nInterestingly, with this aspect of Lisp, the way it was done made it harder to “think good thoughts” than was helpful.\n\nThe offspring of Peter’s PDP-1 Lisp was BBN Lisp (1.85) and it supplied “all of the above”, and much more.\n\nAnother interesting dynamic back then was Warren Teitelman’s thesis “Pilot”, his heroic pass at McCarthy’s “Advice Taker” via meta programming rather than AI. Many of these ideas were incorporated in BBN Lisp when Teitelman came to BBN. (This and more became Interlisp.)\n\nAnd then there were more and more really interesting AI systems — many of them thesis projects — that were done in Lisp.\n\nI thought that the key principle here was that Lisp allowed you to learn more — in part by being able to try more and change your mind more — than any other programming system. This was especially the case when you needed something that the bottom of the system didn’t have — the meta reflective properties of Lisp gave you more recourses to *still* do something that would help.\n\nIn a world in which almost nothing was known about programming — even by the best programmers — this was really huge.\n\nI don’t think a lot more is known today about programming than then, though Parc did make a few important advances. But many of the advances happened precisely because Smalltalk manifested its ideas about “instantiating intercommunicating processes” by using Lisp techniques. If I had to do it again, I would retain much of this approach, but would do a very different design in place of “Lisp sharing”."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "What Aspects of Lisp Influenced Smalltalk?",
        "story": []
      },
      "date": 1707811560969
    },
    {
      "item": {
        "type": "factory",
        "id": "151c87f8e17041da"
      },
      "id": "151c87f8e17041da",
      "type": "add",
      "date": 1707811580423
    },
    {
      "type": "edit",
      "id": "151c87f8e17041da",
      "item": {
        "type": "markdown",
        "id": "151c87f8e17041da",
        "text": "I’ve written about this variously elsewhere: including in “The Early History of Smalltalk”, and I’ve emphasized especially that “the idea of Lisp” pervades the underpinnings of Smalltalk as it was developed at Xerox Parc.\n\nFor me, it was spending a Sunday afternoon in the late 60s tracing through John McCarthy’s eval-apply for Lisp that rotated my perspective to thinking about computing “that way”. This included how late binding everywhere and finding a universal composable and embedable computing element could astoundingly reveal “hidden simplicities” in programming, its design, and implications for programming languages.\n\nThis got me to go back and really look at Peter Deutsch’s PDP-1 interactive Lisp in the Bobrow & Berkeley book “The Programming Language Lisp” — the entire PDP-1 assembly code was included in the paper, and Peter had realized that if you’ve got a really good live language, that you don’t need a separate OS. I was using a very similar approach for the Flex Machine personal computer, and the Flex language was derived from Euler, an extreme generalization of Algol by Wirth that came close to being a Lisp.\n\n(At Parc the several versions of Smalltalk that we did used internals that were variants of the internals of both Lisp and Euler.)\n\nThe peculiar organization of Lisp — once you got used to it — made deep fundamental things easier to think about than Euler (and the general Algolish approaches of the day).\n\nOne thing that stands out is that about half the tiny McCarthy interpreter is only there because he was thinking about functions that evaluate their arguments before the lambda form is “applied” to them. This requires forms of quoting to delay evaling for the “special forms” of QUOTE, COND, LAMBDA, etc. But you don’t need that, and the MIT Lisp 1.5 people had already introduced the idea of an FEXPR, which doesn’t eval its arguments at call time, but simply is handed the argument list. The FEXPRs can then eval parameters if needed and desired (this allows e.g. “COND” to just be a vanilla FEXPR).\n\nAlso, Lisp raised the sibling question in a thinkable way about passing a lambda with free (global) variables.\n\n    What does this mean in the new environment? Look for them dynamically in the new environment? Can be useful.\n    Bind the globals to their values in the calling environment and package the lambda with its own a-list? Very useful (and very like a Simula-style object).\n    Search for the values of the globals dynamically in the calling environment? Also very useful, and rather like the *thunk* ideas in Algol for “calling by name”.\n\nLisp also had a second set of interesting ideas that were “loosely” (meaning “not really”) integrated into the rest of the language: the property lists on atoms. If you were thinking “Sketchpad” or “Simula” thoughts, and looked at Lisp, you would see some of what you wanted in closures and some of what you wanted in property lists.\n\nThe term “object” in the 60s meant “a compound data structure with fields”, and this was another way to use property lists. Since you could stash procedures on the property list along with anything else you wanted, you could make a whole little world. If you had the procedures have a global variable that contained a pointer to the property list the procedure was part of, then quite a bit could be done.\n\nInterestingly, with this aspect of Lisp, the way it was done made it harder to “think good thoughts” than was helpful.\n\nThe offspring of Peter’s PDP-1 Lisp was BBN Lisp (1.85) and it supplied “all of the above”, and much more.\n\nAnother interesting dynamic back then was Warren Teitelman’s thesis “Pilot”, his heroic pass at McCarthy’s “Advice Taker” via meta programming rather than AI. Many of these ideas were incorporated in BBN Lisp when Teitelman came to BBN. (This and more became Interlisp.)\n\nAnd then there were more and more really interesting AI systems — many of them thesis projects — that were done in Lisp.\n\nI thought that the key principle here was that Lisp allowed you to learn more — in part by being able to try more and change your mind more — than any other programming system. This was especially the case when you needed something that the bottom of the system didn’t have — the meta reflective properties of Lisp gave you more recourses to *still* do something that would help.\n\nIn a world in which almost nothing was known about programming — even by the best programmers — this was really huge.\n\nI don’t think a lot more is known today about programming than then, though Parc did make a few important advances. But many of the advances happened precisely because Smalltalk manifested its ideas about “instantiating intercommunicating processes” by using Lisp techniques. If I had to do it again, I would retain much of this approach, but would do a very different design in place of “Lisp sharing”."
      },
      "date": 1707811582402
    },
    {
      "type": "edit",
      "id": "151c87f8e17041da",
      "item": {
        "type": "markdown",
        "id": "151c87f8e17041da",
        "text": "I’ve written about this variously elsewhere: including in “The Early History of Smalltalk”, and I’ve emphasized especially that “the idea of Lisp” pervades the underpinnings of Smalltalk as it was developed at Xerox Parc.\n\nFor me, it was spending a Sunday afternoon in the late 60s tracing through John McCarthy’s eval-apply for Lisp that rotated my perspective to thinking about computing “that way”. This included how late binding everywhere and finding a universal composable and embedable computing element could astoundingly reveal “hidden simplicities” in programming, its design, and implications for programming languages.\n\nThis got me to go back and really look at Peter Deutsch’s PDP-1 interactive Lisp in the Bobrow & Berkeley book “The Programming Language Lisp” — the entire PDP-1 assembly code was included in the paper, and Peter had realized that if you’ve got a really good live language, that you don’t need a separate OS. I was using a very similar approach for the Flex Machine personal computer, and the Flex language was derived from Euler, an extreme generalization of Algol by Wirth that came close to being a Lisp.\n\n(At Parc the several versions of Smalltalk that we did used internals that were variants of the internals of both Lisp and Euler.)\n\nThe peculiar organization of Lisp — once you got used to it — made deep fundamental things easier to think about than Euler (and the general Algolish approaches of the day).\n\nOne thing that stands out is that about half the tiny McCarthy interpreter is only there because he was thinking about functions that evaluate their arguments before the lambda form is “applied” to them. This requires forms of quoting to delay evaling for the “special forms” of QUOTE, COND, LAMBDA, etc. But you don’t need that, and the MIT Lisp 1.5 people had already introduced the idea of an FEXPR, which doesn’t eval its arguments at call time, but simply is handed the argument list. The FEXPRs can then eval parameters if needed and desired (this allows e.g. “COND” to just be a vanilla FEXPR).\n\nAlso, Lisp raised the sibling question in a thinkable way about passing a lambda with free (global) variables.\n\n* What does this mean in the new environment? Look for them dynamically in the new environment? Can be useful.\n* Bind the globals to their values in the calling environment and package the lambda with its own a-list? Very useful (and very like a Simula-style object).\n* Search for the values of the globals dynamically in the calling environment? Also very useful, and rather like the *thunk* ideas in Algol for “calling by name”.\n\nLisp also had a second set of interesting ideas that were “loosely” (meaning “not really”) integrated into the rest of the language: the property lists on atoms. If you were thinking “Sketchpad” or “Simula” thoughts, and looked at Lisp, you would see some of what you wanted in closures and some of what you wanted in property lists.\n\nThe term “object” in the 60s meant “a compound data structure with fields”, and this was another way to use property lists. Since you could stash procedures on the property list along with anything else you wanted, you could make a whole little world. If you had the procedures have a global variable that contained a pointer to the property list the procedure was part of, then quite a bit could be done.\n\nInterestingly, with this aspect of Lisp, the way it was done made it harder to “think good thoughts” than was helpful.\n\nThe offspring of Peter’s PDP-1 Lisp was BBN Lisp (1.85) and it supplied “all of the above”, and much more.\n\nAnother interesting dynamic back then was Warren Teitelman’s thesis “Pilot”, his heroic pass at McCarthy’s “Advice Taker” via meta programming rather than AI. Many of these ideas were incorporated in BBN Lisp when Teitelman came to BBN. (This and more became Interlisp.)\n\nAnd then there were more and more really interesting AI systems — many of them thesis projects — that were done in Lisp.\n\nI thought that the key principle here was that Lisp allowed you to learn more — in part by being able to try more and change your mind more — than any other programming system. This was especially the case when you needed something that the bottom of the system didn’t have — the meta reflective properties of Lisp gave you more recourses to *still* do something that would help.\n\nIn a world in which almost nothing was known about programming — even by the best programmers — this was really huge.\n\nI don’t think a lot more is known today about programming than then, though Parc did make a few important advances. But many of the advances happened precisely because Smalltalk manifested its ideas about “instantiating intercommunicating processes” by using Lisp techniques. If I had to do it again, I would retain much of this approach, but would do a very different design in place of “Lisp sharing”."
      },
      "date": 1707811615707
    },
    {
      "type": "edit",
      "id": "151c87f8e17041da",
      "item": {
        "type": "markdown",
        "id": "151c87f8e17041da",
        "text": "I (Alan Kay) have written about this variously elsewhere: including in “The Early History of Smalltalk”, and I’ve emphasized especially that “the idea of Lisp” pervades the underpinnings of Smalltalk as it was developed at Xerox Parc.\n\nFor me, it was spending a Sunday afternoon in the late 60s tracing through John McCarthy’s eval-apply for Lisp that rotated my perspective to thinking about computing “that way”. This included how late binding everywhere and finding a universal composable and embedable computing element could astoundingly reveal “hidden simplicities” in programming, its design, and implications for programming languages.\n\nThis got me to go back and really look at Peter Deutsch’s PDP-1 interactive Lisp in the Bobrow & Berkeley book “The Programming Language Lisp” — the entire PDP-1 assembly code was included in the paper, and Peter had realized that if you’ve got a really good live language, that you don’t need a separate OS. I was using a very similar approach for the Flex Machine personal computer, and the Flex language was derived from Euler, an extreme generalization of Algol by Wirth that came close to being a Lisp.\n\n(At Parc the several versions of Smalltalk that we did used internals that were variants of the internals of both Lisp and Euler.)\n\nThe peculiar organization of Lisp — once you got used to it — made deep fundamental things easier to think about than Euler (and the general Algolish approaches of the day).\n\nOne thing that stands out is that about half the tiny McCarthy interpreter is only there because he was thinking about functions that evaluate their arguments before the lambda form is “applied” to them. This requires forms of quoting to delay evaling for the “special forms” of QUOTE, COND, LAMBDA, etc. But you don’t need that, and the MIT Lisp 1.5 people had already introduced the idea of an FEXPR, which doesn’t eval its arguments at call time, but simply is handed the argument list. The FEXPRs can then eval parameters if needed and desired (this allows e.g. “COND” to just be a vanilla FEXPR).\n\nAlso, Lisp raised the sibling question in a thinkable way about passing a lambda with free (global) variables.\n\n* What does this mean in the new environment? Look for them dynamically in the new environment? Can be useful.\n* Bind the globals to their values in the calling environment and package the lambda with its own a-list? Very useful (and very like a Simula-style object).\n* Search for the values of the globals dynamically in the calling environment? Also very useful, and rather like the *thunk* ideas in Algol for “calling by name”.\n\nLisp also had a second set of interesting ideas that were “loosely” (meaning “not really”) integrated into the rest of the language: the property lists on atoms. If you were thinking “Sketchpad” or “Simula” thoughts, and looked at Lisp, you would see some of what you wanted in closures and some of what you wanted in property lists.\n\nThe term “object” in the 60s meant “a compound data structure with fields”, and this was another way to use property lists. Since you could stash procedures on the property list along with anything else you wanted, you could make a whole little world. If you had the procedures have a global variable that contained a pointer to the property list the procedure was part of, then quite a bit could be done.\n\nInterestingly, with this aspect of Lisp, the way it was done made it harder to “think good thoughts” than was helpful.\n\nThe offspring of Peter’s PDP-1 Lisp was BBN Lisp (1.85) and it supplied “all of the above”, and much more.\n\nAnother interesting dynamic back then was Warren Teitelman’s thesis “Pilot”, his heroic pass at McCarthy’s “Advice Taker” via meta programming rather than AI. Many of these ideas were incorporated in BBN Lisp when Teitelman came to BBN. (This and more became Interlisp.)\n\nAnd then there were more and more really interesting AI systems — many of them thesis projects — that were done in Lisp.\n\nI thought that the key principle here was that Lisp allowed you to learn more — in part by being able to try more and change your mind more — than any other programming system. This was especially the case when you needed something that the bottom of the system didn’t have — the meta reflective properties of Lisp gave you more recourses to *still* do something that would help.\n\nIn a world in which almost nothing was known about programming — even by the best programmers — this was really huge.\n\nI don’t think a lot more is known today about programming than then, though Parc did make a few important advances. But many of the advances happened precisely because Smalltalk manifested its ideas about “instantiating intercommunicating processes” by using Lisp techniques. If I had to do it again, I would retain much of this approach, but would do a very different design in place of “Lisp sharing”."
      },
      "date": 1707811779032
    },
    {
      "type": "edit",
      "id": "151c87f8e17041da",
      "item": {
        "type": "markdown",
        "id": "151c87f8e17041da",
        "text": "I (Alan Kay) have written about this variously elsewhere: including in “The Early History of Smalltalk”, and I’ve emphasized especially that “the idea of Lisp” pervades the underpinnings of Smalltalk as it was developed at Xerox Parc. [https://www.quora.com/What-aspects-of-Lisp-influenced-Smalltalk quora]"
      },
      "date": 1707811812718
    },
    {
      "type": "add",
      "id": "00e406dcb5a2a540",
      "item": {
        "type": "markdown",
        "id": "00e406dcb5a2a540",
        "text": "For me, it was spending a Sunday afternoon in the late 60s tracing through John McCarthy’s eval-apply for Lisp that rotated my perspective to thinking about computing “that way”. This included how late binding everywhere and finding a universal composable and embedable computing element could astoundingly reveal “hidden simplicities” in programming, its design, and implications for programming languages.\n\nThis got me to go back and really look at Peter Deutsch’s PDP-1 interactive Lisp in the Bobrow & Berkeley book “The Programming Language Lisp” — the entire PDP-1 assembly code was included in the paper, and Peter had realized that if you’ve got a really good live language, that you don’t need a separate OS. I was using a very similar approach for the Flex Machine personal computer, and the Flex language was derived from Euler, an extreme generalization of Algol by Wirth that came close to being a Lisp.\n\n(At Parc the several versions of Smalltalk that we did used internals that were variants of the internals of both Lisp and Euler.)\n\nThe peculiar organization of Lisp — once you got used to it — made deep fundamental things easier to think about than Euler (and the general Algolish approaches of the day).\n\nOne thing that stands out is that about half the tiny McCarthy interpreter is only there because he was thinking about functions that evaluate their arguments before the lambda form is “applied” to them. This requires forms of quoting to delay evaling for the “special forms” of QUOTE, COND, LAMBDA, etc. But you don’t need that, and the MIT Lisp 1.5 people had already introduced the idea of an FEXPR, which doesn’t eval its arguments at call time, but simply is handed the argument list. The FEXPRs can then eval parameters if needed and desired (this allows e.g. “COND” to just be a vanilla FEXPR).\n\nAlso, Lisp raised the sibling question in a thinkable way about passing a lambda with free (global) variables.\n\n* What does this mean in the new environment? Look for them dynamically in the new environment? Can be useful.\n* Bind the globals to their values in the calling environment and package the lambda with its own a-list? Very useful (and very like a Simula-style object).\n* Search for the values of the globals dynamically in the calling environment? Also very useful, and rather like the *thunk* ideas in Algol for “calling by name”.\n\nLisp also had a second set of interesting ideas that were “loosely” (meaning “not really”) integrated into the rest of the language: the property lists on atoms. If you were thinking “Sketchpad” or “Simula” thoughts, and looked at Lisp, you would see some of what you wanted in closures and some of what you wanted in property lists.\n\nThe term “object” in the 60s meant “a compound data structure with fields”, and this was another way to use property lists. Since you could stash procedures on the property list along with anything else you wanted, you could make a whole little world. If you had the procedures have a global variable that contained a pointer to the property list the procedure was part of, then quite a bit could be done.\n\nInterestingly, with this aspect of Lisp, the way it was done made it harder to “think good thoughts” than was helpful.\n\nThe offspring of Peter’s PDP-1 Lisp was BBN Lisp (1.85) and it supplied “all of the above”, and much more.\n\nAnother interesting dynamic back then was Warren Teitelman’s thesis “Pilot”, his heroic pass at McCarthy’s “Advice Taker” via meta programming rather than AI. Many of these ideas were incorporated in BBN Lisp when Teitelman came to BBN. (This and more became Interlisp.)\n\nAnd then there were more and more really interesting AI systems — many of them thesis projects — that were done in Lisp.\n\nI thought that the key principle here was that Lisp allowed you to learn more — in part by being able to try more and change your mind more — than any other programming system. This was especially the case when you needed something that the bottom of the system didn’t have — the meta reflective properties of Lisp gave you more recourses to *still* do something that would help.\n\nIn a world in which almost nothing was known about programming — even by the best programmers — this was really huge.\n\nI don’t think a lot more is known today about programming than then, though Parc did make a few important advances. But many of the advances happened precisely because Smalltalk manifested its ideas about “instantiating intercommunicating processes” by using Lisp techniques. If I had to do it again, I would retain much of this approach, but would do a very different design in place of “Lisp sharing”."
      },
      "after": "151c87f8e17041da",
      "date": 1707811813239
    }
  ]
}