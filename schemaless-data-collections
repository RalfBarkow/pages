{
  "title": "Schemaless Data Collections",
  "story": [
    {
      "type": "paragraph",
      "id": "164bf871cb65a2fd",
      "text": "JSON data collections are usually schemaless. "
    },
    {
      "type": "paragraph",
      "id": "ce12f4a5f22374c0",
      "text": "This enables applications to be quickly deployed without waiting for a schema to be specified and makes them resilient to data irregularity. Unfortunately, the lack of a schema makes it impossible to statically detect any mismatch between the actual structure of data and the structure expected by complex queries and programs; furthermore, programmers cannot use a schema description to ease the production of correct code, and schema-based optimizations are not possible."
    },
    {
      "type": "markdown",
      "id": "66cabf19c8ba2a08",
      "text": "We deal with the problem of inferring a schema from massive JSON datasets. Our main goal in this work is to infer structural properties of JSON data, that is, a description of the structure of JSON objects and arrays that take into account nested values, optional keys, and any other kind of structural variations. These are the main properties that characterize semi-structured data, and having a tool that ensures *fast*, *precise*, and *concise* inference is crucial in modern applications characterized by the agile consumption of huge amounts of data coming from multiple and disparate sources.\n"
    },
    {
      "type": "markdown",
      "id": "092e7ec950c88679",
      "text": "The approach we propose here is based on a JSON schema language and on an efficient, parametric, inference algorithm. The schema language is able to capture detailed structural information about input data despite the presence of any irregularity and can express that information at different levels of abstraction. This language resembles and borrows mechanisms from existing proposals [30], but it has the advantage to be simple yet very expressive.\n"
    },
    {
      "type": "markdown",
      "id": "d65ce36aa4a8e1b2",
      "text": "The algorithm is designed for an optimized map-reduce implementation, in order to be applicable to massive datasets. It is based on the parallel extraction of a schema for each data item and on the fusion of those schemas that are equivalent during the Reduce phase, according to an equivalence relation, which is a parameter of the algorithm. The equivalence relation specifies which properties types must enjoy to be fused together, e.g., one can decide to fuse record types having exactly the same set of labels, or to fuse record types sharing just a common core of fields; hence, a different equivalence relation leads to a different balance of precision and compactness. We will prove that, whichever equivalence is chosen, the resulting fusion function enjoys the commutative and associative properties, enabling local aggregation in a map-reduce setting, which is crucial for an efficient execution.\n"
    },
    {
      "type": "markdown",
      "id": "33c385c3155c68bc",
      "text": "The parametrization is a central feature of our approach. In this paper, we present some different equivalence relations, to illustrate the flexibility of the approach, and focus on the two equivalences that have the maximal practical interest. These equivalences differ in the way record types are fused. While the first one fuses any two record types, by marking as ‘optional’ those fields that are not mandatory in both, the second one only fuses record types that share the same set of labels. So, while in the first case we obtain very compact schemas, in the second case we obtain a schema that is potentially much bigger, but where field correlation information is preserved.\n"
    },
    {
      "type": "markdown",
      "id": "19db2cbe38c94a92",
      "text": "In a typical scenario, a programmer or data analyst will first run the most compact version in order to gain a general view of the data structure and will later use a less abstract version in order to get a more complete knowledge of the structural variations. When the first equivalence is used, the resulting schema has usually a size that is small enough to enable a user to consult it in a reasonable amount of time, in order to get a global knowledge of the structural and type properties of the JSON collection, while the second equivalence may generate, depending on the regularity of the data, a schema that is quite bigger.\n"
    },
    {
      "type": "markdown",
      "id": "fd123fad6d981747",
      "text": "The generated schema is in any case *path covering*, in the sense that each path that can be traversed in the tree structure of the input JSON value can be traversed in the inferred schema as well. This property is crucial to enable a series of query optimization tasks. For instance, thanks to this property JSON queries [8,35] can be optimized at compile time by means of schema-based path rewriting and wildcard expansion [26] or schema-based projection [3,6]. These optimizations are not possible if the schema hides some of the structural properties of the data, as happens in related approaches [36].\n"
    },
    {
      "type": "markdown",
      "id": "cc4b1877d49ac3df",
      "text": "Even in its most compact version, our inferred schema precisely captures the presence of optional and mandatory fields in a collection of JSON records, so that the user has already a clear knowledge about (i) all possible fields of records, (ii) a distinction between optional and mandatory ones. When the schema is expanded to a more precise version, the user can also know which sets of optional fields do, or do not, co-occur in some records."
    },
    {
      "type": "pagefold",
      "id": "71bd66fbe7639725",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "96b4c38b30bf9f8b",
      "text": "BAAZIZI, Mohamed-Amine, COLAZZO, Dario, GHELLI, Giorgio and SARTIANI, Carlo, 2019. Parametric schema inference for massive JSON datasets. The VLDB Journal. 1 August 2019. Vol. 28, no. 4, p. 497–521. DOI 10.1007/s00778-018-0532-7. "
    },
    {
      "type": "markdown",
      "id": "05ea200e4300dcce",
      "text": "> In recent years, JSON established itself as a very popular data format for representing massive data collections. JSON data collections are usually schemaless. While this ensures several advantages, the absence of schema information has important negative consequences as well: Data analysts and programmers cannot exploit a schema for a reliable description of the structure of the dataset, the correctness of complex queries and programs cannot be statically checked, and many schema-based optimizations are not possible. In this paper, we deal with the problem of inferring a schema from massive JSON datasets. We first identify a JSON type language which is simple and, at the same time, expressive enough to capture irregularities and to give complete structural information about input data. We then present our contributions, which are the design of a parametric and parallelizable schema inference algorithm, its theoretical study, and its implementation based on Spark, enabling reasonable schema inference time for massive collections. Our algorithm is parametric as the analyst can specify a parameter determining the level of precision and conciseness of the inferred schema. Finally, we report about an experimental analysis showing the effectiveness of our approach in terms of execution time, conciseness of inferred schemas, and scalability.\n"
    },
    {
      "type": "paragraph",
      "id": "9b7698da5b444fb2",
      "text": "30. Pezoa, F., Reutter, J.L., Suarez, F., Ugarte, M., Vrgoˇ c, D.: Foundations of JSON Schema. In: WWW ’16, pp. 263–273 (2016)"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Schemaless Data Collections",
        "story": []
      },
      "date": 1664348290414
    },
    {
      "id": "164bf871cb65a2fd",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "164bf871cb65a2fd",
        "text": "JSON data collections are usually schemaless. (Baazizi u. a., „Parametric Schema Inference for Massive JSON Datasets“.)"
      },
      "date": 1664348293134
    },
    {
      "item": {
        "type": "factory",
        "id": "96b4c38b30bf9f8b"
      },
      "id": "96b4c38b30bf9f8b",
      "type": "add",
      "after": "164bf871cb65a2fd",
      "date": 1664348307414
    },
    {
      "type": "edit",
      "id": "96b4c38b30bf9f8b",
      "item": {
        "type": "paragraph",
        "id": "96b4c38b30bf9f8b",
        "text": "\nBAAZIZI, Mohamed-Amine, COLAZZO, Dario, GHELLI, Giorgio and SARTIANI, Carlo, 2019. Parametric schema inference for massive JSON datasets. The VLDB Journal. 1 August 2019. Vol. 28, no. 4, p. 497–521. DOI 10.1007/s00778-018-0532-7. In recent years, JSON established itself as a very popular data format for representing massive data collections. JSON data collections are usually schemaless. While this ensures several advantages, the absence of schema information has important negative consequences as well: Data analysts and programmers cannot exploit a schema for a reliable description of the structure of the dataset, the correctness of complex queries and programs cannot be statically checked, and many schema-based optimizations are not possible. In this paper, we deal with the problem of inferring a schema from massive JSON datasets. We first identify a JSON type language which is simple and, at the same time, expressive enough to capture irregularities and to give complete structural information about input data. We then present our contributions, which are the design of a parametric and parallelizable schema inference algorithm, its theoretical study, and its implementation based on Spark, enabling reasonable schema inference time for massive collections. Our algorithm is parametric as the analyst can specify a parameter determining the level of precision and conciseness of the inferred schema. Finally, we report about an experimental analysis showing the effectiveness of our approach in terms of execution time, conciseness of inferred schemas, and scalability.\n"
      },
      "date": 1664348310384
    },
    {
      "type": "edit",
      "id": "96b4c38b30bf9f8b",
      "item": {
        "type": "paragraph",
        "id": "96b4c38b30bf9f8b",
        "text": "BAAZIZI, Mohamed-Amine, COLAZZO, Dario, GHELLI, Giorgio and SARTIANI, Carlo, 2019. Parametric schema inference for massive JSON datasets. The VLDB Journal. 1 August 2019. Vol. 28, no. 4, p. 497–521. DOI 10.1007/s00778-018-0532-7. "
      },
      "date": 1664348317942
    },
    {
      "type": "add",
      "id": "05ea200e4300dcce",
      "item": {
        "type": "paragraph",
        "id": "05ea200e4300dcce",
        "text": "> In recent years, JSON established itself as a very popular data format for representing massive data collections. JSON data collections are usually schemaless. While this ensures several advantages, the absence of schema information has important negative consequences as well: Data analysts and programmers cannot exploit a schema for a reliable description of the structure of the dataset, the correctness of complex queries and programs cannot be statically checked, and many schema-based optimizations are not possible. In this paper, we deal with the problem of inferring a schema from massive JSON datasets. We first identify a JSON type language which is simple and, at the same time, expressive enough to capture irregularities and to give complete structural information about input data. We then present our contributions, which are the design of a parametric and parallelizable schema inference algorithm, its theoretical study, and its implementation based on Spark, enabling reasonable schema inference time for massive collections. Our algorithm is parametric as the analyst can specify a parameter determining the level of precision and conciseness of the inferred schema. Finally, we report about an experimental analysis showing the effectiveness of our approach in terms of execution time, conciseness of inferred schemas, and scalability.\n"
      },
      "after": "96b4c38b30bf9f8b",
      "date": 1664348319134
    },
    {
      "type": "edit",
      "id": "05ea200e4300dcce",
      "item": {
        "type": "markdown",
        "id": "05ea200e4300dcce",
        "text": "> In recent years, JSON established itself as a very popular data format for representing massive data collections. JSON data collections are usually schemaless. While this ensures several advantages, the absence of schema information has important negative consequences as well: Data analysts and programmers cannot exploit a schema for a reliable description of the structure of the dataset, the correctness of complex queries and programs cannot be statically checked, and many schema-based optimizations are not possible. In this paper, we deal with the problem of inferring a schema from massive JSON datasets. We first identify a JSON type language which is simple and, at the same time, expressive enough to capture irregularities and to give complete structural information about input data. We then present our contributions, which are the design of a parametric and parallelizable schema inference algorithm, its theoretical study, and its implementation based on Spark, enabling reasonable schema inference time for massive collections. Our algorithm is parametric as the analyst can specify a parameter determining the level of precision and conciseness of the inferred schema. Finally, we report about an experimental analysis showing the effectiveness of our approach in terms of execution time, conciseness of inferred schemas, and scalability.\n"
      },
      "date": 1664348320470
    },
    {
      "item": {
        "type": "factory",
        "id": "71bd66fbe7639725"
      },
      "id": "71bd66fbe7639725",
      "type": "add",
      "after": "05ea200e4300dcce",
      "date": 1664348323380
    },
    {
      "id": "71bd66fbe7639725",
      "type": "move",
      "order": [
        "164bf871cb65a2fd",
        "71bd66fbe7639725",
        "96b4c38b30bf9f8b",
        "05ea200e4300dcce"
      ],
      "date": 1664348326741
    },
    {
      "type": "edit",
      "id": "71bd66fbe7639725",
      "item": {
        "type": "pagefold",
        "id": "71bd66fbe7639725",
        "text": "~"
      },
      "date": 1664348329350
    },
    {
      "type": "edit",
      "id": "164bf871cb65a2fd",
      "item": {
        "type": "paragraph",
        "id": "164bf871cb65a2fd",
        "text": "JSON data collections are usually schemaless. "
      },
      "date": 1664348334465
    },
    {
      "type": "add",
      "id": "ce12f4a5f22374c0",
      "item": {
        "type": "paragraph",
        "id": "ce12f4a5f22374c0",
        "text": "This enables applications to be quickly deployed without waiting for a schema to be specified and makes them resilient to data irregularity. Unfortunately, the lack of a schema makes it impossible to statically detect any mismatch between the actual structure of data and the structure expected by complex queries and programs; furthermore, programmers cannot use a schema description to ease the production of correct code, and schema-based optimizations are not possible."
      },
      "after": "164bf871cb65a2fd",
      "date": 1664348395076
    },
    {
      "type": "add",
      "id": "66cabf19c8ba2a08",
      "item": {
        "type": "paragraph",
        "id": "66cabf19c8ba2a08",
        "text": "We deal with the problem of inferring a schema from massive JSON datasets. Our main goal in this work is to infer structural properties of JSON data, that is, a"
      },
      "after": "ce12f4a5f22374c0",
      "date": 1664348447162
    },
    {
      "type": "edit",
      "id": "66cabf19c8ba2a08",
      "item": {
        "type": "paragraph",
        "id": "66cabf19c8ba2a08",
        "text": "We deal with the problem of inferring a schema from massive JSON datasets. Our main goal in this work is to infer structural properties of JSON data, that is, a description of the structure of JSON objects and arrays that take into account nested values, optional keys, and any other kind of structural variations. These are the main properties that characterize semi-structured data, and having a tool that ensures fast, precise, and concise inference is crucial in modern applications characterized by the agile consumption of huge amounts of data coming from multiple and disparate sources."
      },
      "date": 1664348458377
    },
    {
      "type": "edit",
      "id": "66cabf19c8ba2a08",
      "item": {
        "type": "paragraph",
        "id": "66cabf19c8ba2a08",
        "text": "We deal with the problem of inferring a schema from massive JSON datasets. Our main goal in this work is to infer structural properties of JSON data, that is, a description of the structure of JSON objects and arrays that take into account nested values, optional keys, and any other kind of structural variations. These are the main properties that characterize semi-structured data, and having a tool that ensures *fast*, *precise*, and *concise* inference is crucial in modern applications characterized by the agile consumption of huge amounts of data coming from multiple and disparate sources."
      },
      "date": 1664348500729
    },
    {
      "type": "edit",
      "id": "66cabf19c8ba2a08",
      "item": {
        "type": "markdown",
        "id": "66cabf19c8ba2a08",
        "text": "We deal with the problem of inferring a schema from massive JSON datasets. Our main goal in this work is to infer structural properties of JSON data, that is, a description of the structure of JSON objects and arrays that take into account nested values, optional keys, and any other kind of structural variations. These are the main properties that characterize semi-structured data, and having a tool that ensures *fast*, *precise*, and *concise* inference is crucial in modern applications characterized by the agile consumption of huge amounts of data coming from multiple and disparate sources."
      },
      "date": 1664348502235
    },
    {
      "type": "edit",
      "id": "66cabf19c8ba2a08",
      "item": {
        "type": "markdown",
        "id": "66cabf19c8ba2a08",
        "text": "We deal with the problem of inferring a schema from massive JSON datasets. Our main goal in this work is to infer structural properties of JSON data, that is, a description of the structure of JSON objects and arrays that take into account nested values, optional keys, and any other kind of structural variations. These are the main properties that characterize semi-structured data, and having a tool that ensures *fast*, *precise*, and *concise* inference is crucial in modern applications characterized by the agile consumption of huge amounts of data coming from multiple and disparate sources.\n"
      },
      "date": 1664348541608
    },
    {
      "type": "add",
      "id": "092e7ec950c88679",
      "item": {
        "type": "markdown",
        "id": "092e7ec950c88679",
        "text": "The approach we propose here is based on a JSON schema language and on an efficient, parametric, inference algorithm. The schema language is able to capture detailed structural information about input data despite the presence of any irregularity and can express that information at different levels of abstraction. This language resembles and borrows mechanisms from existing proposals [30], but it has the advantage to be simple yet very expressive."
      },
      "after": "66cabf19c8ba2a08",
      "date": 1664348542144
    },
    {
      "item": {
        "type": "factory",
        "id": "9b7698da5b444fb2"
      },
      "id": "9b7698da5b444fb2",
      "type": "add",
      "after": "05ea200e4300dcce",
      "date": 1664348574749
    },
    {
      "type": "edit",
      "id": "9b7698da5b444fb2",
      "item": {
        "type": "paragraph",
        "id": "9b7698da5b444fb2",
        "text": "30. Pezoa, F., Reutter, J.L., Suarez, F., Ugarte, M., Vrgoˇ c, D.: Foundations of JSON Schema. In: WWW ’16, pp. 263–273 (2016)"
      },
      "date": 1664348576517
    },
    {
      "type": "edit",
      "id": "092e7ec950c88679",
      "item": {
        "type": "markdown",
        "id": "092e7ec950c88679",
        "text": "The approach we propose here is based on a JSON schema language and on an efficient, parametric, inference algorithm. The schema language is able to capture detailed structural information about input data despite the presence of any irregularity and can express that information at different levels of abstraction. This language resembles and borrows mechanisms from existing proposals [30], but it has the advantage to be simple yet very expressive.\n"
      },
      "date": 1664348628190
    },
    {
      "type": "add",
      "id": "d65ce36aa4a8e1b2",
      "item": {
        "type": "markdown",
        "id": "d65ce36aa4a8e1b2",
        "text": "The algorithm is designed for an optimized map-reduce implementation, in order to be applicable to massive datasets. It is based on the parallel extraction of a schema for each data item and on the fusion of those schemas that are equivalent during the Reduce phase, according to an equivalence relation, which is a parameter of the algorithm. The equivalence relation specifies which properties types must enjoy to be fused together, e.g., one can decide to fuse record types having exactly the same set of labels, or to fuse record types sharing just a common core of fields; hence, a different equivalence relation leads to a different balance of precision and compactness. We will prove that, whichever equivalence is chosen, the resulting fusion function enjoys the commutative and associative properties, enabling local aggregation in a map-reduce setting, which is crucial for an efficient execution."
      },
      "after": "092e7ec950c88679",
      "date": 1664348628820
    },
    {
      "type": "edit",
      "id": "d65ce36aa4a8e1b2",
      "item": {
        "type": "markdown",
        "id": "d65ce36aa4a8e1b2",
        "text": "The algorithm is designed for an optimized map-reduce implementation, in order to be applicable to massive datasets. It is based on the parallel extraction of a schema for each data item and on the fusion of those schemas that are equivalent during the Reduce phase, according to an equivalence relation, which is a parameter of the algorithm. The equivalence relation specifies which properties types must enjoy to be fused together, e.g., one can decide to fuse record types having exactly the same set of labels, or to fuse record types sharing just a common core of fields; hence, a different equivalence relation leads to a different balance of precision and compactness. We will prove that, whichever equivalence is chosen, the resulting fusion function enjoys the commutative and associative properties, enabling local aggregation in a map-reduce setting, which is crucial for an efficient execution.\n"
      },
      "date": 1664348680843
    },
    {
      "type": "add",
      "id": "33c385c3155c68bc",
      "item": {
        "type": "markdown",
        "id": "33c385c3155c68bc",
        "text": "The parametrization is a central feature of our approach. In this paper, we present some different equivalence relations, to illustrate the flexibility of the approach, and focus on the two equivalences that have the maximal practical interest. These equivalences differ in the way record types are fused. While the first one fuses any two record types, by marking as ‘optional’ those fields that are not mandatory in both, the second one only fuses record types that share the same set of labels. So, while in the first case we obtain very compact schemas, in the second case we obtain a schema that is poten-"
      },
      "after": "d65ce36aa4a8e1b2",
      "date": 1664348681414
    },
    {
      "type": "edit",
      "id": "33c385c3155c68bc",
      "item": {
        "type": "markdown",
        "id": "33c385c3155c68bc",
        "text": "The parametrization is a central feature of our approach. In this paper, we present some different equivalence relations, to illustrate the flexibility of the approach, and focus on the two equivalences that have the maximal practical interest. These equivalences differ in the way record types are fused. While the first one fuses any two record types, by marking as ‘optional’ those fields that are not mandatory in both, the second one only fuses record types that share the same set of labels. So, while in the first case we obtain very compact schemas, in the second case we obtain a schema that is potentially much bigger, but where field correlation information is preserved."
      },
      "date": 1664348719272
    },
    {
      "type": "edit",
      "id": "33c385c3155c68bc",
      "item": {
        "type": "markdown",
        "id": "33c385c3155c68bc",
        "text": "The parametrization is a central feature of our approach. In this paper, we present some different equivalence relations, to illustrate the flexibility of the approach, and focus on the two equivalences that have the maximal practical interest. These equivalences differ in the way record types are fused. While the first one fuses any two record types, by marking as ‘optional’ those fields that are not mandatory in both, the second one only fuses record types that share the same set of labels. So, while in the first case we obtain very compact schemas, in the second case we obtain a schema that is potentially much bigger, but where field correlation information is preserved.\n"
      },
      "date": 1664348744531
    },
    {
      "type": "add",
      "id": "19db2cbe38c94a92",
      "item": {
        "type": "markdown",
        "id": "19db2cbe38c94a92",
        "text": "In a typical scenario, a programmer or data analyst will first run the most compact version in order to gain a general view of the data structure and will later use a less abstract version in order to get a more complete knowledge of the structural variations. When the first equivalence is used, the resulting schema has usually a size that is small enough to enable a user to consult it in a reasonable amount of time, in order to get a global knowledge of the structural and type properties of the JSON collection, while the second equivalence may generate, depending on the regularity of the data, a schema that is quite bigger."
      },
      "after": "33c385c3155c68bc",
      "date": 1664348745127
    },
    {
      "type": "edit",
      "id": "19db2cbe38c94a92",
      "item": {
        "type": "markdown",
        "id": "19db2cbe38c94a92",
        "text": "In a typical scenario, a programmer or data analyst will first run the most compact version in order to gain a general view of the data structure and will later use a less abstract version in order to get a more complete knowledge of the structural variations. When the first equivalence is used, the resulting schema has usually a size that is small enough to enable a user to consult it in a reasonable amount of time, in order to get a global knowledge of the structural and type properties of the JSON collection, while the second equivalence may generate, depending on the regularity of the data, a schema that is quite bigger.\n"
      },
      "date": 1664348802063
    },
    {
      "type": "add",
      "id": "fd123fad6d981747",
      "item": {
        "type": "markdown",
        "id": "fd123fad6d981747",
        "text": "The generated schema is in any case path covering,in the sense that each path that can be traversed in the tree structure of the input JSON value can be traversed in the inferred schema as well. This property is crucial to enable a series of query optimization tasks. For instance, thanks to this property JSON queries [8,35] can be optimized at compile time by means of schema-based path rewriting and wildcard expansion [26] or schema-based projection [3,6]. These optimizations are not possible if the schema hides some of the structural properties of the data, as happens in related approaches [36]."
      },
      "after": "19db2cbe38c94a92",
      "date": 1664348802700
    },
    {
      "type": "edit",
      "id": "fd123fad6d981747",
      "item": {
        "type": "markdown",
        "id": "fd123fad6d981747",
        "text": "The generated schema is in any case *path covering*, in the sense that each path that can be traversed in the tree structure of the input JSON value can be traversed in the inferred schema as well. This property is crucial to enable a series of query optimization tasks. For instance, thanks to this property JSON queries [8,35] can be optimized at compile time by means of schema-based path rewriting and wildcard expansion [26] or schema-based projection [3,6]. These optimizations are not possible if the schema hides some of the structural properties of the data, as happens in related approaches [36]."
      },
      "date": 1664348815806
    },
    {
      "type": "edit",
      "id": "fd123fad6d981747",
      "item": {
        "type": "markdown",
        "id": "fd123fad6d981747",
        "text": "The generated schema is in any case *path covering*, in the sense that each path that can be traversed in the tree structure of the input JSON value can be traversed in the inferred schema as well. This property is crucial to enable a series of query optimization tasks. For instance, thanks to this property JSON queries [8,35] can be optimized at compile time by means of schema-based path rewriting and wildcard expansion [26] or schema-based projection [3,6]. These optimizations are not possible if the schema hides some of the structural properties of the data, as happens in related approaches [36].\n"
      },
      "date": 1664348861101
    },
    {
      "type": "add",
      "id": "cc4b1877d49ac3df",
      "item": {
        "type": "markdown",
        "id": "cc4b1877d49ac3df",
        "text": "Even in its most compact version, our inferred schema precisely captures the presence of optional and mandatory fields in a collection of JSON records, so that the user has already a clear knowledge about (i) all possible fields of records, (ii) a distinction between optional and mandatory ones. When the schema is expanded to a more precise version, the user can also know which sets of optional fields do, or do not, co-occur in some records."
      },
      "after": "fd123fad6d981747",
      "date": 1664348861729
    }
  ]
}