{
  "title": "Review of Kanerva’s SDM",
  "story": [
    {
      "type": "paragraph",
      "id": "af9433394ad15dc9",
      "text": "Here, we present a short overview of SDM. A deeper review on the motivations behind SDM and the features that make it biologically plausible can be found in [13, 15]. SDM provides an algorithm for how memories (patterns) are stored in, and retrieved from, neurons in the brain. There are three primitives that all exist in the space of n dimensional binary vectors: "
    },
    {
      "type": "paragraph",
      "id": "95ff68ce971af2f7",
      "text": "[[Patterns]] (p) - have two components: the pattern address, paμ 2{0, 1}n, is the vector representation of a memory; the pattern “pointer”, ppμ 2{0, 1}n, is bound to the address and points to itself when autoassociative or to a different pattern address when heteroassociative. A heteroassociative example is memorizing the alphabet where the pattern address for the letter a points to pattern address b, b points to c etc. For tractability in analyzing SDM, we assume our pattern addresses and pointers are random. There are m patterns and they are indexed by the superscript μ 2{1,...,m}."
    },
    {
      "type": "paragraph",
      "id": "fbae26741bda5e7a",
      "text": "Neurons (x) - in showing SDM’s relationship to [[Attention]] it is sufficient to know there are r neurons with fixed addresses x⌧a 2{0, 1}n that store a set of all patterns written to them. Each neuron will sum over its set of patterns to create a [[Superposition]]. This creates minimal noise interference between patterns because of the high dimensional nature of the vector space and enables all patterns to be stored in an n dimensional storage vector denoted x⌧v 2 Zn+, constrained to the positive integers. Their biologically plausible features are outlined in [13, 15]. When we assume our patterns are random, we also assume our neuron addresses are randomly distributed. Of the 2n possible vectors in our binary vector space, SDM is “sparse” because it assumes that r ⌧ 2n neurons exist in the space."
    },
    {
      "type": "paragraph",
      "id": "a6d715242325e9ba",
      "text": "Query (⇠) - is the input to SDM, denoted ⇠ 2{0, 1}n. The goal in the Best Match Problem is to return the pattern pointer stored at the closest pattern address to the query. We will often care about the maximum noise corruption that can be applied to our query, while still having it read out the correct pattern. An autoassociative example is wanting to recognize familiar faces in poor lighting. Images of faces we have seen before are patterns stored in memory and our query is a noisy representation of one of the faces. We want SDM to return the noise-free version of the queried face, assuming it is stored in memory."
    },
    {
      "type": "paragraph",
      "id": "65b77d30e9e915dc",
      "text": "SDM uses the [[Hamming Distance]] metric between any two vectors defined: d(a, b) := 1Tn |a b|. The all ones vector 1n is of n dimensions and |a b| takes the absolute value of the element-wise difference between the binary vectors. When it is clear what two vectors the Hamming distance is between, we will sometimes use the shorthand dv := d(a, b)."
    },
    {
      "type": "paragraph",
      "id": "fbdb2ae8823aff11",
      "text": "[…]"
    },
    {
      "type": "pagefold",
      "id": "b19efd9f18f3d018",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "00cde83235e491ac",
      "text": "pattern pointer ⇒"
    },
    {
      "type": "reference",
      "id": "dfa846c42847a792",
      "site": "wiki.ralfbarkow.ch",
      "slug": "pile-systems",
      "title": "Pile Systems",
      "text": "Pile Systems Inc. [https://web.archive.org/web/20010429154816/http://www.pilesys.com/ wayback]"
    },
    {
      "type": "reference",
      "id": "1f7b7afe010509fb",
      "site": "ward.asia.wiki.org",
      "slug": "escaping-addressing",
      "title": "Escaping Addressing",
      "text": "Address busses works well and are at the heart of both processor and memory design on a variety of scales. Address busses make computers a logical machine for when they are properly clocked we can reason knowing all elements have been considered. But this pattern is rare or nonexistent in nature. Let's understand why."
    },
    {
      "type": "paragraph",
      "id": "7821f0c9f1a228d5",
      "text": "⇒ [[Hamming Distance]]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Review of Kanerva’s SDM",
        "story": []
      },
      "date": 1673854854572
    },
    {
      "item": {
        "type": "factory",
        "id": "af9433394ad15dc9"
      },
      "id": "af9433394ad15dc9",
      "type": "add",
      "date": 1673854856062
    },
    {
      "type": "edit",
      "id": "af9433394ad15dc9",
      "item": {
        "type": "paragraph",
        "id": "af9433394ad15dc9",
        "text": "Here, we present a short overview of SDM. A deeper review on the motivations behind SDM and the features that make it biologically plausible can be found in [13, 15]. SDM provides an algorithm for how memories (patterns) are stored in, and retrieved from, neurons in the brain. There are three primitives that all exist in the space of n dimensional binary vectors: […]"
      },
      "date": 1673854860969
    },
    {
      "type": "edit",
      "id": "af9433394ad15dc9",
      "item": {
        "type": "paragraph",
        "id": "af9433394ad15dc9",
        "text": "Here, we present a short overview of SDM. A deeper review on the motivations behind SDM and the features that make it biologically plausible can be found in [13, 15]. SDM provides an algorithm for how memories (patterns) are stored in, and retrieved from, neurons in the brain. There are three primitives that all exist in the space of n dimensional binary vectors: "
      },
      "date": 1673854896182
    },
    {
      "type": "add",
      "id": "fbdb2ae8823aff11",
      "item": {
        "type": "paragraph",
        "id": "fbdb2ae8823aff11",
        "text": "[…]"
      },
      "after": "af9433394ad15dc9",
      "date": 1673854899072
    },
    {
      "type": "add",
      "id": "95ff68ce971af2f7",
      "item": {
        "type": "paragraph",
        "id": "95ff68ce971af2f7",
        "text": "Patterns (p) - have two components: the pattern address, paμ 2{0, 1}n, is the vector representation of a memory; the pattern “pointer”, ppμ 2{0, 1}n, is bound to the address and points to itself when autoassociative or to a different pattern address when heteroassociative. A heteroassociative example is memorizing the alphabet where the pattern address for the letter a points to pattern address b, b points to c etc. For tractability in analyzing SDM, we assume our pattern addresses and pointers are random. There are m patterns and they are indexed by the superscript μ 2{1,...,m}."
      },
      "after": "af9433394ad15dc9",
      "date": 1673854900996
    },
    {
      "item": {
        "type": "factory",
        "id": "b19efd9f18f3d018"
      },
      "id": "b19efd9f18f3d018",
      "type": "add",
      "after": "fbdb2ae8823aff11",
      "date": 1673854934100
    },
    {
      "type": "edit",
      "id": "b19efd9f18f3d018",
      "item": {
        "type": "pagefold",
        "id": "b19efd9f18f3d018",
        "text": "~"
      },
      "date": 1673854937204
    },
    {
      "item": {
        "type": "factory",
        "id": "dfa846c42847a792"
      },
      "id": "dfa846c42847a792",
      "type": "add",
      "after": "b19efd9f18f3d018",
      "date": 1673854939242
    },
    {
      "type": "edit",
      "id": "dfa846c42847a792",
      "item": {
        "type": "reference",
        "id": "dfa846c42847a792",
        "site": "wiki.ralfbarkow.ch",
        "slug": "pile-systems",
        "title": "Pile Systems",
        "text": "Pile Systems Inc. [https://web.archive.org/web/20010429154816/http://www.pilesys.com/ wayback]"
      },
      "date": 1673854941894
    },
    {
      "type": "add",
      "id": "fbae26741bda5e7a",
      "item": {
        "type": "paragraph",
        "id": "fbae26741bda5e7a",
        "text": "Neurons (x) - in showing SDM’s relationship to Attention it is sufficient to know there are r neurons with fixed addresses x⌧a 2{0, 1}n that store a set of all patterns written to them. Each neuron will sum over its set of patterns to create a superposition. This creates minimal noise interference between patterns because of the high dimensional nature of the vector space and enables all patterns to be stored in an n dimensional storage vector denoted x⌧v 2 Zn+, constrained to the positive integers. Their biologically plausible features are outlined in [13, 15]. When we assume our patterns are random, we also assume our neuron addresses are randomly distributed. Of the 2n possible vectors in our binary vector space, SDM is “sparse” because it assumes that r ⌧ 2n neurons exist in the space."
      },
      "after": "95ff68ce971af2f7",
      "date": 1673854977112
    },
    {
      "type": "edit",
      "id": "95ff68ce971af2f7",
      "item": {
        "type": "paragraph",
        "id": "95ff68ce971af2f7",
        "text": "[[Patterns]] (p) - have two components: the pattern address, paμ 2{0, 1}n, is the vector representation of a memory; the pattern “pointer”, ppμ 2{0, 1}n, is bound to the address and points to itself when autoassociative or to a different pattern address when heteroassociative. A heteroassociative example is memorizing the alphabet where the pattern address for the letter a points to pattern address b, b points to c etc. For tractability in analyzing SDM, we assume our pattern addresses and pointers are random. There are m patterns and they are indexed by the superscript μ 2{1,...,m}."
      },
      "date": 1673854990149
    },
    {
      "type": "edit",
      "id": "fbae26741bda5e7a",
      "item": {
        "type": "paragraph",
        "id": "fbae26741bda5e7a",
        "text": "Neurons (x) - in showing SDM’s relationship to [[Attention]] it is sufficient to know there are r neurons with fixed addresses x⌧a 2{0, 1}n that store a set of all patterns written to them. Each neuron will sum over its set of patterns to create a superposition. This creates minimal noise interference between patterns because of the high dimensional nature of the vector space and enables all patterns to be stored in an n dimensional storage vector denoted x⌧v 2 Zn+, constrained to the positive integers. Their biologically plausible features are outlined in [13, 15]. When we assume our patterns are random, we also assume our neuron addresses are randomly distributed. Of the 2n possible vectors in our binary vector space, SDM is “sparse” because it assumes that r ⌧ 2n neurons exist in the space."
      },
      "date": 1673855061882
    },
    {
      "type": "edit",
      "id": "fbae26741bda5e7a",
      "item": {
        "type": "paragraph",
        "id": "fbae26741bda5e7a",
        "text": "Neurons (x) - in showing SDM’s relationship to [[Attention]] it is sufficient to know there are r neurons with fixed addresses x⌧a 2{0, 1}n that store a set of all patterns written to them. Each neuron will sum over its set of patterns to create a [[Superposition]]. This creates minimal noise interference between patterns because of the high dimensional nature of the vector space and enables all patterns to be stored in an n dimensional storage vector denoted x⌧v 2 Zn+, constrained to the positive integers. Their biologically plausible features are outlined in [13, 15]. When we assume our patterns are random, we also assume our neuron addresses are randomly distributed. Of the 2n possible vectors in our binary vector space, SDM is “sparse” because it assumes that r ⌧ 2n neurons exist in the space."
      },
      "date": 1673855084113
    },
    {
      "type": "add",
      "id": "a6d715242325e9ba",
      "item": {
        "type": "paragraph",
        "id": "a6d715242325e9ba",
        "text": "Query (⇠) - is the input to SDM, denoted ⇠ 2{0, 1}n. The goal in the Best Match Problem is to return the pattern pointer stored at the closest pattern address to the query. We will often care about the maximum noise corruption that can be applied to our query, while still having it read out the correct pattern. An autoassociative example is wanting to recognize familiar faces in poor lighting. Images of faces we have seen before are patterns stored in memory and our query is a noisy representation of one of the faces. We want SDM to return the noise-free version of the queried face, assuming it is stored in memory."
      },
      "after": "fbae26741bda5e7a",
      "date": 1673855129597
    },
    {
      "item": {
        "type": "factory",
        "id": "00cde83235e491ac"
      },
      "id": "00cde83235e491ac",
      "type": "add",
      "after": "dfa846c42847a792",
      "date": 1673855164508
    },
    {
      "type": "edit",
      "id": "00cde83235e491ac",
      "item": {
        "type": "paragraph",
        "id": "00cde83235e491ac",
        "text": "pattern pointer ⇒"
      },
      "date": 1673855171834
    },
    {
      "id": "00cde83235e491ac",
      "type": "move",
      "order": [
        "af9433394ad15dc9",
        "95ff68ce971af2f7",
        "fbae26741bda5e7a",
        "a6d715242325e9ba",
        "fbdb2ae8823aff11",
        "b19efd9f18f3d018",
        "00cde83235e491ac",
        "dfa846c42847a792"
      ],
      "date": 1673855173382
    },
    {
      "item": {
        "type": "factory",
        "id": "65b77d30e9e915dc"
      },
      "id": "65b77d30e9e915dc",
      "type": "add",
      "after": "dfa846c42847a792",
      "date": 1673855209595
    },
    {
      "id": "65b77d30e9e915dc",
      "type": "move",
      "order": [
        "af9433394ad15dc9",
        "95ff68ce971af2f7",
        "fbae26741bda5e7a",
        "a6d715242325e9ba",
        "65b77d30e9e915dc",
        "fbdb2ae8823aff11",
        "b19efd9f18f3d018",
        "00cde83235e491ac",
        "dfa846c42847a792"
      ],
      "date": 1673855211588
    },
    {
      "type": "edit",
      "id": "65b77d30e9e915dc",
      "item": {
        "type": "paragraph",
        "id": "65b77d30e9e915dc",
        "text": "SDM uses the Hamming distance metric between any two vectors defined: d(a, b) := 1Tn |a b|. The all ones vector 1n is of n dimensions and |a b| takes the absolute value of the element-wise difference between the binary vectors. When it is clear what two vectors the Hamming distance is between, we will sometimes use the shorthand dv := d(a, b)."
      },
      "date": 1673855215829
    },
    {
      "item": {
        "type": "factory",
        "id": "1f7b7afe010509fb"
      },
      "id": "1f7b7afe010509fb",
      "type": "add",
      "after": "dfa846c42847a792",
      "date": 1673855224883
    },
    {
      "type": "edit",
      "id": "1f7b7afe010509fb",
      "item": {
        "type": "reference",
        "id": "1f7b7afe010509fb",
        "site": "ward.asia.wiki.org",
        "slug": "escaping-addressing",
        "title": "Escaping Addressing",
        "text": "Address busses works well and are at the heart of both processor and memory design on a variety of scales. Address busses make computers a logical machine for when they are properly clocked we can reason knowing all elements have been considered. But this pattern is rare or nonexistent in nature. Let's understand why."
      },
      "date": 1673855234995
    },
    {
      "item": {
        "type": "factory",
        "id": "7821f0c9f1a228d5"
      },
      "id": "7821f0c9f1a228d5",
      "type": "add",
      "after": "1f7b7afe010509fb",
      "date": 1673855273457
    },
    {
      "type": "edit",
      "id": "7821f0c9f1a228d5",
      "item": {
        "type": "paragraph",
        "id": "7821f0c9f1a228d5",
        "text": "⇒ [[Hamming Distance]]"
      },
      "date": 1673855282655
    },
    {
      "type": "edit",
      "id": "65b77d30e9e915dc",
      "item": {
        "type": "paragraph",
        "id": "65b77d30e9e915dc",
        "text": "SDM uses the [[Hamming Distance]] metric between any two vectors defined: d(a, b) := 1Tn |a b|. The all ones vector 1n is of n dimensions and |a b| takes the absolute value of the element-wise difference between the binary vectors. When it is clear what two vectors the Hamming distance is between, we will sometimes use the shorthand dv := d(a, b)."
      },
      "date": 1673855314605
    }
  ]
}