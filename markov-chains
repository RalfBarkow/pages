{
  "title": "Markov Chains",
  "story": [
    {
      "type": "paragraph",
      "id": "0e4c2feedfd49d17",
      "text": " A visual explanation by [[Victor Powell]]\n/w text by [[Lewis Lehe]]. [https://setosa.io/blog/2014/07/26/markov-chains/ page] [https://setosa.io/markov/index.html#%7B%22tm%22%3A%5B%5B0.5%2C0.5%5D%2C%5B0.5%2C0.5%5D%5D%7D setosa.io]"
    },
    {
      "type": "paragraph",
      "id": "897ee14749fcdc04",
      "text": "Markov chains, named after [[Andrey Markov]], are mathematical systems that hop from one \"state\" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby's behavior, you might include \"playing,\" \"eating\", \"sleeping,\" and \"crying\" as states, which together with other behaviors could form a 'state space': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or \"transitioning,\" from one state to any other state – e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first."
    },
    {
      "type": "paragraph",
      "id": "8a9a03860550209b",
      "text": "A simple, two-state Markov chain […]. With two states (A and B) in our state space, there are 4 possible transitions (not 2, because a state can transition back into itself). If we're at 'A' we could transition to 'B' or stay at 'A'. If we're at 'B' we could transition to 'A' or stay at 'B'. In this two state diagram, the probability of transitioning from any state to any other state is 0.5."
    },
    {
      "type": "paragraph",
      "id": "935f95521c8862f0",
      "text": "See [[Flip Operation]]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Markov Chains",
        "story": []
      },
      "date": 1638706022540
    },
    {
      "item": {
        "type": "factory",
        "id": "89e32b38828a4f2d"
      },
      "id": "89e32b38828a4f2d",
      "type": "add",
      "date": 1638706030235
    },
    {
      "type": "edit",
      "id": "89e32b38828a4f2d",
      "item": {
        "type": "paragraph",
        "id": "89e32b38828a4f2d",
        "text": "https://setosa.io/blog/2014/07/26/markov-chains/"
      },
      "date": 1638706032113
    },
    {
      "item": {
        "type": "factory",
        "id": "897ee14749fcdc04"
      },
      "id": "897ee14749fcdc04",
      "type": "add",
      "after": "89e32b38828a4f2d",
      "date": 1638706041126
    },
    {
      "type": "edit",
      "id": "897ee14749fcdc04",
      "item": {
        "type": "paragraph",
        "id": "897ee14749fcdc04",
        "text": "Markov chains, named after Andrey Markov, are mathematical systems that hop from one \"state\" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby's behavior, you might include \"playing,\" \"eating\", \"sleeping,\" and \"crying\" as states, which together with other behaviors could form a 'state space': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or \"transitioning,\" from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first."
      },
      "date": 1638706042597
    },
    {
      "item": {
        "type": "factory",
        "id": "0e4c2feedfd49d17"
      },
      "id": "0e4c2feedfd49d17",
      "type": "add",
      "after": "897ee14749fcdc04",
      "date": 1638706052684
    },
    {
      "type": "edit",
      "id": "0e4c2feedfd49d17",
      "item": {
        "type": "paragraph",
        "id": "0e4c2feedfd49d17",
        "text": " A visual explanation by Victor Powell\n/w text by Lewis Lehe "
      },
      "date": 1638706054063
    },
    {
      "id": "0e4c2feedfd49d17",
      "type": "move",
      "order": [
        "0e4c2feedfd49d17",
        "89e32b38828a4f2d",
        "897ee14749fcdc04"
      ],
      "date": 1638706055963
    },
    {
      "type": "edit",
      "id": "89e32b38828a4f2d",
      "item": {
        "type": "paragraph",
        "id": "89e32b38828a4f2d",
        "text": "https://setosa.io/blog/2014/07/26/markov-chains/ page]"
      },
      "date": 1692169635322
    },
    {
      "type": "edit",
      "id": "89e32b38828a4f2d",
      "item": {
        "type": "paragraph",
        "id": "89e32b38828a4f2d",
        "text": "[https://setosa.io/blog/2014/07/26/markov-chains/ page]"
      },
      "date": 1692169639948
    },
    {
      "type": "edit",
      "id": "897ee14749fcdc04",
      "item": {
        "type": "paragraph",
        "id": "897ee14749fcdc04",
        "text": "Markov chains, named after [[Andrey Markov]], are mathematical systems that hop from one \"state\" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby's behavior, you might include \"playing,\" \"eating\", \"sleeping,\" and \"crying\" as states, which together with other behaviors could form a 'state space': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or \"transitioning,\" from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first."
      },
      "date": 1692169669592
    },
    {
      "type": "edit",
      "id": "0e4c2feedfd49d17",
      "item": {
        "type": "paragraph",
        "id": "0e4c2feedfd49d17",
        "text": " A visual explanation by [[Victor Powell]]\n/w text by [[Lewis Lehe]]. "
      },
      "date": 1692169693461
    },
    {
      "type": "remove",
      "id": "89e32b38828a4f2d",
      "date": 1692169698111
    },
    {
      "type": "edit",
      "id": "0e4c2feedfd49d17",
      "item": {
        "type": "paragraph",
        "id": "0e4c2feedfd49d17",
        "text": " A visual explanation by [[Victor Powell]]\n/w text by [[Lewis Lehe]]. [https://setosa.io/blog/2014/07/26/markov-chains/ page]"
      },
      "date": 1692169699760
    },
    {
      "type": "edit",
      "id": "0e4c2feedfd49d17",
      "item": {
        "type": "paragraph",
        "id": "0e4c2feedfd49d17",
        "text": " A visual explanation by [[Victor Powell]]\n/w text by [[Lewis Lehe]]. [https://setosa.io/blog/2014/07/26/markov-chains/ page] [https://setosa.io/markov/index.html#%7B%22tm%22%3A%5B%5B0.5%2C0.5%5D%2C%5B0.5%2C0.5%5D%5D%7D setosa.io]"
      },
      "date": 1707578730011
    },
    {
      "type": "edit",
      "id": "897ee14749fcdc04",
      "item": {
        "type": "paragraph",
        "id": "897ee14749fcdc04",
        "text": "Markov chains, named after [[Andrey Markov]], are mathematical systems that hop from one \"state\" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby's behavior, you might include \"playing,\" \"eating\", \"sleeping,\" and \"crying\" as states, which together with other behaviors could form a 'state space': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or \"transitioning,\" from one state to any other state – e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first."
      },
      "date": 1707578759783
    },
    {
      "item": {
        "type": "factory",
        "id": "8a9a03860550209b"
      },
      "id": "8a9a03860550209b",
      "type": "add",
      "after": "897ee14749fcdc04",
      "date": 1707578791476
    },
    {
      "type": "edit",
      "id": "8a9a03860550209b",
      "item": {
        "type": "paragraph",
        "id": "8a9a03860550209b",
        "text": "A simple, two-state Markov chain […]."
      },
      "date": 1707578796337
    },
    {
      "type": "edit",
      "id": "8a9a03860550209b",
      "item": {
        "type": "paragraph",
        "id": "8a9a03860550209b",
        "text": "A simple, two-state Markov chain […]. With two states (A and B) in our state space, there are 4 possible transitions (not 2, because a state can transition back into itself). If we're at 'A' we could transition to 'B' or stay at 'A'. If we're at 'B' we could transition to 'A' or stay at 'B'. In this two state diagram, the probability of transitioning from any state to any other state is 0.5."
      },
      "date": 1707578819565
    },
    {
      "item": {
        "type": "factory",
        "id": "935f95521c8862f0"
      },
      "id": "935f95521c8862f0",
      "type": "add",
      "after": "8a9a03860550209b",
      "date": 1707578850044
    },
    {
      "type": "edit",
      "id": "935f95521c8862f0",
      "item": {
        "type": "paragraph",
        "id": "935f95521c8862f0",
        "text": "See [[Flip Operation]]"
      },
      "date": 1707578857884
    },
    {
      "type": "fork",
      "site": "wiki.ralfbarkow.ch",
      "date": 1707805559344
    }
  ]
}