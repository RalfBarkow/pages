{
  "title": "BERT",
  "story": [
    {
      "type": "paragraph",
      "id": "a583f895d11ef341",
      "text": "In October of 2018, Google introduced a new method nicknamed [[BERT]] (Bidirectional Encoder Representations from Transformers). It produced a [[GLUE]] score of 80.5."
    },
    {
      "type": "pagefold",
      "id": "63ec2242bc927a36",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "70f0c87837352d77",
      "text": "[[John Pavlus]], Machines Beat Humans on a Reading Test. But Do They Understand? [https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017 post]"
    },
    {
      "type": "pagefold",
      "id": "930b96ae33f84b1a",
      "text": "~",
      "alias": "63ec2242bc927a36"
    },
    {
      "type": "paragraph",
      "id": "4a5341e0d25c60ac",
      "text": "So what exactly is [[BERT]]?\n\nFirst, it’s not a fully trained neural network capable of besting human performance right out of the box. Instead, said Bowman, BERT is “a very precise [[recipe]] for [[pretraining]] a neural network.” Just as a baker can follow a recipe to reliably produce a delicious prebaked pie crust — which can then be used to make many different kinds of pie, from blueberry to spinach quiche — Google researchers developed BERT’s recipe to serve as an ideal foundation for “baking” neural networks (that is, fine-tuning them) to do well on many different natural language processing tasks. Google also open-sourced BERT’s code, which means that other researchers don’t have to repeat the recipe from scratch — they can just download BERT as-is, like buying a prebaked pie crust from the supermarket."
    },
    {
      "type": "paragraph",
      "id": "cca80528df0c5deb",
      "text": "If BERT is essentially a recipe, what’s the ingredient list? “It’s the result of three things coming together to really make things click,” said Omer Levy, a research scientist at Facebook who has analyzed BERT’s inner workings."
    },
    {
      "type": "paragraph",
      "id": "76c7f33c22b0d358",
      "text": "The first is a pretrained language model, those reference books in our Chinese room. The second is the ability to figure out which features of a sentence are most important."
    },
    {
      "type": "paragraph",
      "id": "0fe80856f000d5cf",
      "text": "In 2017, an engineer at Google Brain named [[Jakob Uszkoreit]] was working on ways to accelerate Google’s language-understanding efforts. He noticed that state-of-the-art neural networks also suffered from a built-in constraint: They all looked through the sequence of words one by one. This “sequentiality” seemed to match intuitions of how humans actually read written sentences. But Uszkoreit wondered if “it might be the case that understanding language in a linear, sequential fashion is suboptimal,” he said."
    },
    {
      "type": "paragraph",
      "id": "4603f6881a6dc116",
      "text": "Uszkoreit and his collaborators devised a new architecture for neural networks focused on “[[attention]],” a mechanism that lets each layer of the network assign more weight to some specific features of the input than to others. This new attention-focused architecture, called a [[transformer]], could take a sentence like “a dog bites the man” as input and encode each word in many different ways in parallel. For example, a transformer might connect “bites” and “man” together as verb and object, while ignoring “a”; at the same time, it could connect “bites” and “dog” together as verb and subject, while mostly ignoring “the.”"
    },
    {
      "type": "paragraph",
      "id": "74e69df04f3b8946",
      "text": "The nonsequential nature of the transformer represented sentences in a more expressive form, which Uszkoreit calls treelike. Each layer of the neural network makes multiple, parallel connections between certain words while ignoring others — akin to a student diagramming a sentence in elementary school. These connections are often drawn between words that may not actually sit next to each other in the sentence. “Those structures effectively look like a number of trees that are overlaid,” Uszkoreit explained."
    },
    {
      "type": "paragraph",
      "id": "a6a56153b444db74",
      "text": "This treelike representation of sentences gave transformers a powerful way to model contextual meaning, and also to efficiently learn associations between words that might be far away from each other in complex sentences. “It’s a bit counterintuitive,” Uszkoreit said, “but it is rooted in results from linguistics, which has for a long time looked at [[treelike models of language]].”\n\n"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "BERT",
        "story": []
      },
      "date": 1650971805156
    },
    {
      "item": {
        "type": "factory",
        "id": "a583f895d11ef341"
      },
      "id": "a583f895d11ef341",
      "type": "add",
      "date": 1650971832259
    },
    {
      "type": "edit",
      "id": "a583f895d11ef341",
      "item": {
        "type": "paragraph",
        "id": "a583f895d11ef341",
        "text": "In October of 2018, Google introduced a new method nicknamed [[BERT]] (Bidirectional Encoder Representations from Transformers). It produced a [[GLUE]] score of 80.5."
      },
      "date": 1650971849170
    },
    {
      "item": {
        "type": "factory",
        "id": "4a5341e0d25c60ac"
      },
      "id": "4a5341e0d25c60ac",
      "type": "add",
      "after": "a583f895d11ef341",
      "date": 1650972836031
    },
    {
      "type": "edit",
      "id": "4a5341e0d25c60ac",
      "item": {
        "type": "paragraph",
        "id": "4a5341e0d25c60ac",
        "text": "So what exactly is BERT?\n\nFirst, it’s not a fully trained neural network capable of besting human performance right out of the box. Instead, said Bowman, BERT is “a very precise recipe for pretraining a neural network.” Just as a baker can follow a recipe to reliably produce a delicious prebaked pie crust — which can then be used to make many different kinds of pie, from blueberry to spinach quiche — Google researchers developed BERT’s recipe to serve as an ideal foundation for “baking” neural networks (that is, fine-tuning them) to do well on many different natural language processing tasks. Google also open-sourced BERT’s code, which means that other researchers don’t have to repeat the recipe from scratch — they can just download BERT as-is, like buying a prebaked pie crust from the supermarket."
      },
      "date": 1650972837946
    },
    {
      "type": "edit",
      "id": "4a5341e0d25c60ac",
      "item": {
        "type": "paragraph",
        "id": "4a5341e0d25c60ac",
        "text": "So what exactly is [[BERT]]?\n\nFirst, it’s not a fully trained neural network capable of besting human performance right out of the box. Instead, said Bowman, BERT is “a very precise recipe for pretraining a neural network.” Just as a baker can follow a recipe to reliably produce a delicious prebaked pie crust — which can then be used to make many different kinds of pie, from blueberry to spinach quiche — Google researchers developed BERT’s recipe to serve as an ideal foundation for “baking” neural networks (that is, fine-tuning them) to do well on many different natural language processing tasks. Google also open-sourced BERT’s code, which means that other researchers don’t have to repeat the recipe from scratch — they can just download BERT as-is, like buying a prebaked pie crust from the supermarket."
      },
      "date": 1650972846652
    },
    {
      "type": "edit",
      "id": "4a5341e0d25c60ac",
      "item": {
        "type": "paragraph",
        "id": "4a5341e0d25c60ac",
        "text": "So what exactly is [[BERT]]?\n\nFirst, it’s not a fully trained neural network capable of besting human performance right out of the box. Instead, said Bowman, BERT is “a very precise [[recipe]] for pretraining a neural network.” Just as a baker can follow a recipe to reliably produce a delicious prebaked pie crust — which can then be used to make many different kinds of pie, from blueberry to spinach quiche — Google researchers developed BERT’s recipe to serve as an ideal foundation for “baking” neural networks (that is, fine-tuning them) to do well on many different natural language processing tasks. Google also open-sourced BERT’s code, which means that other researchers don’t have to repeat the recipe from scratch — they can just download BERT as-is, like buying a prebaked pie crust from the supermarket."
      },
      "date": 1650972876478
    },
    {
      "type": "edit",
      "id": "4a5341e0d25c60ac",
      "item": {
        "type": "paragraph",
        "id": "4a5341e0d25c60ac",
        "text": "So what exactly is [[BERT]]?\n\nFirst, it’s not a fully trained neural network capable of besting human performance right out of the box. Instead, said Bowman, BERT is “a very precise [[recipe]] for [[pretraining]] a neural network.” Just as a baker can follow a recipe to reliably produce a delicious prebaked pie crust — which can then be used to make many different kinds of pie, from blueberry to spinach quiche — Google researchers developed BERT’s recipe to serve as an ideal foundation for “baking” neural networks (that is, fine-tuning them) to do well on many different natural language processing tasks. Google also open-sourced BERT’s code, which means that other researchers don’t have to repeat the recipe from scratch — they can just download BERT as-is, like buying a prebaked pie crust from the supermarket."
      },
      "date": 1650972913685
    },
    {
      "item": {
        "type": "factory",
        "id": "cca80528df0c5deb"
      },
      "id": "cca80528df0c5deb",
      "type": "add",
      "after": "4a5341e0d25c60ac",
      "date": 1650990158365
    },
    {
      "type": "edit",
      "id": "cca80528df0c5deb",
      "item": {
        "type": "paragraph",
        "id": "cca80528df0c5deb",
        "text": "If BERT is essentially a recipe, what’s the ingredient list? “It’s the result of three things coming together to really make things click,” said Omer Levy, a research scientist at Facebook who has analyzed BERT’s inner workings."
      },
      "date": 1650990160601
    },
    {
      "item": {
        "type": "factory",
        "id": "76c7f33c22b0d358"
      },
      "id": "76c7f33c22b0d358",
      "type": "add",
      "after": "cca80528df0c5deb",
      "date": 1650990178719
    },
    {
      "type": "edit",
      "id": "76c7f33c22b0d358",
      "item": {
        "type": "paragraph",
        "id": "76c7f33c22b0d358",
        "text": "The first is a pretrained language model, those reference books in our Chinese room. The second is the ability to figure out which features of a sentence are most important."
      },
      "date": 1650990182242
    },
    {
      "item": {
        "type": "factory",
        "id": "0fe80856f000d5cf"
      },
      "id": "0fe80856f000d5cf",
      "type": "add",
      "after": "76c7f33c22b0d358",
      "date": 1650990204569
    },
    {
      "type": "edit",
      "id": "0fe80856f000d5cf",
      "item": {
        "type": "paragraph",
        "id": "0fe80856f000d5cf",
        "text": "In 2017, an engineer at Google Brain named Jakob Uszkoreit was working on ways to accelerate Google’s language-understanding efforts. He noticed that state-of-the-art neural networks also suffered from a built-in constraint: They all looked through the sequence of words one by one. This “sequentiality” seemed to match intuitions of how humans actually read written sentences. But Uszkoreit wondered if “it might be the case that understanding language in a linear, sequential fashion is suboptimal,” he said."
      },
      "date": 1650990206800
    },
    {
      "type": "edit",
      "id": "0fe80856f000d5cf",
      "item": {
        "type": "paragraph",
        "id": "0fe80856f000d5cf",
        "text": "In 2017, an engineer at Google Brain named [[Jakob Uszkoreit]] was working on ways to accelerate Google’s language-understanding efforts. He noticed that state-of-the-art neural networks also suffered from a built-in constraint: They all looked through the sequence of words one by one. This “sequentiality” seemed to match intuitions of how humans actually read written sentences. But Uszkoreit wondered if “it might be the case that understanding language in a linear, sequential fashion is suboptimal,” he said."
      },
      "date": 1650990231567
    },
    {
      "item": {
        "type": "factory",
        "id": "4603f6881a6dc116"
      },
      "id": "4603f6881a6dc116",
      "type": "add",
      "after": "0fe80856f000d5cf",
      "date": 1650990366733
    },
    {
      "type": "edit",
      "id": "4603f6881a6dc116",
      "item": {
        "type": "paragraph",
        "id": "4603f6881a6dc116",
        "text": "Uszkoreit and his collaborators devised a new architecture for neural networks focused on “[[attention]],” a mechanism that lets each layer of the network assign more weight to some specific features of the input than to others. This new attention-focused architecture, called a transformer, could take a sentence like “a dog bites the man” as input and encode each word in many different ways in parallel. For example, a transformer might connect “bites” and “man” together as verb and object, while ignoring “a”; at the same time, it could connect “bites” and “dog” together as verb and subject, while mostly ignoring “the.”"
      },
      "date": 1650990378241
    },
    {
      "type": "edit",
      "id": "4603f6881a6dc116",
      "item": {
        "type": "paragraph",
        "id": "4603f6881a6dc116",
        "text": "Uszkoreit and his collaborators devised a new architecture for neural networks focused on “[[attention]],” a mechanism that lets each layer of the network assign more weight to some specific features of the input than to others. This new attention-focused architecture, called a [[transformer]], could take a sentence like “a dog bites the man” as input and encode each word in many different ways in parallel. For example, a transformer might connect “bites” and “man” together as verb and object, while ignoring “a”; at the same time, it could connect “bites” and “dog” together as verb and subject, while mostly ignoring “the.”"
      },
      "date": 1650990397303
    },
    {
      "item": {
        "type": "factory",
        "id": "74e69df04f3b8946"
      },
      "id": "74e69df04f3b8946",
      "type": "add",
      "after": "4603f6881a6dc116",
      "date": 1650990422619
    },
    {
      "type": "edit",
      "id": "74e69df04f3b8946",
      "item": {
        "type": "paragraph",
        "id": "74e69df04f3b8946",
        "text": "The nonsequential nature of the transformer represented sentences in a more expressive form, which Uszkoreit calls treelike. Each layer of the neural network makes multiple, parallel connections between certain words while ignoring others — akin to a student diagramming a sentence in elementary school. These connections are often drawn between words that may not actually sit next to each other in the sentence. “Those structures effectively look like a number of trees that are overlaid,” Uszkoreit explained."
      },
      "date": 1650990424215
    },
    {
      "item": {
        "type": "factory",
        "id": "a6a56153b444db74"
      },
      "id": "a6a56153b444db74",
      "type": "add",
      "after": "74e69df04f3b8946",
      "date": 1650990459402
    },
    {
      "type": "edit",
      "id": "a6a56153b444db74",
      "item": {
        "type": "paragraph",
        "id": "a6a56153b444db74",
        "text": "This treelike representation of sentences gave transformers a powerful way to model contextual meaning, and also to efficiently learn associations between words that might be far away from each other in complex sentences. “It’s a bit counterintuitive,” Uszkoreit said, “but it is rooted in results from linguistics, which has for a long time looked at treelike models of language.”\n\n"
      },
      "date": 1650990461106
    },
    {
      "type": "edit",
      "id": "a6a56153b444db74",
      "item": {
        "type": "paragraph",
        "id": "a6a56153b444db74",
        "text": "This treelike representation of sentences gave transformers a powerful way to model contextual meaning, and also to efficiently learn associations between words that might be far away from each other in complex sentences. “It’s a bit counterintuitive,” Uszkoreit said, “but it is rooted in results from linguistics, which has for a long time looked at [[treelike models of language]].”\n\n"
      },
      "date": 1650990500239
    },
    {
      "id": "70f0c87837352d77",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "70f0c87837352d77",
        "text": "[[John Pavlus]], Machines Beat Humans on a Reading Test. But Do They Understand? [https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017 post]"
      },
      "after": "a583f895d11ef341",
      "date": 1651070362283
    },
    {
      "id": "63ec2242bc927a36",
      "type": "add",
      "item": {
        "type": "pagefold",
        "id": "63ec2242bc927a36",
        "text": "~"
      },
      "after": "a583f895d11ef341",
      "date": 1651070386360
    },
    {
      "id": "930b96ae33f84b1a",
      "type": "add",
      "item": {
        "type": "pagefold",
        "id": "930b96ae33f84b1a",
        "text": "~",
        "alias": "63ec2242bc927a36"
      },
      "after": "70f0c87837352d77",
      "date": 1651070400988
    }
  ]
}