{
  "title": "Bayesian Reader",
  "story": [
    {
      "type": "paragraph",
      "id": "f600e9fc49bd1047",
      "text": "The Bayesian Reader (Norris, 2006) is a stimulus sampling model, based on the assumption that readers approximate optimal Bayesian decision makers. It accumulates noisy evidence from the input and uses Bayes’ theorem to make optimal decisions about the identity of the input, or whether the input is a word or not. In its existing instantiation, the model is effectively a slot model where words are represented as a concatenation of letter vectors. Each letter vector corresponds to a position-specific slot. This means that although there is uncertainty about the identity of letters (because of the sampling noise) the order of the letters is always known perfectly. However, this slotcoding scheme is merely a simplification to make the model more tractable. The assumption has always been that the noisy sampling procedure applies to all of the information that has to be extracted from the stimulus. A more general implementation of the theory would allow both item and order information to be accumulated gradually over time. Thus, early on, there will be uncertainty as to both the identity of the letters and their order. In fact, very early on, there may even be uncertainty about how many letters there are in the input. The idea that the system should make allowance for the possibility that not all letter-objects may be detected early in processing is consistent with the results of a number of studies that have shown that it is possible to obtain priming from primes in which some of the letters of the target are missing, so long as the remaining letters appear in the same order as they do in the target (Davis & Bowers, 2006; Grainger et al., 2006; Humphreys, Evett, & Quinlan, 1990; Peressotti & Grainger, 1999)."
    },
    {
      "type": "pagefold",
      "id": "376b6d5bf5912105",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "1107150b326e307e",
      "text": "NORRIS, Dennis, KINOSHITA, Sachiko and VAN CASTEREN, Maarten, 2010. A stimulus sampling theory of letter identity and order. Journal of Memory and Language. 2010. Vol. 62, no. 3, p. 254–271. [https://doi.org/10.1016/j.jml.2009.11.002 doi], p. 255."
    },
    {
      "type": "paragraph",
      "id": "efbfc69695325221",
      "text": "Norris, D. (2006). The Bayesian Reader: Explaining word recognition as an optimal Bayesian decision process. Psychological Review, 113, 327–357."
    },
    {
      "type": "paragraph",
      "id": "6d8d9f1bd0500552",
      "text": "For example, Peressotti and Grainger found priming between blcn and BALCON, but not between bcln and BALCON. If the input is so ambiguous that neither the order nor the number of letter objects is known for certain, then an optimal Bayesian decision process must also take account of the likelihood of omissions and insertions. Peressotti and Grainger also found that inserting hyphens in place of the deleted letters (e.g. b-lc-n, BALCON), so as to make the letters occupy the correct serial positions, did not increase priming. This is exactly what one would expect if the system is trying to recover order and not position."
    },
    {
      "type": "paragraph",
      "id": "c6c33f987e22f6c1",
      "text": "In sum, according to the Bayesian Reader, the task of the perceptual system in visual word recognition is to discover the optimal mapping between the noisy representation of the input and lexical entries. One way to view this process is to think of visual processing as having to solve three problems. The first is to determine how many letters there are in the input. Very early in processing even this task may be difficult. Because of the perceptual noise, the system may initially postulate too many or too few letters. That is, some letters will be missing from the developing perceptual representation, and some spurious letters might be inserted. For example, at this point HAT might match THAT to some degree, because of uncertainty as to whether HAT is preceded just by white space or by a partially resolved letter. The second is to determine the order of the letters. Order will also be ambiguous, so it might be unclear whether D comes before or after G in JUGDE. So long as there is some possibility that D precedes G, JUGDE will match JUDGE to some degree. The third is to identify each of the letters. Until the system can be completely confident that the input contains the letter P, there will still be some degree of match between JUDPE and JUDGE. Of course, these three problems need to be solved in parallel; a potential letter might be detected, assigned some possible location and its identity partially resolved. […] (p. 256)"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Bayesian Reader",
        "story": []
      },
      "date": 1709715562781
    },
    {
      "item": {
        "type": "factory",
        "id": "f600e9fc49bd1047"
      },
      "id": "f600e9fc49bd1047",
      "type": "add",
      "date": 1709715576510
    },
    {
      "type": "edit",
      "id": "f600e9fc49bd1047",
      "item": {
        "type": "paragraph",
        "id": "f600e9fc49bd1047",
        "text": "The Bayesian Reader (Norris, 2006) is a stimulus sampling model, based on the assumption that readers approximate optimal Bayesian decision makers. It accumulates noisy evidence from the input and uses Bayes’ theorem to make optimal decisions about the identity of the input, or whether the input is a word or not. In its existing instantiation, the model is effectively a slot model where words are represented as a concatenation of letter vectors. Each letter vector corresponds to a position-specific slot. This means that although there is uncertainty about the identity of letters (because of the sampling noise) the order of the letters is always known perfectly. However, this slotcoding scheme is merely a simplification to make the model more tractable. The assumption has always been that the noisy sampling procedure applies to all of the information that has to be extracted from the stimulus. A more general implementation of the theory would allow both item and order information to be accumulated gradually over time. Thus, early on, there will be uncertainty as to both the identity of the letters and their order. In fact, very early on, there may even be uncertainty about how many letters there are in the input. The idea that the system should make allowance for the possibility that not all letter-objects may be detected early in processing is consistent with the results of a number of studies that have shown that it is possible to obtain priming from primes in which some of the letters of the target are missing, so long as the remaining letters appear in the same order as they do in the target (Davis & Bowers, 2006; Grainger et al., 2006; Humphreys, Evett, & Quinlan, 1990; Peressotti & Grainger, 1999)."
      },
      "date": 1709715578350
    },
    {
      "item": {
        "type": "factory",
        "id": "376b6d5bf5912105"
      },
      "id": "376b6d5bf5912105",
      "type": "add",
      "after": "f600e9fc49bd1047",
      "date": 1709715589616
    },
    {
      "type": "edit",
      "id": "376b6d5bf5912105",
      "item": {
        "type": "pagefold",
        "id": "376b6d5bf5912105",
        "text": "~"
      },
      "date": 1709715593657
    },
    {
      "item": {
        "type": "factory",
        "id": "1107150b326e307e"
      },
      "id": "1107150b326e307e",
      "type": "add",
      "after": "376b6d5bf5912105",
      "date": 1709715596824
    },
    {
      "type": "edit",
      "id": "1107150b326e307e",
      "item": {
        "type": "paragraph",
        "id": "1107150b326e307e",
        "text": "\nNORRIS, Dennis, KINOSHITA, Sachiko and VAN CASTEREN, Maarten, 2010. A stimulus sampling theory of letter identity and order. Journal of Memory and Language. Online. 2010. Vol. 62, no. 3, p. 254–271. Available from: https://www.sciencedirect.com/science/article/pii/S0749596X09001077 [Accessed 6 March 2024]. \n"
      },
      "date": 1709715598681
    },
    {
      "type": "edit",
      "id": "1107150b326e307e",
      "item": {
        "type": "paragraph",
        "id": "1107150b326e307e",
        "text": "NORRIS, Dennis, KINOSHITA, Sachiko and VAN CASTEREN, Maarten, 2010. A stimulus sampling theory of letter identity and order. Journal of Memory and Language. 2010. Vol. 62, no. 3, p. 254–271. [https://www.sciencedirect.com/science/article/pii/S0749596X09001077 sciencedirect] [Accessed 6 March 2024]. \n"
      },
      "date": 1709715624363
    },
    {
      "type": "edit",
      "id": "1107150b326e307e",
      "item": {
        "type": "paragraph",
        "id": "1107150b326e307e",
        "text": "NORRIS, Dennis, KINOSHITA, Sachiko and VAN CASTEREN, Maarten, 2010. A stimulus sampling theory of letter identity and order. Journal of Memory and Language. 2010. Vol. 62, no. 3, p. 254–271. [https://www.sciencedirect.com/science/article/pii/S0749596X09001077 sciencedirect] [https://doi.org/10.1016/j.jml.2009.11.002 doi][Accessed 6 March 2024]. \n"
      },
      "date": 1709715646461
    },
    {
      "type": "edit",
      "id": "1107150b326e307e",
      "item": {
        "type": "paragraph",
        "id": "1107150b326e307e",
        "text": "NORRIS, Dennis, KINOSHITA, Sachiko and VAN CASTEREN, Maarten, 2010. A stimulus sampling theory of letter identity and order. Journal of Memory and Language. 2010. Vol. 62, no. 3, p. 254–271. [https://www.sciencedirect.com/science/article/pii/S0749596X09001077 sciencedirect] [https://doi.org/10.1016/j.jml.2009.11.002 doi] [Accessed 6 March 2024]. \n"
      },
      "date": 1709715650629
    },
    {
      "type": "edit",
      "id": "1107150b326e307e",
      "item": {
        "type": "paragraph",
        "id": "1107150b326e307e",
        "text": "NORRIS, Dennis, KINOSHITA, Sachiko and VAN CASTEREN, Maarten, 2010. A stimulus sampling theory of letter identity and order. Journal of Memory and Language. 2010. Vol. 62, no. 3, p. 254–271.[https://doi.org/10.1016/j.jml.2009.11.002 doi] [Accessed 6 March 2024]. \n"
      },
      "date": 1709715684651
    },
    {
      "type": "edit",
      "id": "1107150b326e307e",
      "item": {
        "type": "paragraph",
        "id": "1107150b326e307e",
        "text": "NORRIS, Dennis, KINOSHITA, Sachiko and VAN CASTEREN, Maarten, 2010. A stimulus sampling theory of letter identity and order. Journal of Memory and Language. 2010. Vol. 62, no. 3, p. 254–271. [https://doi.org/10.1016/j.jml.2009.11.002 doi] [Accessed 6 March 2024]. \n"
      },
      "date": 1709715689406
    },
    {
      "type": "edit",
      "id": "1107150b326e307e",
      "item": {
        "type": "paragraph",
        "id": "1107150b326e307e",
        "text": "NORRIS, Dennis, KINOSHITA, Sachiko and VAN CASTEREN, Maarten, 2010. A stimulus sampling theory of letter identity and order. Journal of Memory and Language. 2010. Vol. 62, no. 3, p. 254–271. [https://doi.org/10.1016/j.jml.2009.11.002 doi], p. 255."
      },
      "date": 1709715879697
    },
    {
      "item": {
        "type": "factory",
        "id": "6d8d9f1bd0500552"
      },
      "id": "6d8d9f1bd0500552",
      "type": "add",
      "after": "1107150b326e307e",
      "date": 1709715893759
    },
    {
      "type": "edit",
      "id": "6d8d9f1bd0500552",
      "item": {
        "type": "paragraph",
        "id": "6d8d9f1bd0500552",
        "text": "For example, Peressotti and Grainger found priming between"
      },
      "date": 1709715895882
    },
    {
      "type": "edit",
      "id": "6d8d9f1bd0500552",
      "item": {
        "type": "paragraph",
        "id": "6d8d9f1bd0500552",
        "text": "For example, Peressotti and Grainger found priming between blcn and BALCON, but not between bcln and BALCON. If the input is so ambiguous that neither the order nor the number of letter objects is known for certain, then an optimal Bayesian decision process must also take account of the likelihood of omissions and insertions. Peressotti and Grainger also found that inserting hyphens in place of the deleted letters (e.g. b-lc-n, BALCON), so as to make the letters occupy the correct serial positions, did not increase priming. This is exactly what one would expect if the system is trying to recover order and not position."
      },
      "date": 1709715916125
    },
    {
      "type": "add",
      "id": "c6c33f987e22f6c1",
      "item": {
        "type": "paragraph",
        "id": "c6c33f987e22f6c1",
        "text": "In sum, according to the Bayesian Reader, the task of the perceptual system in visual word recognition is to discover the optimal mapping between the noisy representation of the input and lexical entries. One way to view this process is to think of visual processing as having to solve three problems. The first is to determine how many letters there are in the input. Very early in processing even this task may be difficult. Because of the perceptual noise, the system may initially postulate too many or too few letters. That is, some letters will be missing from the developing perceptual representation, and some spurious letters might be inserted. For example, at this point HAT might match THAT to some degree, because of uncertainty as to whether HAT is preceded just by white space or by a partially resolved letter. The second is to determine the order of the letters. Order will also be ambiguous, so it might be unclear whether D comes before or after G in JUGDE. So long as there is some possibility that D precedes G, JUGDE will match JUDGE to some degree. The third is to identify each of the letters. Until the system can be completely confident that the input contains the letter P, there will still be some degree of match between JUDPE and JUDGE. Of course, these three problems need to be solved in parallel; […]"
      },
      "after": "6d8d9f1bd0500552",
      "date": 1709715991049
    },
    {
      "type": "edit",
      "id": "c6c33f987e22f6c1",
      "item": {
        "type": "paragraph",
        "id": "c6c33f987e22f6c1",
        "text": "In sum, according to the Bayesian Reader, the task of the perceptual system in visual word recognition is to discover the optimal mapping between the noisy representation of the input and lexical entries. One way to view this process is to think of visual processing as having to solve three problems. The first is to determine how many letters there are in the input. Very early in processing even this task may be difficult. Because of the perceptual noise, the system may initially postulate too many or too few letters. That is, some letters will be missing from the developing perceptual representation, and some spurious letters might be inserted. For example, at this point HAT might match THAT to some degree, because of uncertainty as to whether HAT is preceded just by white space or by a partially resolved letter. The second is to determine the order of the letters. Order will also be ambiguous, so it might be unclear whether D comes before or after G in JUGDE. So long as there is some possibility that D precedes G, JUGDE will match JUDGE to some degree. The third is to identify each of the letters. Until the system can be completely confident that the input contains the letter P, there will still be some degree of match between JUDPE and JUDGE. Of course, these three problems need to be solved in parallel; a potential letter might be detected, assigned some possible location and its identity partially resolved. […]"
      },
      "date": 1709716017164
    },
    {
      "type": "edit",
      "id": "c6c33f987e22f6c1",
      "item": {
        "type": "paragraph",
        "id": "c6c33f987e22f6c1",
        "text": "In sum, according to the Bayesian Reader, the task of the perceptual system in visual word recognition is to discover the optimal mapping between the noisy representation of the input and lexical entries. One way to view this process is to think of visual processing as having to solve three problems. The first is to determine how many letters there are in the input. Very early in processing even this task may be difficult. Because of the perceptual noise, the system may initially postulate too many or too few letters. That is, some letters will be missing from the developing perceptual representation, and some spurious letters might be inserted. For example, at this point HAT might match THAT to some degree, because of uncertainty as to whether HAT is preceded just by white space or by a partially resolved letter. The second is to determine the order of the letters. Order will also be ambiguous, so it might be unclear whether D comes before or after G in JUGDE. So long as there is some possibility that D precedes G, JUGDE will match JUDGE to some degree. The third is to identify each of the letters. Until the system can be completely confident that the input contains the letter P, there will still be some degree of match between JUDPE and JUDGE. Of course, these three problems need to be solved in parallel; a potential letter might be detected, assigned some possible location and its identity partially resolved. […] (p. 256)"
      },
      "date": 1709716055278
    },
    {
      "type": "add",
      "id": "efbfc69695325221",
      "item": {
        "type": "paragraph",
        "id": "efbfc69695325221",
        "text": "Norris, D. (2006). The Bayesian Reader: Explaining word recognition as an optimal Bayesian decision process. Psychological Review, 113, 327–357."
      },
      "after": "1107150b326e307e",
      "date": 1709716145733
    },
    {
      "type": "fork",
      "site": "localhost:3000",
      "date": 1709736832974
    },
    {
      "type": "fork",
      "site": "wiki.ralfbarkow.ch",
      "date": 1709746536697
    }
  ]
}