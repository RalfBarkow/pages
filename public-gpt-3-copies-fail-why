{
  "title": "Public GPT-3 Copies Fail Why?",
  "story": [
    {
      "type": "paragraph",
      "id": "37a4e952bade7a36",
      "text": "Why did all of the public reproduction of GPT-3 fail? "
    },
    {
      "type": "pagefold",
      "id": "3347700980661403",
      "text": "chat.openai"
    },
    {
      "type": "paragraph",
      "id": "0dce7eb78fe93b46",
      "text": "I'm sorry, but your statement that \"all of the public reproduction of GPT-3 failed\" is incorrect.\n\nGPT-3, or Generative Pre-trained Transformer 3, is a highly advanced natural language processing model developed by OpenAI. While it is true that there have been some attempts to reproduce the model's capabilities by other researchers and organizations, it is not accurate to say that all of these attempts have failed.\n\nIt is important to note that GPT-3 is a highly complex and resource-intensive model that requires significant computational power and expertise to develop and train. Therefore, reproducing its capabilities is a challenging task that may not be feasible for many organizations.\n\nThat being said, there have been some successful attempts to reproduce GPT-3's capabilities to some extent, with varying degrees of accuracy and effectiveness. For example, there have been several open-source models and projects that aim to replicate some of GPT-3's capabilities, such as Hugging Face's \"Transformers\" library and EleutherAI's \"GPT-Neo\" models. While these models may not match the exact performance and capabilities of GPT-3, they do represent important progress in the field of natural language processing and show that it is possible to build powerful language models outside of OpenAI's research team."
    },
    {
      "type": "pagefold",
      "id": "1e87396b6f10c247",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "cb3a586f3d256bdd",
      "text": "Why did all of the public reproduction of GPT-3 fail? [https://jingfengyang.github.io/gpt page], [https://news.ycombinator.com/item?id=34815890 hn] \n\nHere I define “fail” as not matching the performance reported in the original GPT-3 paper, with similar or even bigger sizes of models. Under this criteria, GPT-3 and PaLM (540B) are successful, but both of the models are not public, while all of the public models (e.g. OPT-175B, and BLOOM-176B) are “failures”, to some extent. But still, we can learn many lessons from such “failures”. Note that if we have tried different settings for many times, the public community could probably eventually reproduce GPT-3. But till now, the expense is still too high to train even another version of OPT-175B. Because training one pass of such a large model requires running at least for 2 months on ~1000 80G A100 GPUs."
    },
    {
      "type": "pagefold",
      "id": "697852c7e700bf59",
      "text": "chat.openai"
    },
    {
      "type": "paragraph",
      "id": "2c7c10504b7dd550",
      "text": "Thank you for clarifying your definition of \"fail.\" It is true that many attempts to reproduce the performance of GPT-3 in public models have not yet matched the reported performance of the original GPT-3 model.\n\nThere are several reasons why reproducing GPT-3's performance in public models has been challenging:\n\n"
    },
    {
      "type": "paragraph",
      "id": "59107d9b10809668",
      "text": "Computational resources: Training models as large and complex as GPT-3 requires enormous amounts of computational resources. These resources are often not available to smaller research teams or organizations, making it difficult to create models that can match GPT-3's performance.\n"
    },
    {
      "type": "paragraph",
      "id": "3fe74aea08c376ce",
      "text": "Access to data: The quality and size of the data used to train language models is a critical factor in their performance. OpenAI has access to vast amounts of high-quality data, including the internet, books, and academic papers. Reproducing this level of data access is difficult, especially for organizations that do not have the resources to collect and label large amounts of high-quality data.\n"
    },
    {
      "type": "paragraph",
      "id": "b2f136265adbdeed",
      "text": "Model architecture and training techniques: GPT-3 uses a unique architecture and training techniques that have been carefully optimized by OpenAI researchers. Reproducing these techniques accurately is challenging, and even small deviations can significantly impact the model's performance.\n"
    },
    {
      "type": "paragraph",
      "id": "0bd8b42e54cd579d",
      "text": "[[Intellectual Property]]: GPT-3 is a proprietary model developed by OpenAI, and the exact details of its architecture and training techniques are not publicly available. This makes it difficult for other researchers and organizations to accurately reproduce its capabilities.\n"
    },
    {
      "type": "paragraph",
      "id": "3527a5b634525105",
      "text": "While reproducing GPT-3's performance in public models has been challenging, there have been some significant successes in this area, as I mentioned earlier. It is likely that as more research teams and organizations focus on developing large-scale language models, we will continue to see improvements in their performance and capabilities."
    },
    {
      "type": "pagefold",
      "id": "96ff21e6fc97be7e",
      "text": "~"
    },
    {
      "type": "reference",
      "id": "2bdac59385a13bf4",
      "site": "wiki.ralfbarkow.ch",
      "slug": "reproduce",
      "title": "reproduce",
      "text": "Reproducible builds [https://reproducible-builds.org/ site] are a set of software development practices that create an independently-verifiable [[Path]] from source to binary code. [https://reproducible-builds.org/docs/definition/ page]"
    },
    {
      "type": "pagefold",
      "id": "b4878bc38901fc1a",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "e3e77676520c709b",
      "text": "How to create an independently-verifiable path from source to binary code in case of ChatGPT?"
    },
    {
      "type": "pagefold",
      "id": "1dfd904071765a4b",
      "text": "chat.openai"
    },
    {
      "type": "paragraph",
      "id": "af249d14f31fe3f8",
      "text": "As an AI [[Language Model]], ChatGPT's source code is not publicly available, so it is not possible to create an independently-verifiable path from source to binary code. Additionally, ChatGPT is a cloud-based service, so its code is executed remotely on [[Server]]s controlled by OpenAI."
    },
    {
      "type": "paragraph",
      "id": "81d94d11f0373291",
      "text": "However, OpenAI has stated that they take steps to ensure the safety and transparency of their AI models. They have published papers and technical documentation that describe the architecture and training processes of their models, and they have also developed tools and APIs that allow developers to use their models while maintaining control over the data being processed.\n\nIn addition, OpenAI has released several language models as open-source software, including GPT-2 and GPT-3 via the OpenAI API. These open-source models can be audited and verified by independent researchers, providing a degree of transparency and accountability.\n\nOverall, while it may not be possible to create an independently-verifiable path from source to binary code for ChatGPT specifically, OpenAI has taken steps to ensure the safety and transparency of their AI models through documentation, open-source releases, and tools for developers."
    },
    {
      "type": "pagefold",
      "id": "4fbd6279c9067dfc",
      "text": "~"
    },
    {
      "type": "reference",
      "id": "35ed62fa362000b2",
      "site": "wiki.ralfbarkow.ch",
      "slug": "tech--art--one-beyond-truefalse",
      "title": "Tech & Art = One, beyond True/False",
      "text": "With the [[Hologram]], it makes no sense to want to consider it real or fictitious and its algorithms true or false. All should categories like \"[[Good]]\" and \"bad\", \"beautiful\" and \"ugly\", however, will apply to the new technology [⇒ [[Beauty, Good, Truth, and Economy]]]. In this meaning it will be approximately what was called \"art\" in modern times, and Nietzsche's sentence \"art is better than truth\" will only gain the radical meaning that is meant in it. The modern separation of technology and art, of \"hard\" and \"soft\" culture, will become meaningless"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Public GPT-3 Copies Fail Why?",
        "story": []
      },
      "date": 1676545625487
    },
    {
      "item": {
        "type": "factory",
        "id": "37a4e952bade7a36"
      },
      "id": "37a4e952bade7a36",
      "type": "add",
      "date": 1676545638833
    },
    {
      "type": "edit",
      "id": "37a4e952bade7a36",
      "item": {
        "type": "paragraph",
        "id": "37a4e952bade7a36",
        "text": "Why did all of the public reproduction of GPT-3 fail? "
      },
      "date": 1676545642054
    },
    {
      "id": "3347700980661403",
      "type": "add",
      "item": {
        "type": "pagefold",
        "id": "3347700980661403",
        "text": "chat.openai"
      },
      "after": "37a4e952bade7a36",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545666138
    },
    {
      "id": "a7b5a997fb0df11f",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "a7b5a997fb0df11f",
        "text": "I'm sorry, but your statement that \"all of the public reproduction of GPT-3 failed\" is incorrect.\n\nGPT-3, or Generative Pre-trained Transformer 3, is a highly advanced natural language processing model developed by OpenAI. While it is true that there have been some attempts to reproduce the model's capabilities by other researchers and organizations, it is not accurate to say that all of these attempts have failed.\n\nIt is important to note that GPT-3 is a highly complex and resource-intensive model that requires significant computational power and expertise to develop and train. Therefore, reproducing its capabilities is a challenging task that may not be feasible for many organizations.\n\nThat being said, there have been some successful attempts to reproduce GPT-3's capabilities to some extent, with varying degrees of accuracy and effectiveness. For example, there have been several open-source models and projects that aim to replicate some of GPT-3's capabilities, such as Hugging Face's \"Transformers\" library and EleutherAI's \"GPT-Neo\" models. While these models may not match the exact performance and capabilities of GPT-3, they do represent important progress in the field of natural language processing and show that it is possible to build powerful language models outside of OpenAI's research team.\n\n\n"
      },
      "after": "3347700980661403",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545672754
    },
    {
      "id": "1e87396b6f10c247",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545678554
    },
    {
      "id": "1e87396b6f10c247",
      "type": "add",
      "item": {
        "type": "pagefold",
        "id": "1e87396b6f10c247",
        "text": "~"
      },
      "after": "a7b5a997fb0df11f",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545689357
    },
    {
      "id": "cb3a586f3d256bdd",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545720160
    },
    {
      "id": "cb3a586f3d256bdd",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "cb3a586f3d256bdd",
        "text": "Why did all of the public reproduction of GPT-3 fail? https://jingfengyang.github.io/gpt page], [https://news.ycombinator.com/item?id=34815890 hn] \n\nHere I define “fail” as not matching the performance reported in the original GPT-3 paper, with similar or even bigger sizes of models. Under this criteria, GPT-3 and PaLM (540B) are successful, but both of the models are not public, while all of the public models (e.g. OPT-175B, and BLOOM-176B) are “failures”, to some extent. But still, we can learn many lessons from such “failures”. Note that if we have tried different settings for many times, the public community could probably eventually reproduce GPT-3. But till now, the expense is still too high to train even another version of OPT-175B. Because training one pass of such a large model requires running at least for 2 months on ~1000 80G A100 GPUs."
      },
      "after": "a7b5a997fb0df11f",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545721993
    },
    {
      "id": "1e87396b6f10c247",
      "type": "move",
      "order": [
        "37a4e952bade7a36",
        "3347700980661403",
        "a7b5a997fb0df11f",
        "1e87396b6f10c247",
        "cb3a586f3d256bdd"
      ],
      "date": 1676545726403
    },
    {
      "id": "697852c7e700bf59",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545730454
    },
    {
      "id": "697852c7e700bf59",
      "type": "add",
      "item": {
        "type": "pagefold",
        "id": "697852c7e700bf59",
        "text": "chat.openai"
      },
      "after": "cb3a586f3d256bdd",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545732571
    },
    {
      "id": "2c7c10504b7dd550",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545736416
    },
    {
      "id": "2c7c10504b7dd550",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "2c7c10504b7dd550",
        "text": "Thank you for clarifying your definition of \"fail.\" It is true that many attempts to reproduce the performance of GPT-3 in public models have not yet matched the reported performance of the original GPT-3 model.\n\nThere are several reasons why reproducing GPT-3's performance in public models has been challenging:\n\n"
      },
      "after": "697852c7e700bf59",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545738159
    },
    {
      "type": "remove",
      "id": "a7b5a997fb0df11f",
      "date": 1676545753698
    },
    {
      "id": "59107d9b10809668",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545769116
    },
    {
      "id": "59107d9b10809668",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "59107d9b10809668",
        "text": "Computational resources: Training models as large and complex as GPT-3 requires enormous amounts of computational resources. These resources are often not available to smaller research teams or organizations, making it difficult to create models that can match GPT-3's performance.\n"
      },
      "after": "2c7c10504b7dd550",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545770976
    },
    {
      "id": "3fe74aea08c376ce",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545774676
    },
    {
      "id": "3fe74aea08c376ce",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "3fe74aea08c376ce",
        "text": "Access to data: The quality and size of the data used to train language models is a critical factor in their performance. OpenAI has access to vast amounts of high-quality data, including the internet, books, and academic papers. Reproducing this level of data access is difficult, especially for organizations that do not have the resources to collect and label large amounts of high-quality data.\n"
      },
      "after": "59107d9b10809668",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545777626
    },
    {
      "id": "b2f136265adbdeed",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545784216
    },
    {
      "id": "b2f136265adbdeed",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "b2f136265adbdeed",
        "text": "Model architecture and training techniques: GPT-3 uses a unique architecture and training techniques that have been carefully optimized by OpenAI researchers. Reproducing these techniques accurately is challenging, and even small deviations can significantly impact the model's performance.\n"
      },
      "after": "3fe74aea08c376ce",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545787941
    },
    {
      "id": "0bd8b42e54cd579d",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545790560
    },
    {
      "id": "0bd8b42e54cd579d",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "0bd8b42e54cd579d",
        "text": "Intellectual property: GPT-3 is a proprietary model developed by OpenAI, and the exact details of its architecture and training techniques are not publicly available. This makes it difficult for other researchers and organizations to accurately reproduce its capabilities.\n"
      },
      "after": "b2f136265adbdeed",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545796985
    },
    {
      "id": "3527a5b634525105",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676545800797
    },
    {
      "id": "3527a5b634525105",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "3527a5b634525105",
        "text": "While reproducing GPT-3's performance in public models has been challenging, there have been some significant successes in this area, as I mentioned earlier. It is likely that as more research teams and organizations focus on developing large-scale language models, we will continue to see improvements in their performance and capabilities."
      },
      "after": "0bd8b42e54cd579d",
      "attribution": {
        "page": "2023-02-16"
      },
      "date": 1676545805884
    },
    {
      "id": "1e87396b6f10c247",
      "type": "move",
      "order": [
        "37a4e952bade7a36",
        "3347700980661403",
        "cb3a586f3d256bdd",
        "1e87396b6f10c247",
        "697852c7e700bf59",
        "2c7c10504b7dd550",
        "59107d9b10809668",
        "3fe74aea08c376ce",
        "b2f136265adbdeed",
        "0bd8b42e54cd579d",
        "3527a5b634525105"
      ],
      "date": 1676545841962
    },
    {
      "item": {
        "type": "factory",
        "id": "4fbd7983b18e9a0f"
      },
      "id": "4fbd7983b18e9a0f",
      "type": "add",
      "after": "3527a5b634525105",
      "date": 1676545858590
    },
    {
      "type": "edit",
      "id": "4fbd7983b18e9a0f",
      "item": {
        "type": "paragraph",
        "id": "4fbd7983b18e9a0f",
        "text": "Why did all of the public reproduction of GPT-3 fail?\n\nHere I define “fail” as not matching the performance reported in the original GPT-3 paper, with similar or even bigger sizes of models. Under this criteria, GPT-3 and PaLM (540B) are successful, but both of the models are not public, while all of the public models (e.g. OPT-175B, and BLOOM-176B) are “failures”, to some extent. But still, we can learn many lessons from such “failures”. Note that if we have tried different settings for many times, the public community could probably eventually reproduce GPT-3. But till now, the expense is still too high to train even another version of OPT-175B. Because training one pass of such a large model requires running at least for 2 months on ~1000 80G A100 GPUs."
      },
      "date": 1676545860457
    },
    {
      "id": "4fbd7983b18e9a0f",
      "type": "move",
      "order": [
        "37a4e952bade7a36",
        "3347700980661403",
        "cb3a586f3d256bdd",
        "1e87396b6f10c247",
        "4fbd7983b18e9a0f",
        "697852c7e700bf59",
        "2c7c10504b7dd550",
        "59107d9b10809668",
        "3fe74aea08c376ce",
        "b2f136265adbdeed",
        "0bd8b42e54cd579d",
        "3527a5b634525105"
      ],
      "date": 1676545864475
    },
    {
      "type": "remove",
      "id": "4fbd7983b18e9a0f",
      "date": 1676545892415
    },
    {
      "id": "cb3a586f3d256bdd",
      "type": "move",
      "order": [
        "37a4e952bade7a36",
        "3347700980661403",
        "1e87396b6f10c247",
        "697852c7e700bf59",
        "cb3a586f3d256bdd",
        "2c7c10504b7dd550",
        "59107d9b10809668",
        "3fe74aea08c376ce",
        "b2f136265adbdeed",
        "0bd8b42e54cd579d",
        "3527a5b634525105"
      ],
      "date": 1676545909949
    },
    {
      "id": "cb3a586f3d256bdd",
      "type": "move",
      "order": [
        "37a4e952bade7a36",
        "3347700980661403",
        "1e87396b6f10c247",
        "cb3a586f3d256bdd",
        "697852c7e700bf59",
        "2c7c10504b7dd550",
        "59107d9b10809668",
        "3fe74aea08c376ce",
        "b2f136265adbdeed",
        "0bd8b42e54cd579d",
        "3527a5b634525105"
      ],
      "date": 1676545915179
    },
    {
      "type": "add",
      "id": "0dce7eb78fe93b46",
      "item": {
        "type": "paragraph",
        "id": "0dce7eb78fe93b46",
        "text": "I'm sorry, but your statement that \"all of the public reproduction of GPT-3 failed\" is incorrect.\n\nGPT-3, or Generative Pre-trained Transformer 3, is a highly advanced natural language processing model developed by OpenAI. While it is true that there have been some attempts to reproduce the model's capabilities by other researchers and organizations, it is not accurate to say that all of these attempts have failed.\n\nIt is important to note that GPT-3 is a highly complex and resource-intensive model that requires significant computational power and expertise to develop and train. Therefore, reproducing its capabilities is a challenging task that may not be feasible for many organizations.\n\nThat being said, there have been some successful attempts to reproduce GPT-3's capabilities to some extent, with varying degrees of accuracy and effectiveness. For example, there have been several open-source models and projects that aim to replicate some of GPT-3's capabilities, such as Hugging Face's \"Transformers\" library and EleutherAI's \"GPT-Neo\" models. While these models may not match the exact performance and capabilities of GPT-3, they do represent important progress in the field of natural language processing and show that it is possible to build powerful language models outside of OpenAI's research team."
      },
      "after": "37a4e952bade7a36",
      "date": 1676545933815
    },
    {
      "id": "3347700980661403",
      "type": "move",
      "order": [
        "37a4e952bade7a36",
        "3347700980661403",
        "0dce7eb78fe93b46",
        "1e87396b6f10c247",
        "cb3a586f3d256bdd",
        "697852c7e700bf59",
        "2c7c10504b7dd550",
        "59107d9b10809668",
        "3fe74aea08c376ce",
        "b2f136265adbdeed",
        "0bd8b42e54cd579d",
        "3527a5b634525105"
      ],
      "date": 1676545936721
    },
    {
      "type": "edit",
      "id": "cb3a586f3d256bdd",
      "item": {
        "type": "paragraph",
        "id": "cb3a586f3d256bdd",
        "text": "Why did all of the public reproduction of GPT-3 fail? [https://jingfengyang.github.io/gpt page], [https://news.ycombinator.com/item?id=34815890 hn] \n\nHere I define “fail” as not matching the performance reported in the original GPT-3 paper, with similar or even bigger sizes of models. Under this criteria, GPT-3 and PaLM (540B) are successful, but both of the models are not public, while all of the public models (e.g. OPT-175B, and BLOOM-176B) are “failures”, to some extent. But still, we can learn many lessons from such “failures”. Note that if we have tried different settings for many times, the public community could probably eventually reproduce GPT-3. But till now, the expense is still too high to train even another version of OPT-175B. Because training one pass of such a large model requires running at least for 2 months on ~1000 80G A100 GPUs."
      },
      "date": 1676545955761
    },
    {
      "type": "edit",
      "id": "0bd8b42e54cd579d",
      "item": {
        "type": "paragraph",
        "id": "0bd8b42e54cd579d",
        "text": "[[Intellectual Property]]: GPT-3 is a proprietary model developed by OpenAI, and the exact details of its architecture and training techniques are not publicly available. This makes it difficult for other researchers and organizations to accurately reproduce its capabilities.\n"
      },
      "date": 1676546085037
    },
    {
      "item": {
        "type": "factory",
        "id": "96ff21e6fc97be7e"
      },
      "id": "96ff21e6fc97be7e",
      "type": "add",
      "after": "3527a5b634525105",
      "date": 1676546374274
    },
    {
      "type": "edit",
      "id": "96ff21e6fc97be7e",
      "item": {
        "type": "pagefold",
        "id": "96ff21e6fc97be7e",
        "text": "~"
      },
      "date": 1676546376990
    },
    {
      "id": "2bdac59385a13bf4",
      "type": "add",
      "item": {
        "type": "reference",
        "id": "2bdac59385a13bf4",
        "site": "wiki.ralfbarkow.ch",
        "slug": "reproduce",
        "title": "reproduce",
        "text": "Reproducible builds [https://reproducible-builds.org/ site] are a set of software development practices that create an independently-verifiable [[Path]] from source to binary code. [https://reproducible-builds.org/docs/definition/ page]"
      },
      "after": "96ff21e6fc97be7e",
      "attribution": {
        "page": "Jingfeng Yang"
      },
      "date": 1676546640066
    },
    {
      "item": {
        "type": "factory",
        "id": "b4878bc38901fc1a"
      },
      "id": "b4878bc38901fc1a",
      "type": "add",
      "after": "2bdac59385a13bf4",
      "date": 1676546725459
    },
    {
      "type": "edit",
      "id": "b4878bc38901fc1a",
      "item": {
        "type": "pagefold",
        "id": "b4878bc38901fc1a",
        "text": "~"
      },
      "date": 1676546728933
    },
    {
      "item": {
        "type": "factory",
        "id": "e3e77676520c709b"
      },
      "id": "e3e77676520c709b",
      "type": "add",
      "after": "b4878bc38901fc1a",
      "date": 1676546730373
    },
    {
      "type": "edit",
      "id": "e3e77676520c709b",
      "item": {
        "type": "paragraph",
        "id": "e3e77676520c709b",
        "text": "How to create an independently-verifiable path from source to binary code in case of ChatGPT?"
      },
      "date": 1676546732569
    },
    {
      "item": {
        "type": "factory",
        "id": "1dfd904071765a4b"
      },
      "id": "1dfd904071765a4b",
      "type": "add",
      "after": "e3e77676520c709b",
      "date": 1676546738148
    },
    {
      "type": "edit",
      "id": "1dfd904071765a4b",
      "item": {
        "type": "pagefold",
        "id": "1dfd904071765a4b",
        "text": "chat.openai"
      },
      "date": 1676546742356
    },
    {
      "item": {
        "type": "factory",
        "id": "af249d14f31fe3f8"
      },
      "id": "af249d14f31fe3f8",
      "type": "add",
      "after": "1dfd904071765a4b",
      "date": 1676546751906
    },
    {
      "type": "edit",
      "id": "af249d14f31fe3f8",
      "item": {
        "type": "paragraph",
        "id": "af249d14f31fe3f8",
        "text": "As an AI language model, ChatGPT's source code is not publicly available, so it is not possible to create an independently-verifiable path from source to binary code. Additionally, ChatGPT is a cloud-based service, so its code is executed remotely on servers controlled by OpenAI.\n\nHowever, OpenAI has stated that they take steps to ensure the safety and transparency of their AI models. They have published papers and technical documentation that describe the architecture and training processes of their models, and they have also developed tools and APIs that allow developers to use their models while maintaining control over the data being processed.\n\nIn addition, OpenAI has released several language models as open-source software, including GPT-2 and GPT-3 via the OpenAI API. These open-source models can be audited and verified by independent researchers, providing a degree of transparency and accountability.\n\nOverall, while it may not be possible to create an independently-verifiable path from source to binary code for ChatGPT specifically, OpenAI has taken steps to ensure the safety and transparency of their AI models through documentation, open-source releases, and tools for developers."
      },
      "date": 1676546753603
    },
    {
      "item": {
        "type": "factory",
        "id": "4fbd6279c9067dfc"
      },
      "id": "4fbd6279c9067dfc",
      "type": "add",
      "after": "af249d14f31fe3f8",
      "date": 1676546756125
    },
    {
      "type": "edit",
      "id": "4fbd6279c9067dfc",
      "item": {
        "type": "pagefold",
        "id": "4fbd6279c9067dfc",
        "text": "~"
      },
      "date": 1676546759012
    },
    {
      "item": {
        "type": "factory",
        "id": "35ed62fa362000b2"
      },
      "id": "35ed62fa362000b2",
      "type": "add",
      "after": "4fbd6279c9067dfc",
      "date": 1676546872787
    },
    {
      "type": "edit",
      "id": "35ed62fa362000b2",
      "item": {
        "type": "reference",
        "id": "35ed62fa362000b2",
        "site": "wiki.ralfbarkow.ch",
        "slug": "tech--art--one-beyond-truefalse",
        "title": "Tech & Art = One, beyond True/False",
        "text": "With the [[Hologram]], it makes no sense to want to consider it real or fictitious and its algorithms true or false. All should categories like \"[[Good]]\" and \"bad\", \"beautiful\" and \"ugly\", however, will apply to the new technology [⇒ [[Beauty, Good, Truth, and Economy]]]. In this meaning it will be approximately what was called \"art\" in modern times, and Nietzsche's sentence \"art is better than truth\" will only gain the radical meaning that is meant in it. The modern separation of technology and art, of \"hard\" and \"soft\" culture, will become meaningless"
      },
      "date": 1676546875338
    },
    {
      "type": "edit",
      "id": "af249d14f31fe3f8",
      "item": {
        "type": "paragraph",
        "id": "af249d14f31fe3f8",
        "text": "As an AI language model, ChatGPT's source code is not publicly available, so it is not possible to create an independently-verifiable path from source to binary code. Additionally, ChatGPT is a cloud-based service, so its code is executed remotely on [[Server]]s controlled by OpenAI."
      },
      "date": 1676547137165
    },
    {
      "type": "add",
      "id": "81d94d11f0373291",
      "item": {
        "type": "paragraph",
        "id": "81d94d11f0373291",
        "text": "However, OpenAI has stated that they take steps to ensure the safety and transparency of their AI models. They have published papers and technical documentation that describe the architecture and training processes of their models, and they have also developed tools and APIs that allow developers to use their models while maintaining control over the data being processed.\n\nIn addition, OpenAI has released several language models as open-source software, including GPT-2 and GPT-3 via the OpenAI API. These open-source models can be audited and verified by independent researchers, providing a degree of transparency and accountability.\n\nOverall, while it may not be possible to create an independently-verifiable path from source to binary code for ChatGPT specifically, OpenAI has taken steps to ensure the safety and transparency of their AI models through documentation, open-source releases, and tools for developers."
      },
      "after": "af249d14f31fe3f8",
      "date": 1676547139295
    },
    {
      "type": "edit",
      "id": "af249d14f31fe3f8",
      "item": {
        "type": "paragraph",
        "id": "af249d14f31fe3f8",
        "text": "As an AI [[Language Model]], ChatGPT's source code is not publicly available, so it is not possible to create an independently-verifiable path from source to binary code. Additionally, ChatGPT is a cloud-based service, so its code is executed remotely on [[Server]]s controlled by OpenAI."
      },
      "date": 1676547303768
    }
  ]
}