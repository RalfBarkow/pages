{
  "title": "RaptorJIT+Studio",
  "story": [
    {
      "type": "markdown",
      "id": "64e78bfba62aa346",
      "text": "[https://github.com/lukego/blog/issues/20 Analyzing the evolving C heap of a JIT compiler]"
    },
    {
      "type": "markdown",
      "id": "960ea3c8f2ffdd14",
      "text": "Let me tell you about a ~~~cute hack~~~ long story for logging and making sense of diagnostic data from the RaptorJIT virtual machine. (Note: The pretty screenshots are at the bottom.)\n\n[[RaptorJIT]] is a high-performance Lua virtual machine (LuaJIT fork) and it has to reconcile a couple of tricky requirements for diagnostics. On the one hand we need full diagnostic data to always be available in production (of course!) On the other hand production applications need to run at maximum speed and with absolute minimum latency. So how do we support both?\n\nThe approach taken here is to split the diagnostic work into two parts. The RaptorJIT virtual machine produces raw data as efficiently as possible and then separate tooling analyzes this data.\n\nThe virtual machine is kept as simple and efficient as possible: the logging needs to be enabled at all times and there can't be any measurable overhead (and certainly not any crashes.) The logging also needs to be comprehensive. We want to capture the loaded code, the JIT compilation attempts, the intermediate representations of generated code, and so on.\n\nThe analysis tooling then has to absorb all of the complexity. This is tolerable because it runs offline, out of harms way, and can be written in a relaxed high-level style. Accepting the complexity can be beneficial too: making the tooling understand internal data structures of the virtual machine makes it possible to invent new analysis to apply to existing data. That's a lot better than asking users, \"Please take this updated virtual machine into production, make it crash, and send new logs.\"\n\nLet's roll up our sleeves and look at how this works.\nRaptorJIT\n\nThe RaptorJIT diagnostic data production is implemented in lj_auditlog.c. It's only about 100 LOC. It opens a binary log file and writes two kinds of message in msgpack format. (Aside: msgpack rocks.)\n\nThe first kind of log message is called memory. These message snapshot the contents of a raw piece of memory in the process address space. The log message is an array of bytes, the 64-bit starting address, and an optional \"hint\" to help with decoding. The application is responsible for logging each block of memory that the analysis tools will need.\n\nThe second kind of log message is called event. These messages show when something interesting has happened. The log message is an event name and other free-form attributes, including references to previously logged memory.\n\nThe same piece of memory can be logged many times to track its evolution. The memory references in event log messages are understood to refer to the memory at the time the event was logged. So when the tooling wants to \"peek\" a byte of process memory it will need to search backwards in the log starting from the event of interest. This way we can track the evolution of the process heap and allow the VM to reuse the same memory for different purposes e.g. reusing the same JIT datastructures to compile different code at different times.\n\nHere is what some raw log looks like when decoded from binary msgpack into json:\n\n$ msgpack2json -d -p -c -i audit.log\n{\n    \"type\": \"memory\",\n    \"hint\": \"GCstr\",\n    \"address\": 139675345683296,\n    \"data\": <bin of size 32>\n}\n{\n    \"type\": \"memory\",\n    \"hint\": \"GCproto\",\n    \"address\": 139675345683344,\n    \"data\": <bin of size 168>\n}\n{\n    \"type\": \"event\",\n    \"event\": \"new_prototype\",\n    \"GCproto\": 139675345683344\n}\n\nWe can read this backwards:\n\n    There is an event of type new_prototype, which means that the virtual machine defined a new bytecode function. This event references a GCproto object at address 139675345683344 (0x7f08b35d0390).\n    There is a 168-byte block of memory at address 139675345683344 logged. This is the address referenced in the event. The contents is the raw C datatype struct GCproto which includes the bytecode, the debug info to resolve source line numbers, etc. It also references the name of the source file that the bytecode was loaded from, which is a Lua string object stored elsewhere in memory.\n    Finally another object is stored: it's the 32-byte GCstr object containing the name of the source file. The address of this object is 139675345683296 (0x7f08b35d0390) and this happens to be referenced by the previous GCproto object (you can't see the address in the log because it's inside the <bin of size 168>.)\n\nHalf-mission accomplished! The RaptorJIT virtual machine is now exposing its raw state to the outside world very efficiently, and the code is so simple that we can be confident about putting it into production.\nStudio\n\nThe second part of the problem is to extract high-level information from the logs. We are not interested in reading hex dumps! We want the tooling to present really high-level information about which code has been JITed, how the compiled code has been optimized, which compilation attempts failed and why, and which code is hot in the profiler, and so on.\n\nWe solve this problem using Studio, which is \"an extensible debugger for the data produced by complex applications.\" Studio is the perfect fit for this application - as it should be, since this problem was the motivation for creating the Studio project :-).\n\nWe take the direct \"brute force\" approach. This is conceptually like reading a coredump into gdb and writing macros to inspect it, but Studio means the tools will be written in Pharo Smalltalk with any awkward chores offloaded with Nix scripts.\n\nHere is the plan of attack:\n\n    Read RaptorJIT DWARF metadata to understand the memory layout of native C objects.\n    Decode application types (GCproto, GCstr, etc) into higher-level Smalltalk objects.\n    Extend the Glamorous Inspector framework to interactively browse our objects.\n    Use the Agile Visualization framework to visualize the more complex objects.\n\nLet's do this!\nRead RaptorJIT DWARF metadata\n\nLooking at DWARF for the first time, several things are immediately apparent:\n\n    The DWARF format is elaborate and arcane.\n    We wouldn't want to touch the libdwarf C library with a ten foot pole.\n    None of the dwarf2foo utilities on the internet seem to really work.\n\nThis is great news: it means that we are perfectly justified in cheating. (The alternative would be to become DWARF experts, but what we are really trying to do here is develop a JIT compiler, remember?)\n\nCheating is easy with Nix. Nix provides \"dependency heaven.\" We can write simple scripts, we can use arbitrary versions of random utility programs, and we can be confident that everything will work the same way every time.\n\nWe create a Nix API with an elf2json function that converts a messy ELF file (produced by clang/gcc during RaptorJIT compilation) into a simple JSON description of what we care about, which are the definitions of types and #define macros and so on."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "RaptorJIT+Studio",
        "story": []
      },
      "date": 1635946817788
    },
    {
      "item": {
        "type": "factory",
        "id": "64e78bfba62aa346"
      },
      "id": "64e78bfba62aa346",
      "type": "add",
      "date": 1635946865512
    },
    {
      "type": "edit",
      "id": "64e78bfba62aa346",
      "item": {
        "type": "markdown",
        "id": "64e78bfba62aa346",
        "text": "https://github.com/lukego/blog/issues/20"
      },
      "date": 1635946868469
    },
    {
      "type": "edit",
      "id": "64e78bfba62aa346",
      "item": {
        "type": "markdown",
        "id": "64e78bfba62aa346",
        "text": "[https://github.com/lukego/blog/issues/20 Analyzing the evolving C heap of a JIT compiler]"
      },
      "date": 1635946902668
    },
    {
      "item": {
        "type": "factory",
        "id": "960ea3c8f2ffdd14"
      },
      "id": "960ea3c8f2ffdd14",
      "type": "add",
      "after": "64e78bfba62aa346",
      "date": 1635946928820
    },
    {
      "type": "edit",
      "id": "960ea3c8f2ffdd14",
      "item": {
        "type": "markdown",
        "id": "960ea3c8f2ffdd14",
        "text": "Let me tell you about a ~~~cute hack~~~ long story for logging and making sense of diagnostic data from the RaptorJIT virtual machine. (Note: The pretty screenshots are at the bottom.)\n\nRaptorJIT is a high-performance Lua virtual machine (LuaJIT fork) and it has to reconcile a couple of tricky requirements for diagnostics. On the one hand we need full diagnostic data to always be available in production (of course!) On the other hand production applications need to run at maximum speed and with absolute minimum latency. So how do we support both?\n\nThe approach taken here is to split the diagnostic work into two parts. The RaptorJIT virtual machine produces raw data as efficiently as possible and then separate tooling analyzes this data.\n\nThe virtual machine is kept as simple and efficient as possible: the logging needs to be enabled at all times and there can't be any measurable overhead (and certainly not any crashes.) The logging also needs to be comprehensive. We want to capture the loaded code, the JIT compilation attempts, the intermediate representations of generated code, and so on.\n\nThe analysis tooling then has to absorb all of the complexity. This is tolerable because it runs offline, out of harms way, and can be written in a relaxed high-level style. Accepting the complexity can be beneficial too: making the tooling understand internal data structures of the virtual machine makes it possible to invent new analysis to apply to existing data. That's a lot better than asking users, \"Please take this updated virtual machine into production, make it crash, and send new logs.\"\n\nLet's roll up our sleeves and look at how this works.\nRaptorJIT\n\nThe RaptorJIT diagnostic data production is implemented in lj_auditlog.c. It's only about 100 LOC. It opens a binary log file and writes two kinds of message in msgpack format. (Aside: msgpack rocks.)\n\nThe first kind of log message is called memory. These message snapshot the contents of a raw piece of memory in the process address space. The log message is an array of bytes, the 64-bit starting address, and an optional \"hint\" to help with decoding. The application is responsible for logging each block of memory that the analysis tools will need.\n\nThe second kind of log message is called event. These messages show when something interesting has happened. The log message is an event name and other free-form attributes, including references to previously logged memory.\n\nThe same piece of memory can be logged many times to track its evolution. The memory references in event log messages are understood to refer to the memory at the time the event was logged. So when the tooling wants to \"peek\" a byte of process memory it will need to search backwards in the log starting from the event of interest. This way we can track the evolution of the process heap and allow the VM to reuse the same memory for different purposes e.g. reusing the same JIT datastructures to compile different code at different times.\n\nHere is what some raw log looks like when decoded from binary msgpack into json:\n\n$ msgpack2json -d -p -c -i audit.log\n{\n    \"type\": \"memory\",\n    \"hint\": \"GCstr\",\n    \"address\": 139675345683296,\n    \"data\": <bin of size 32>\n}\n{\n    \"type\": \"memory\",\n    \"hint\": \"GCproto\",\n    \"address\": 139675345683344,\n    \"data\": <bin of size 168>\n}\n{\n    \"type\": \"event\",\n    \"event\": \"new_prototype\",\n    \"GCproto\": 139675345683344\n}\n\nWe can read this backwards:\n\n    There is an event of type new_prototype, which means that the virtual machine defined a new bytecode function. This event references a GCproto object at address 139675345683344 (0x7f08b35d0390).\n    There is a 168-byte block of memory at address 139675345683344 logged. This is the address referenced in the event. The contents is the raw C datatype struct GCproto which includes the bytecode, the debug info to resolve source line numbers, etc. It also references the name of the source file that the bytecode was loaded from, which is a Lua string object stored elsewhere in memory.\n    Finally another object is stored: it's the 32-byte GCstr object containing the name of the source file. The address of this object is 139675345683296 (0x7f08b35d0390) and this happens to be referenced by the previous GCproto object (you can't see the address in the log because it's inside the <bin of size 168>.)\n\nHalf-mission accomplished! The RaptorJIT virtual machine is now exposing its raw state to the outside world very efficiently, and the code is so simple that we can be confident about putting it into production.\nStudio\n\nThe second part of the problem is to extract high-level information from the logs. We are not interested in reading hex dumps! We want the tooling to present really high-level information about which code has been JITed, how the compiled code has been optimized, which compilation attempts failed and why, and which code is hot in the profiler, and so on.\n\nWe solve this problem using Studio, which is \"an extensible debugger for the data produced by complex applications.\" Studio is the perfect fit for this application - as it should be, since this problem was the motivation for creating the Studio project :-).\n\nWe take the direct \"brute force\" approach. This is conceptually like reading a coredump into gdb and writing macros to inspect it, but Studio means the tools will be written in Pharo Smalltalk with any awkward chores offloaded with Nix scripts.\n\nHere is the plan of attack:\n\n    Read RaptorJIT DWARF metadata to understand the memory layout of native C objects.\n    Decode application types (GCproto, GCstr, etc) into higher-level Smalltalk objects.\n    Extend the Glamorous Inspector framework to interactively browse our objects.\n    Use the Agile Visualization framework to visualize the more complex objects.\n\nLet's do this!\nRead RaptorJIT DWARF metadata\n\nLooking at DWARF for the first time, several things are immediately apparent:\n\n    The DWARF format is elaborate and arcane.\n    We wouldn't want to touch the libdwarf C library with a ten foot pole.\n    None of the dwarf2foo utilities on the internet seem to really work.\n\nThis is great news: it means that we are perfectly justified in cheating. (The alternative would be to become DWARF experts, but what we are really trying to do here is develop a JIT compiler, remember?)\n\nCheating is easy with Nix. Nix provides \"dependency heaven.\" We can write simple scripts, we can use arbitrary versions of random utility programs, and we can be confident that everything will work the same way every time.\n\nWe create a Nix API with an elf2json function that converts a messy ELF file (produced by clang/gcc during RaptorJIT compilation) into a simple JSON description of what we care about, which are the definitions of types and #define macros and so on."
      },
      "date": 1635946930873
    },
    {
      "type": "edit",
      "id": "960ea3c8f2ffdd14",
      "item": {
        "type": "markdown",
        "id": "960ea3c8f2ffdd14",
        "text": "Let me tell you about a ~~~cute hack~~~ long story for logging and making sense of diagnostic data from the RaptorJIT virtual machine. (Note: The pretty screenshots are at the bottom.)\n\n[[RaptorJIT]] is a high-performance Lua virtual machine (LuaJIT fork) and it has to reconcile a couple of tricky requirements for diagnostics. On the one hand we need full diagnostic data to always be available in production (of course!) On the other hand production applications need to run at maximum speed and with absolute minimum latency. So how do we support both?\n\nThe approach taken here is to split the diagnostic work into two parts. The RaptorJIT virtual machine produces raw data as efficiently as possible and then separate tooling analyzes this data.\n\nThe virtual machine is kept as simple and efficient as possible: the logging needs to be enabled at all times and there can't be any measurable overhead (and certainly not any crashes.) The logging also needs to be comprehensive. We want to capture the loaded code, the JIT compilation attempts, the intermediate representations of generated code, and so on.\n\nThe analysis tooling then has to absorb all of the complexity. This is tolerable because it runs offline, out of harms way, and can be written in a relaxed high-level style. Accepting the complexity can be beneficial too: making the tooling understand internal data structures of the virtual machine makes it possible to invent new analysis to apply to existing data. That's a lot better than asking users, \"Please take this updated virtual machine into production, make it crash, and send new logs.\"\n\nLet's roll up our sleeves and look at how this works.\nRaptorJIT\n\nThe RaptorJIT diagnostic data production is implemented in lj_auditlog.c. It's only about 100 LOC. It opens a binary log file and writes two kinds of message in msgpack format. (Aside: msgpack rocks.)\n\nThe first kind of log message is called memory. These message snapshot the contents of a raw piece of memory in the process address space. The log message is an array of bytes, the 64-bit starting address, and an optional \"hint\" to help with decoding. The application is responsible for logging each block of memory that the analysis tools will need.\n\nThe second kind of log message is called event. These messages show when something interesting has happened. The log message is an event name and other free-form attributes, including references to previously logged memory.\n\nThe same piece of memory can be logged many times to track its evolution. The memory references in event log messages are understood to refer to the memory at the time the event was logged. So when the tooling wants to \"peek\" a byte of process memory it will need to search backwards in the log starting from the event of interest. This way we can track the evolution of the process heap and allow the VM to reuse the same memory for different purposes e.g. reusing the same JIT datastructures to compile different code at different times.\n\nHere is what some raw log looks like when decoded from binary msgpack into json:\n\n$ msgpack2json -d -p -c -i audit.log\n{\n    \"type\": \"memory\",\n    \"hint\": \"GCstr\",\n    \"address\": 139675345683296,\n    \"data\": <bin of size 32>\n}\n{\n    \"type\": \"memory\",\n    \"hint\": \"GCproto\",\n    \"address\": 139675345683344,\n    \"data\": <bin of size 168>\n}\n{\n    \"type\": \"event\",\n    \"event\": \"new_prototype\",\n    \"GCproto\": 139675345683344\n}\n\nWe can read this backwards:\n\n    There is an event of type new_prototype, which means that the virtual machine defined a new bytecode function. This event references a GCproto object at address 139675345683344 (0x7f08b35d0390).\n    There is a 168-byte block of memory at address 139675345683344 logged. This is the address referenced in the event. The contents is the raw C datatype struct GCproto which includes the bytecode, the debug info to resolve source line numbers, etc. It also references the name of the source file that the bytecode was loaded from, which is a Lua string object stored elsewhere in memory.\n    Finally another object is stored: it's the 32-byte GCstr object containing the name of the source file. The address of this object is 139675345683296 (0x7f08b35d0390) and this happens to be referenced by the previous GCproto object (you can't see the address in the log because it's inside the <bin of size 168>.)\n\nHalf-mission accomplished! The RaptorJIT virtual machine is now exposing its raw state to the outside world very efficiently, and the code is so simple that we can be confident about putting it into production.\nStudio\n\nThe second part of the problem is to extract high-level information from the logs. We are not interested in reading hex dumps! We want the tooling to present really high-level information about which code has been JITed, how the compiled code has been optimized, which compilation attempts failed and why, and which code is hot in the profiler, and so on.\n\nWe solve this problem using Studio, which is \"an extensible debugger for the data produced by complex applications.\" Studio is the perfect fit for this application - as it should be, since this problem was the motivation for creating the Studio project :-).\n\nWe take the direct \"brute force\" approach. This is conceptually like reading a coredump into gdb and writing macros to inspect it, but Studio means the tools will be written in Pharo Smalltalk with any awkward chores offloaded with Nix scripts.\n\nHere is the plan of attack:\n\n    Read RaptorJIT DWARF metadata to understand the memory layout of native C objects.\n    Decode application types (GCproto, GCstr, etc) into higher-level Smalltalk objects.\n    Extend the Glamorous Inspector framework to interactively browse our objects.\n    Use the Agile Visualization framework to visualize the more complex objects.\n\nLet's do this!\nRead RaptorJIT DWARF metadata\n\nLooking at DWARF for the first time, several things are immediately apparent:\n\n    The DWARF format is elaborate and arcane.\n    We wouldn't want to touch the libdwarf C library with a ten foot pole.\n    None of the dwarf2foo utilities on the internet seem to really work.\n\nThis is great news: it means that we are perfectly justified in cheating. (The alternative would be to become DWARF experts, but what we are really trying to do here is develop a JIT compiler, remember?)\n\nCheating is easy with Nix. Nix provides \"dependency heaven.\" We can write simple scripts, we can use arbitrary versions of random utility programs, and we can be confident that everything will work the same way every time.\n\nWe create a Nix API with an elf2json function that converts a messy ELF file (produced by clang/gcc during RaptorJIT compilation) into a simple JSON description of what we care about, which are the definitions of types and #define macros and so on."
      },
      "date": 1635946973227
    }
  ]
}