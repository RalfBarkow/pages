{
  "title": "Sandboxing",
  "story": [
    {
      "type": "markdown",
      "id": "1f7f4d5d438d602d",
      "text": "[[khinsen]]\n> Perhaps sandboxing should be complemented by app surveillance and conventions. Example: make Teliva log all communication with servers, make the logs easy to access, and declare any app communicating in anything but plain text as suspicious. That's not 100% protection of course, but encouraging users to have a look at the data exchanges and encouraging comprehensible formats and protocols could in the long run establish norms for trusting apps.\n"
    },
    {
      "type": "markdown",
      "id": "240ca493d7e77ee2",
      "text": "Thinking about this further... Is sandboxing the best way to provide security in malleable systems? Should it be the only/main approach? In a system that its users are supposed to be able to inspect and modify, maybe other ways to create trust in software are more appropriate? Imagine you could see the resources that an app accesses, and the data it exchanges with the outside world, in a straightforward way, just like you can access the source code in [[Teliva]]. Wouldn't it make sense then to treat security issues as bugs? If you see a suspicious access, you open an issue in a publicly visible place, and discuss it within a developer-user community. In the long run, people would trust apps to be nice in much the same way as they come to trust apps to work correctly. The role of sandboxing infrastructure would then not be to ensure security by hard rules, but to make it easy to write code that uses resources transparently."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Sandboxing",
        "story": []
      },
      "date": 1640695674792
    },
    {
      "item": {
        "type": "factory",
        "id": "1f7f4d5d438d602d"
      },
      "id": "1f7f4d5d438d602d",
      "type": "add",
      "date": 1640695690438
    },
    {
      "type": "edit",
      "id": "1f7f4d5d438d602d",
      "item": {
        "type": "paragraph",
        "id": "1f7f4d5d438d602d",
        "text": "[[khinsen]]\n> Perhaps sandboxing should be complemented by app surveillance and conventions. Example: make Teliva log all communication with servers, make the logs easy to access, and declare any app communicating in anything but plain text as suspicious. That's not 100% protection of course, but encouraging users to have a look at the data exchanges and encouraging comprehensible formats and protocols could in the long run establish norms for trusting apps.\nThinking about this further... Is sandboxing the best way to provide security in malleable systems? Should it be the only/main approach? In a system that its users are supposed to be able to inspect and modify, maybe other ways to create trust in software are more appropriate? Imagine you could see the resources that an app accesses, and the data it exchanges with the outside world, in a straightforward way, just like you can access the source code in Teliva. Wouldn't it make sense then to treat security issues as bugs? If you see a suspicious access, you open an issue in a publicly visible place, and discuss it within a developer-user community. In the long run, people would trust apps to be nice in much the same way as they come to trust apps to work correctly. The role of sandboxing infrastructure would then not be to ensure security by hard rules, but to make it easy to write code that uses resources transparently."
      },
      "date": 1640695719318
    },
    {
      "type": "edit",
      "id": "1f7f4d5d438d602d",
      "item": {
        "type": "markdown",
        "id": "1f7f4d5d438d602d",
        "text": "[[khinsen]]\n> Perhaps sandboxing should be complemented by app surveillance and conventions. Example: make Teliva log all communication with servers, make the logs easy to access, and declare any app communicating in anything but plain text as suspicious. That's not 100% protection of course, but encouraging users to have a look at the data exchanges and encouraging comprehensible formats and protocols could in the long run establish norms for trusting apps.\nThinking about this further... Is sandboxing the best way to provide security in malleable systems? Should it be the only/main approach? In a system that its users are supposed to be able to inspect and modify, maybe other ways to create trust in software are more appropriate? Imagine you could see the resources that an app accesses, and the data it exchanges with the outside world, in a straightforward way, just like you can access the source code in Teliva. Wouldn't it make sense then to treat security issues as bugs? If you see a suspicious access, you open an issue in a publicly visible place, and discuss it within a developer-user community. In the long run, people would trust apps to be nice in much the same way as they come to trust apps to work correctly. The role of sandboxing infrastructure would then not be to ensure security by hard rules, but to make it easy to write code that uses resources transparently."
      },
      "date": 1640695720038
    },
    {
      "type": "edit",
      "id": "1f7f4d5d438d602d",
      "item": {
        "type": "markdown",
        "id": "1f7f4d5d438d602d",
        "text": "[[khinsen]]\n> Perhaps sandboxing should be complemented by app surveillance and conventions. Example: make Teliva log all communication with servers, make the logs easy to access, and declare any app communicating in anything but plain text as suspicious. That's not 100% protection of course, but encouraging users to have a look at the data exchanges and encouraging comprehensible formats and protocols could in the long run establish norms for trusting apps.\n"
      },
      "date": 1640695745267
    },
    {
      "type": "add",
      "id": "240ca493d7e77ee2",
      "item": {
        "type": "markdown",
        "id": "240ca493d7e77ee2",
        "text": "Thinking about this further... Is sandboxing the best way to provide security in malleable systems? Should it be the only/main approach? In a system that its users are supposed to be able to inspect and modify, maybe other ways to create trust in software are more appropriate? Imagine you could see the resources that an app accesses, and the data it exchanges with the outside world, in a straightforward way, just like you can access the source code in Teliva. Wouldn't it make sense then to treat security issues as bugs? If you see a suspicious access, you open an issue in a publicly visible place, and discuss it within a developer-user community. In the long run, people would trust apps to be nice in much the same way as they come to trust apps to work correctly. The role of sandboxing infrastructure would then not be to ensure security by hard rules, but to make it easy to write code that uses resources transparently."
      },
      "after": "1f7f4d5d438d602d",
      "date": 1640695757588
    },
    {
      "type": "edit",
      "id": "240ca493d7e77ee2",
      "item": {
        "type": "markdown",
        "id": "240ca493d7e77ee2",
        "text": "Thinking about this further... Is sandboxing the best way to provide security in malleable systems? Should it be the only/main approach? In a system that its users are supposed to be able to inspect and modify, maybe other ways to create trust in software are more appropriate? Imagine you could see the resources that an app accesses, and the data it exchanges with the outside world, in a straightforward way, just like you can access the source code in [[Teliva]]. Wouldn't it make sense then to treat security issues as bugs? If you see a suspicious access, you open an issue in a publicly visible place, and discuss it within a developer-user community. In the long run, people would trust apps to be nice in much the same way as they come to trust apps to work correctly. The role of sandboxing infrastructure would then not be to ensure security by hard rules, but to make it easy to write code that uses resources transparently."
      },
      "date": 1640696016358
    }
  ]
}