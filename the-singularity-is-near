{
  "title": "The Singularity Is Near",
  "story": [
    {
      "type": "html",
      "text": "Has somebody read this book of the futurologist [[Ray Kurzweil]]?",
      "id": "c8f80188127c071856197db2657ba117"
    },
    {
      "type": "html",
      "text": "\nWhat role do the Wikis play in this scenario?",
      "id": "cf906907333c6c5db378080fae199abe"
    },
    {
      "type": "html",
      "text": "((Perhaps some stuff of [[The Singularity]] can be refactored to here.))",
      "id": "48e0d17e1359a6ae764ed34d159b1a99"
    },
    {
      "type": "html",
      "text": "<i>You will probably receive more answers, and higher quality answers, if you focus your question more. Nonetheless, some random early answers:</i>",
      "id": "3c75d5d6296e7ffbf68d32176a4961bf"
    },
    {
      "type": "html",
      "text": "<i>In high profile singularity-related books published thus far, wikis do not, to my knowledge, play a role. [[Vernor Vinge]] gave a heavy nod to Usenet groups, while making it obvious to insiders that he was an outsider, despite...various other positives I won't detail for the moment.</i>",
      "id": "167a3eff19d9460b9e426ce5bf63e107"
    },
    {
      "type": "html",
      "text": "<i>In terms of how wikis <b>might</b> play a role in singularity-related futures (which will not involve an asymptote to infinity at a finite point in time), as far as I know, the subject is wide open. How do <b>you</b> think wikis <b>should</b> figure into such futures? The scaling problems that c2 and wikipedia have had are somewhat sobering, albeit by no means a sign that such thinking should be abandoned.</i>",
      "id": "8968dea03e11b72c5f87d1295282ef9a"
    },
    {
      "type": "html",
      "text": "\nOnce the Singularity is upon us I will hopefully be able to boost my biological capacity using nano-robots and/or genetic engineering and/or a friendly AI plug-in, so I can finish reading Kurweil's book. As it stands, I am consuming an exponentially increasing amount of willpower, from what I assume to be a linearly increasing resource. I think this is mainly due to the amount of repetition in the text. But so far, it seems to be saying that I will hopefully be able to boost my biological capacity using nano-robots and/or genetic engineering and/or a friendly AI plug-in.",
      "id": "5215c09b61f4aa785e4e944258a1d0aa"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "c442fa25f5d83d2d757270b3012425c8"
    },
    {
      "type": "html",
      "text": "\nThe popular [[Sci Fi]] myth of the \"singularity\" disregards the way growing systems typically undergo a growth spurt. They follow an S-curve:",
      "id": "5b839e4cc0ecb5a260870429b981d07f"
    },
    {
      "type": "code",
      "text": "         _____\n        /\n       /\n _____/",
      "id": "49c3217538f906049e1ad9f33bdea5f7"
    },
    {
      "type": "html",
      "text": "\nOur modern technology is about halfway up that curve. The \"singularity\" ideal states the curve will not level off; it will shoot up asymptotically, as computers perform all their science research in-silico, and give us more and more awesome toys.",
      "id": "7704545b3604a913fadd816a7cedff31"
    },
    {
      "type": "html",
      "text": "\nHowever, if the rate at which scientific research produces technologically useful results starts going down, then we will level off. A billion dollars of research will not produce new consumer items worth a billion dollars. If this happens, the singularity will not happen, and humans will have enough time to adjust their cultures to each new awesome toy.  --[[Phl Ip]]",
      "id": "34fc224164aecd9d3f168d6cb5cb0c13"
    },
    {
      "type": "html",
      "text": "\nPersonally, I think we aren't going to be here to see this future....be it global warming, fundamentalist religion, AIDS or an asteroid.\nWe think we are advancing, but the average person is going backwards.  I would love to see a graph of, say IQ versus number of children....'[[Nissim Hadar]]",
      "id": "991ad1b0c607cc3174c214d550ed97c1"
    },
    {
      "type": "html",
      "text": "<i>Apocalyptic thinking is not a healthy way of living and damages your karma.</i>",
      "id": "6efb70bc09a4f1ce0769ab7ecc38fdeb"
    },
    {
      "type": "html",
      "text": "\nThat doesn't change the facts ....",
      "id": "931278ae90abe786651cf8dcdbea80db"
    },
    {
      "type": "html",
      "text": "See original on  [http://c2.com/cgi/wiki?TheSingularityIsNear c2.com]",
      "id": "4cadad0c303ca7a39ed680d3dab03750"
    }
  ],
  "journal": [
    {
      "date": 1192947131000,
      "id": "189a0c347f649f5fc5e86d13e9130511",
      "type": "create",
      "item": {
        "title": "The Singularity Is Near",
        "story": [
          {
            "type": "html",
            "text": "Has somebody read this book of the futurologist [[Ray Kurzweil]]?",
            "id": "c8f80188127c071856197db2657ba117"
          },
          {
            "type": "html",
            "text": "\nWhat role do the Wikis play in this scenario?",
            "id": "cf906907333c6c5db378080fae199abe"
          },
          {
            "type": "html",
            "text": "((Perhaps some stuff of [[The Singularity]] can be refactored to here.))",
            "id": "48e0d17e1359a6ae764ed34d159b1a99"
          },
          {
            "type": "html",
            "text": "<i>You will probably receive more answers, and higher quality answers, if you focus your question more. Nonetheless, some random early answers:</i>",
            "id": "3c75d5d6296e7ffbf68d32176a4961bf"
          },
          {
            "type": "html",
            "text": "<i>In high profile singularity-related books published thus far, wikis do not, to my knowledge, play a role. [[Vernor Vinge]] gave a heavy nod to Usenet groups, while making it obvious to insiders that he was an outsider, despite...various other positives I won't detail for the moment.</i>",
            "id": "167a3eff19d9460b9e426ce5bf63e107"
          },
          {
            "type": "html",
            "text": "<i>In terms of how wikis <b>might</b> play a role in singularity-related futures (which will not involve an asymptote to infinity at a finite point in time), as far as I know, the subject is wide open. How do <b>you</b> think wikis <b>should</b> figure into such futures? The scaling problems that c2 and wikipedia have had are somewhat sobering, albeit by no means a sign that such thinking should be abandoned.</i>",
            "id": "8968dea03e11b72c5f87d1295282ef9a"
          },
          {
            "type": "html",
            "text": "\nOnce the Singularity is upon us I will hopefully be able to boost my biological capacity using nano-robots and/or genetic engineering and/or a friendly AI plug-in, so I can finish reading Kurweil's book. As it stands, I am consuming an exponentially increasing amount of willpower, from what I assume to be a linearly increasing resource. I think this is mainly due to the amount of repetition in the text. But so far, it seems to be saying that I will hopefully be able to boost my biological capacity using nano-robots and/or genetic engineering and/or a friendly AI plug-in.",
            "id": "5215c09b61f4aa785e4e944258a1d0aa"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "c442fa25f5d83d2d757270b3012425c8"
          },
          {
            "type": "html",
            "text": "\nThe popular [[Sci Fi]] myth of the \"singularity\" disregards the way growing systems typically undergo a growth spurt. They follow an S-curve:",
            "id": "5b839e4cc0ecb5a260870429b981d07f"
          },
          {
            "type": "code",
            "text": "         _____\n        /\n       /\n _____/",
            "id": "49c3217538f906049e1ad9f33bdea5f7"
          },
          {
            "type": "html",
            "text": "\nOur modern technology is about halfway up that curve. The \"singularity\" ideal states the curve will not level off; it will shoot up asymptotically, as computers perform all their science research in-silico, and give us more and more awesome toys.",
            "id": "7704545b3604a913fadd816a7cedff31"
          },
          {
            "type": "html",
            "text": "\nHowever, if the rate at which scientific research produces technologically useful results starts going down, then we will level off. A billion dollars of research will not produce new consumer items worth a billion dollars. If this happens, the singularity will not happen, and humans will have enough time to adjust their cultures to each new awesome toy.  --[[Phl Ip]]",
            "id": "34fc224164aecd9d3f168d6cb5cb0c13"
          },
          {
            "type": "html",
            "text": "\nPersonally, I think we aren't going to be here to see this future....be it global warming, fundamentalist religion, AIDS or an asteroid.\nWe think we are advancing, but the average person is going backwards.  I would love to see a graph of, say IQ versus number of children....'[[Nissim Hadar]]",
            "id": "991ad1b0c607cc3174c214d550ed97c1"
          },
          {
            "type": "html",
            "text": "<i>Apocalyptic thinking is not a healthy way of living and damages your karma.</i>",
            "id": "6efb70bc09a4f1ce0769ab7ecc38fdeb"
          },
          {
            "type": "html",
            "text": "\nThat doesn't change the facts ....",
            "id": "931278ae90abe786651cf8dcdbea80db"
          },
          {
            "type": "html",
            "text": "See original on  [http://c2.com/cgi/wiki?TheSingularityIsNear c2.com]",
            "id": "4cadad0c303ca7a39ed680d3dab03750"
          }
        ]
      }
    },
    {
      "type": "fork",
      "site": "sfw.c2.com",
      "date": 1674547523039
    }
  ]
}