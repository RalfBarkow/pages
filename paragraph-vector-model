{
  "title": "Paragraph Vector Model",
  "story": [
    {
      "type": "paragraph",
      "id": "d7b2147c01cfa7d7",
      "text": "TSAI, Richard Tzong-Han, LAI, Yu-Ting, PAI, Pi-Ling, WANG, Yu-Chun, HUANG, Sunny Hui-Min and FAN, I.-Chun, 2017. WeisoEvent: A Ming-Weiso Event Analytics Tool with Named Entity Markup and Spatial-Temporal Information Linking. In: DH. 2017. [https://dh2017.adho.org/abstracts/298/298.pdf pdf] [Accessed 19 March 2024]."
    },
    {
      "type": "paragraph",
      "id": "56380fadc8ace6ad",
      "text": "In\t clustering\t algorithms,\t each\t paragraph\t is\t represented\t as\t a\t vector.\t In\t previous\t studies,\t paragraphs\t have\t been\t represented\t using\t the\t vector\t space\t model\t (VSM),\t which\t represents\t each\t text\t as\t a\t feature\tvector\tof\tterms.\tHowever,\tthis\tapproach\tloses\t the\t ordering\t and\t ignores\t semantics.\t Yet\t another\t representation\t scheme\t inspired\t by\t [[word2vec]]\t is\t the\t “[[Paragraph Vector]]”\t proposed\t by\t (Le\t and\t Mikolov,\t 2014),\t an\t unsupervised\t framework\t that\t learns\t continuous\t distributed\t vectors\t for\t pieces\t of\t text.\t In\t their\t model,\t entire\t paragraphs\t are\t represented\t as\t vectors.\tThe\tvector\trepresentation\tis\ttrained\tto\tpredict\t the\t words\t in\t a\t paragraph.\t More\t precisely,\t they\t concatenate\t the\t paragraph\t vector\t with\t several\t word\t vectors\t from\t a\t paragraph\t and\t predict\t the\t following\t word\t in\t the\t given\t context.\t Le’s\t Paragraph\t Vector\t model\t has\t many\t advantages.\t First,\t it\t is\t mostly\t unsupervised\t and\t works\t well\t with\t sparsely\t labeled\t data.\t Second,\t it\t is\t suitable\t for\t text\t strings\t of\t various\t lengths,\tranging\tfrom\tsentences\tto\twhole\tdocuments.\t Finally,\tit\tcan\tovercome\tmany\tweaknesses\tof\tthe\tbagof-words\tand\tbag-of-n-grams\tmodels.\tBecause\tit\tdoes\t not\tsuffer\tfrom\tdata\tscarcity\tand\thigh\tdimensionality,\t it\t also\t preserves\t the\t ordering\t and\t semantic\t information."
    },
    {
      "type": "paragraph",
      "id": "cf633ab9e4326861",
      "text": "In\t summary,\t we\t propose\t a\t classification\t method\t which\tis\tbased\ton\tclustering.\tFirst,\twe\temploy\ta\tnamed\t entity\t(NE)\trecognizer\tto\tlabel\ttexts.\tSecond,\twe\ttrain\t a\tparagraph\tvector\tmodel\tto\trepresent\tparagraphs\tas\t vectors.\tThird,\twe\tcluster\tparagraphs\twith\tlength\t<40 characters.\t Finally,\t we\t use\t the\t clustering\t results\t as\t gold-standard\t categories\t with\t which\t to\t train\t a\t support-vector-machines\t classifier\t to\t predict\t other\t paragraphs’\tcategories."
    },
    {
      "type": "paragraph",
      "id": "76def49e126f42aa",
      "text": "We\tcompare\tour\tmethod\twith\tthe\tstate-of-the-art\t paragraph\tclustering\tmethod\tusing\tcontinuous\tvector\t space\t representation\t proposed\t by\t (M.\t Chinea-Rios\t et\t al.,\t 2015).\t They\t use\t word2vec\t to\t learn\t word\t vectors\t and\trepresent\teach\tsentence\tby\tsumming\tthe\tvectors\t of\tthe\twords\tin\tthat\tsentence.\tLike\tChinea-Rios\tet\tal.,\t we\t use\t the\t k-means\t algorithm\t to\t cluster\t vectors.\t We\t set\t the\t number\t of\t clusters\t to\t 68.\t We\t refer\t to\t the\t evaluation\tmeasures\tused\tin\t(Le\tand\tMikolov,\t2014).\t We\t generate\t sets\t of\t three\t paragraphs:\t two\t with\t the\t same\t event\t type\t and\t one\t with\t a\t different\t event\t type.\t Each\t set\t is\t referred\t to\t as\t a\t [[Paragraph Triplet]].\t The\t distance\tbetween\tthe\ttwo\tvectors\twith\tthe\tsame\tevent\t type\tshould\tbe\tcloser\tthan\tthe\tdistance\tbetween\teither\t of\t these\t two\t and\t the\t unrelated\t one.\t We\t collect\t 923\t [[Paragraph Triplet]]s\tand\tcompute\tthe\taccuracy.\tOur\tbest\t configuration\t that\t combines\t word\t dimensions\t and\t named\t entity\t dimensions\t to\t generate\t paragraph\t vectors\t achieves\t an\t accuracy\t of\t 62.49%,\t outperforming\tChinea-Rios\tet\tal.’s\tpure\ttext-clustering\t approach\t(M.\tChinea-Rios\tet\tal.,\t2015)\tby\t24.65%."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Paragraph Vector Model",
        "story": []
      },
      "date": 1710937619567
    },
    {
      "id": "d7b2147c01cfa7d7",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "d7b2147c01cfa7d7",
        "text": "TSAI, Richard Tzong-Han, LAI, Yu-Ting, PAI, Pi-Ling, WANG, Yu-Chun, HUANG, Sunny Hui-Min and FAN, I.-Chun, 2017. WeisoEvent: A Ming-Weiso Event Analytics Tool with Named Entity Markup and Spatial-Temporal Information Linking. In: DH. Online. 2017. Available from: https://dh2017.adho.org/abstracts/298/298.pdf [Accessed 19 March 2024]."
      },
      "attribution": {
        "page": "2024-03-20"
      },
      "date": 1710937624413
    },
    {
      "type": "edit",
      "id": "d7b2147c01cfa7d7",
      "item": {
        "type": "paragraph",
        "id": "d7b2147c01cfa7d7",
        "text": "TSAI, Richard Tzong-Han, LAI, Yu-Ting, PAI, Pi-Ling, WANG, Yu-Chun, HUANG, Sunny Hui-Min and FAN, I.-Chun, 2017. WeisoEvent: A Ming-Weiso Event Analytics Tool with Named Entity Markup and Spatial-Temporal Information Linking. In: DH. 2017. [https://dh2017.adho.org/abstracts/298/298.pdf pdf] [Accessed 19 March 2024]."
      },
      "date": 1710937649241
    },
    {
      "item": {
        "type": "factory",
        "id": "56380fadc8ace6ad"
      },
      "id": "56380fadc8ace6ad",
      "type": "add",
      "after": "d7b2147c01cfa7d7",
      "date": 1710937948181
    },
    {
      "type": "edit",
      "id": "56380fadc8ace6ad",
      "item": {
        "type": "paragraph",
        "id": "56380fadc8ace6ad",
        "text": "In\t clustering\t algorithms,\t each\t paragraph\t is\t represented\t as\t a\t vector.\t In\t previous\t studies,\t paragraphs\t have\t been\t represented\t using\t the\t vector\t space\t model\t (VSM),\t which\t represents\t each\t text\t as\t a\t feature\tvector\tof\tterms.\tHowever,\tthis\tapproach\tloses\t the\t ordering\t and\t ignores\t semantics.\t Yet\t another\t representation\t scheme\t inspired\t by\t word2vec\t is\t the\t “Paragraph\t Vector”\t proposed\t by\t (Le\t and\t Mikolov,\t 2014),\t an\t unsupervised\t framework\t that\t learns\t continuous\t distributed\t vectors\t for\t pieces\t of\t text.\t In\t their\t model,\t entire\t paragraphs\t are\t represented\t as\t vectors.\tThe\tvector\trepresentation\tis\ttrained\tto\tpredict\t the\t words\t in\t a\t paragraph.\t More\t precisely,\t they\t concatenate\t the\t paragraph\t vector\t with\t several\t word\t vectors\t from\t a\t paragraph\t and\t predict\t the\t following\t word\t in\t the\t given\t context.\t Le’s\t Paragraph\t Vector\t model\t has\t many\t advantages.\t First,\t it\t is\t mostly\t unsupervised\t and\t works\t well\t with\t sparsely\t labeled\t data.\t Second,\t it\t is\t suitable\t for\t text\t strings\t of\t various\t lengths,\tranging\tfrom\tsentences\tto\twhole\tdocuments.\t Finally,\tit\tcan\tovercome\tmany\tweaknesses\tof\tthe\tbagof-words\tand\tbag-of-n-grams\tmodels.\tBecause\tit\tdoes\t not\tsuffer\tfrom\tdata\tscarcity\tand\thigh\tdimensionality,\t it\t also\t preserves\t the\t ordering\t and\t semantic\t information."
      },
      "date": 1710937954553
    },
    {
      "type": "edit",
      "id": "56380fadc8ace6ad",
      "item": {
        "type": "paragraph",
        "id": "56380fadc8ace6ad",
        "text": "In\t clustering\t algorithms,\t each\t paragraph\t is\t represented\t as\t a\t vector.\t In\t previous\t studies,\t paragraphs\t have\t been\t represented\t using\t the\t vector\t space\t model\t (VSM),\t which\t represents\t each\t text\t as\t a\t feature\tvector\tof\tterms.\tHowever,\tthis\tapproach\tloses\t the\t ordering\t and\t ignores\t semantics.\t Yet\t another\t representation\t scheme\t inspired\t by\t [[word2vec]]\t is\t the\t “Paragraph\t Vector”\t proposed\t by\t (Le\t and\t Mikolov,\t 2014),\t an\t unsupervised\t framework\t that\t learns\t continuous\t distributed\t vectors\t for\t pieces\t of\t text.\t In\t their\t model,\t entire\t paragraphs\t are\t represented\t as\t vectors.\tThe\tvector\trepresentation\tis\ttrained\tto\tpredict\t the\t words\t in\t a\t paragraph.\t More\t precisely,\t they\t concatenate\t the\t paragraph\t vector\t with\t several\t word\t vectors\t from\t a\t paragraph\t and\t predict\t the\t following\t word\t in\t the\t given\t context.\t Le’s\t Paragraph\t Vector\t model\t has\t many\t advantages.\t First,\t it\t is\t mostly\t unsupervised\t and\t works\t well\t with\t sparsely\t labeled\t data.\t Second,\t it\t is\t suitable\t for\t text\t strings\t of\t various\t lengths,\tranging\tfrom\tsentences\tto\twhole\tdocuments.\t Finally,\tit\tcan\tovercome\tmany\tweaknesses\tof\tthe\tbagof-words\tand\tbag-of-n-grams\tmodels.\tBecause\tit\tdoes\t not\tsuffer\tfrom\tdata\tscarcity\tand\thigh\tdimensionality,\t it\t also\t preserves\t the\t ordering\t and\t semantic\t information."
      },
      "date": 1710938064635
    },
    {
      "item": {
        "type": "factory",
        "id": "cf633ab9e4326861"
      },
      "id": "cf633ab9e4326861",
      "type": "add",
      "after": "56380fadc8ace6ad",
      "date": 1710938112801
    },
    {
      "type": "edit",
      "id": "cf633ab9e4326861",
      "item": {
        "type": "paragraph",
        "id": "cf633ab9e4326861",
        "text": "In\t summary,\t we\t propose\t a\t classification\t method\t which\tis\tbased\ton\tclustering.\tFirst,\twe\temploy\ta\tnamed\t entity\t(NE)\trecognizer\tto\tlabel\ttexts.\tSecond,\twe\ttrain\t a\tparagraph\tvector\tmodel\tto\trepresent\tparagraphs\tas\t vectors.\tThird,\twe\tcluster\tparagraphs\twith\tlength\t<40"
      },
      "date": 1710938115264
    },
    {
      "type": "edit",
      "id": "cf633ab9e4326861",
      "item": {
        "type": "paragraph",
        "id": "cf633ab9e4326861",
        "text": "In\t summary,\t we\t propose\t a\t classification\t method\t which\tis\tbased\ton\tclustering.\tFirst,\twe\temploy\ta\tnamed\t entity\t(NE)\trecognizer\tto\tlabel\ttexts.\tSecond,\twe\ttrain\t a\tparagraph\tvector\tmodel\tto\trepresent\tparagraphs\tas\t vectors.\tThird,\twe\tcluster\tparagraphs\twith\tlength\t<40 characters.\t Finally,\t we\t use\t the\t clustering\t results\t as\t gold-standard\t categories\t with\t which\t to\t train\t a\t support-vector-machines\t classifier\t to\t predict\t other\t paragraphs’\tcategories."
      },
      "date": 1710938130134
    },
    {
      "item": {
        "type": "factory",
        "id": "76def49e126f42aa"
      },
      "id": "76def49e126f42aa",
      "type": "add",
      "after": "cf633ab9e4326861",
      "date": 1710938147709
    },
    {
      "type": "edit",
      "id": "76def49e126f42aa",
      "item": {
        "type": "paragraph",
        "id": "76def49e126f42aa",
        "text": "We\tcompare\tour\tmethod\twith\tthe\tstate-of-the-art\t paragraph\tclustering\tmethod\tusing\tcontinuous\tvector\t space\t representation\t proposed\t by\t (M.\t Chinea-Rios\t et\t al.,\t 2015).\t They\t use\t word2vec\t to\t learn\t word\t vectors\t and\trepresent\teach\tsentence\tby\tsumming\tthe\tvectors\t of\tthe\twords\tin\tthat\tsentence.\tLike\tChinea-Rios\tet\tal.,\t we\t use\t the\t k-means\t algorithm\t to\t cluster\t vectors.\t We\t set\t the\t number\t of\t clusters\t to\t 68.\t We\t refer\t to\t the\t evaluation\tmeasures\tused\tin\t(Le\tand\tMikolov,\t2014).\t We\t generate\t sets\t of\t three\t paragraphs:\t two\t with\t the\t same\t event\t type\t and\t one\t with\t a\t different\t event\t type.\t Each\t set\t is\t referred\t to\t as\t a\t paragraph\t triplet.\t The\t distance\tbetween\tthe\ttwo\tvectors\twith\tthe\tsame\tevent\t type\tshould\tbe\tcloser\tthan\tthe\tdistance\tbetween\teither\t of\t these\t two\t and\t the\t unrelated\t one.\t We\t collect\t 923\t paragraph\ttriplets\tand\tcompute\tthe\taccuracy.\tOur\tbest\t configuration\t that\t combines\t word\t dimensions\t and\t named\t entity\t dimensions\t to\t generate\t paragraph\t vectors\t achieves\t an\t accuracy\t of\t 62.49%,\t outperforming\tChinea-Rios\tet\tal.’s\tpure\ttext-clustering\t approach\t(M.\tChinea-Rios\tet\tal.,\t2015)\tby\t24.65%."
      },
      "date": 1710938149482
    },
    {
      "type": "edit",
      "id": "76def49e126f42aa",
      "item": {
        "type": "paragraph",
        "id": "76def49e126f42aa",
        "text": "We\tcompare\tour\tmethod\twith\tthe\tstate-of-the-art\t paragraph\tclustering\tmethod\tusing\tcontinuous\tvector\t space\t representation\t proposed\t by\t (M.\t Chinea-Rios\t et\t al.,\t 2015).\t They\t use\t word2vec\t to\t learn\t word\t vectors\t and\trepresent\teach\tsentence\tby\tsumming\tthe\tvectors\t of\tthe\twords\tin\tthat\tsentence.\tLike\tChinea-Rios\tet\tal.,\t we\t use\t the\t k-means\t algorithm\t to\t cluster\t vectors.\t We\t set\t the\t number\t of\t clusters\t to\t 68.\t We\t refer\t to\t the\t evaluation\tmeasures\tused\tin\t(Le\tand\tMikolov,\t2014).\t We\t generate\t sets\t of\t three\t paragraphs:\t two\t with\t the\t same\t event\t type\t and\t one\t with\t a\t different\t event\t type.\t Each\t set\t is\t referred\t to\t as\t a\t [[Paragraph Triplet]].\t The\t distance\tbetween\tthe\ttwo\tvectors\twith\tthe\tsame\tevent\t type\tshould\tbe\tcloser\tthan\tthe\tdistance\tbetween\teither\t of\t these\t two\t and\t the\t unrelated\t one.\t We\t collect\t 923\t [[Paragraph Triplet]]s\tand\tcompute\tthe\taccuracy.\tOur\tbest\t configuration\t that\t combines\t word\t dimensions\t and\t named\t entity\t dimensions\t to\t generate\t paragraph\t vectors\t achieves\t an\t accuracy\t of\t 62.49%,\t outperforming\tChinea-Rios\tet\tal.’s\tpure\ttext-clustering\t approach\t(M.\tChinea-Rios\tet\tal.,\t2015)\tby\t24.65%."
      },
      "date": 1710938254893
    },
    {
      "type": "edit",
      "id": "56380fadc8ace6ad",
      "item": {
        "type": "paragraph",
        "id": "56380fadc8ace6ad",
        "text": "In\t clustering\t algorithms,\t each\t paragraph\t is\t represented\t as\t a\t vector.\t In\t previous\t studies,\t paragraphs\t have\t been\t represented\t using\t the\t vector\t space\t model\t (VSM),\t which\t represents\t each\t text\t as\t a\t feature\tvector\tof\tterms.\tHowever,\tthis\tapproach\tloses\t the\t ordering\t and\t ignores\t semantics.\t Yet\t another\t representation\t scheme\t inspired\t by\t [[word2vec]]\t is\t the\t “[[Paragraph Vector]]”\t proposed\t by\t (Le\t and\t Mikolov,\t 2014),\t an\t unsupervised\t framework\t that\t learns\t continuous\t distributed\t vectors\t for\t pieces\t of\t text.\t In\t their\t model,\t entire\t paragraphs\t are\t represented\t as\t vectors.\tThe\tvector\trepresentation\tis\ttrained\tto\tpredict\t the\t words\t in\t a\t paragraph.\t More\t precisely,\t they\t concatenate\t the\t paragraph\t vector\t with\t several\t word\t vectors\t from\t a\t paragraph\t and\t predict\t the\t following\t word\t in\t the\t given\t context.\t Le’s\t Paragraph\t Vector\t model\t has\t many\t advantages.\t First,\t it\t is\t mostly\t unsupervised\t and\t works\t well\t with\t sparsely\t labeled\t data.\t Second,\t it\t is\t suitable\t for\t text\t strings\t of\t various\t lengths,\tranging\tfrom\tsentences\tto\twhole\tdocuments.\t Finally,\tit\tcan\tovercome\tmany\tweaknesses\tof\tthe\tbagof-words\tand\tbag-of-n-grams\tmodels.\tBecause\tit\tdoes\t not\tsuffer\tfrom\tdata\tscarcity\tand\thigh\tdimensionality,\t it\t also\t preserves\t the\t ordering\t and\t semantic\t information."
      },
      "date": 1710938436308
    },
    {
      "type": "fork",
      "site": "localhost:3000",
      "date": 1710938645207
    },
    {
      "type": "fork",
      "site": "wiki.ralfbarkow.ch",
      "date": 1711030730536
    }
  ]
}