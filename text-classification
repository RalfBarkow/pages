{
  "title": "Text Classification",
  "story": [
    {
      "type": "paragraph",
      "id": "42178f79fd9c21c3",
      "text": "The self-organizing map has already found appreciation for document classification in the information retrieval community. The map display is a highly effective and intuitive metaphor for orientation in the information space established by a document collection. "
    },
    {
      "type": "paragraph",
      "id": "ae490f5412328277",
      "text": "In this paper we discuss ways for using self-organizing maps for document classification. Furthermore, we argue in favor of paying more attention to the fact that document collections lend themselves naturally to a hierarchical structure defined by the subject matter of the documents. We take advantage of this fact by using a hierarchically organized neural network, built up from a number of independent self-organizing maps in order to enable the true establishment of a [[Document Taxonomy]]. "
    },
    {
      "type": "paragraph",
      "id": "48b7c1b93632e2ba",
      "text": "As a highly convenient side effect of using such an architecture, the time needed for training is reduced substantially and the user is provided with an even more intuitive metaphor for visualization. Since the single layers of self-organizing maps represent different aspects of the document collection at different levels of detail, the neural network shows the document collection in a form comparable to an atlas where the user may easily select the most appropriate degree of granularity depending on the actual focus of interest during the exploration of the document collection."
    },
    {
      "type": "paragraph",
      "id": "4e41a4de59503fc6",
      "text": "Keywords: [[Document Classification]]; [[Hierarchical Feature Maps]]; [[Self-Organizing Map]]s; [[Vectorspace Model]]"
    },
    {
      "type": "pagefold",
      "id": "9dc537f93a3ff27e",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "99c1d5fc47e090c1",
      "text": "MERKL, Dieter, 1998. Text classification with self-organizing maps: Some lessons learned. Neurocomputing. 6 November 1998. Vol. 21, no. 1, p. 61–77. DOI 10.1016/S0925-2312(98)00032-0. \n"
    },
    {
      "type": "paragraph",
      "id": "86f41f3330a190e4",
      "text": "⇒ [[Classification]] ⇒ Text Classification ⇒ [[Self-Organizing Map]] ⇒ [[Document Classification]] ⇒ [[Information Retrieval Community]] ⇒ [[Document Taxonomy]]"
    },
    {
      "type": "markdown",
      "id": "73d71023eb5f62d6",
      "text": "# Introduction"
    },
    {
      "type": "paragraph",
      "id": "756a18e1cc8b5150",
      "text": "During recent years we have witnessed the appearance of an ever increasing flood of miscellaneous written information available in computer accessible form culminating in the advent of massive digital libraries. Powerful methods for organizing, exploring, and searching collections of text documents are thus needed to deal with that mass of information. "
    },
    {
      "type": "paragraph",
      "id": "89df43f8bfec73ea",
      "text": "⇒ [[Information Retrieval Community]]"
    },
    {
      "type": "paragraph",
      "id": "319a28f52a7cc4f0",
      "text": "Classical methods developed by the information retrieval community for searching documents are based on keywords assigned either manually or automatically by indexing the full text of the various documents. "
    },
    {
      "type": "paragraph",
      "id": "267c4501a5f9a787",
      "text": "These methods may be enhanced with proximity search functionality and keyword combination according to Boole’s algebra. "
    },
    {
      "type": "paragraph",
      "id": "1ad2a2950500b5e5",
      "text": "Other widely used approaches rather rely on document similarity measures based on a vector-space representation of the various texts. "
    },
    {
      "type": "paragraph",
      "id": "8345e92ba65a69b3",
      "text": "Still missing, however, are tools providing assistance for explorative search in document collections. Explorative search may be characterized as the struggle to uncover useful information when the user is unaware of appropriate keywords which could guide the search process towards relevant information. The reason for the existence of such a situation is twofold. Firstly, the user often has only limited insight in what is actually contained in the text collection and thus, has just vague expectations on what might be found. On the other hand, a usually convenient characteristic of natural language where the same fact of reality may be described in a number of different ways turns out to be a hindrance in locating relevant information because the same piece of information may be represented by using different sets of keywords. This is often referred to as the vocabulary problem in information retrieval literature [4]."
    },
    {
      "type": "paragraph",
      "id": "db23d8873feb7374",
      "text": "⇒ [[Vocabulary Problem]]"
    },
    {
      "type": "paragraph",
      "id": "94c709d61eb5361a",
      "text": "[…]"
    },
    {
      "type": "paragraph",
      "id": "9bfd894c9c9a2b51",
      "text": "Fig. 4. Architecture of a hierarchical feature map. ⇒ [[Hierarchical Feature Maps]] ⇒ [[MultilayerGraphs.jl]]"
    },
    {
      "type": "paragraph",
      "id": "b1652f664af42f50",
      "text": "[…]"
    },
    {
      "type": "paragraph",
      "id": "32c6fcb2fdc10ab2",
      "text": "p. 71 A valuable property of the hierarchical feature map is the substantial speed-up of the training process as compared to conventional self-organizing maps. An explanation that goes beyond the obvious reduction of the input data dimensions emerges from an investigation in the general properties of the self-organizing training process. In self-organizing maps, the units that are subject to adaptation are selected by means of the neighborhood kernel. "
    },
    {
      "type": "paragraph",
      "id": "d111c2890cf3e7cf",
      "text": "It is common practice that, at the start of the training process, almost the whole map is affected by the presentation of an input pattern. With this strategy, the map is forced to establish initial clusters of similar input items at the outset of learning. By reducing the width of the neighborhood kernel in the course of learning, the training process is able to learn ever finer distinctions within the clusters while the overall topology of cluster arrangement is maintained. "
    },
    {
      "type": "paragraph",
      "id": "05f172f8eb216ca0",
      "text": "The flipside of the coin, however, is that units along the boundary between two clusters tend to be occasionally modified as belonging to either one of these clusters. This interference is the reason for the time-consuming self-organizing process. "
    },
    {
      "type": "paragraph",
      "id": "9621cb5ebecc95b7",
      "text": "Such an interference is dramatically reduced in hierarchical feature maps. This reduction is due to the architecture of this neural network. The topology of the high-level categories is represented in the first layer of the hierarchy. Each of its sub-categories are then independently organized within separate maps at lower levels of the hierarchy. These maps in turn are free from having to represent the overall structure, as this structure is already determined by the architecture of the hierarchical feature map. "
    },
    {
      "type": "paragraph",
      "id": "df02778a7d4a5a02",
      "text": "In summary, much computational effort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its ⇒ [[Learning Rule]]."
    },
    {
      "type": "paragraph",
      "id": "bdb67b6ce71eda16",
      "text": "⇒ [[Hierarchical Feature Maps]]: The utilization of hierarchical feature maps, however, has its limitations, too. Since the architecture of the neural network in terms of the size of the various selforganizing maps and the depth of the hierarchy has to be defined prior to training some knowledge concerning the structure of the document archive is definitely necessary in order to guarantee a suitable document classification."
    },
    {
      "type": "paragraph",
      "id": "8bd75e26d77e0f59",
      "text": "[…]"
    },
    {
      "type": "graphviz",
      "id": "5bd23ff47a24d508",
      "text": "DOT FROM lambda-browsing"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Text Classification",
        "story": []
      },
      "date": 1674677760540
    },
    {
      "id": "42178f79fd9c21c3",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "42178f79fd9c21c3",
        "text": "The self-organizing map has already found appreciation for document classification in the information retrieval community. The map display is a highly effective and intuitive metaphor for orientation in the information space established by a document collection. "
      },
      "date": 1674677765757
    },
    {
      "id": "ae490f5412328277",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "ae490f5412328277",
        "text": "In this paper we discuss ways for using self-organizing maps for document classification. Furthermore, we argue in favor of paying more attention to the fact that document collections lend themselves naturally to a hierarchical structure defined by the subject matter of the documents. We take advantage of this fact by using a hierarchically organized neural network, built up from a number of independent self-organizing maps in order to enable the true establishment of a document taxonomy. As a highly convenient side effect of using such an architecture, the time needed for training is reduced substantially and the user is provided with an even more intuitive metaphor for visualization. Since the single layers of self-organizing maps represent different aspects of the document collection at different levels of detail, the neural network shows the document collection in a form comparable to an atlas where the user may easily select the most appropriate degree of granularity depending on the actual focus of interest during the exploration of the document collection."
      },
      "after": "42178f79fd9c21c3",
      "date": 1674677770803
    },
    {
      "item": {
        "type": "factory",
        "id": "9dc537f93a3ff27e"
      },
      "id": "9dc537f93a3ff27e",
      "type": "add",
      "after": "ae490f5412328277",
      "date": 1674677773205
    },
    {
      "type": "edit",
      "id": "9dc537f93a3ff27e",
      "item": {
        "type": "pagefold",
        "id": "9dc537f93a3ff27e",
        "text": "~"
      },
      "date": 1674677777137
    },
    {
      "id": "99c1d5fc47e090c1",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "99c1d5fc47e090c1",
        "text": "MERKL, Dieter, 1998. Text classification with self-organizing maps: Some lessons learned. Neurocomputing. 6 November 1998. Vol. 21, no. 1, p. 61–77. DOI 10.1016/S0925-2312(98)00032-0. \n"
      },
      "after": "ae490f5412328277",
      "date": 1674677780788
    },
    {
      "id": "9dc537f93a3ff27e",
      "type": "move",
      "order": [
        "42178f79fd9c21c3",
        "ae490f5412328277",
        "9dc537f93a3ff27e",
        "99c1d5fc47e090c1"
      ],
      "date": 1674677786510
    },
    {
      "id": "86f41f3330a190e4",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "86f41f3330a190e4",
        "text": "⇒ [[Classification]] ⇒ [[Text Classification]] ⇒ [[Self-Organizing Map]] ⇒ [[Document Classification]] ⇒ [[Information Retrieval Community]]"
      },
      "after": "99c1d5fc47e090c1",
      "date": 1674677790349
    },
    {
      "type": "edit",
      "id": "86f41f3330a190e4",
      "item": {
        "type": "paragraph",
        "id": "86f41f3330a190e4",
        "text": "⇒ [[Classification]] ⇒ Text Classification ⇒ [[Self-Organizing Map]] ⇒ [[Document Classification]] ⇒ [[Information Retrieval Community]]"
      },
      "date": 1674677812742
    },
    {
      "type": "add",
      "id": "4e41a4de59503fc6",
      "item": {
        "type": "paragraph",
        "id": "4e41a4de59503fc6",
        "text": "Keywords: Document classification; Hierarchical feature maps; Self-organizing maps; Vectorspace model"
      },
      "after": "ae490f5412328277",
      "date": 1674683968869
    },
    {
      "type": "edit",
      "id": "4e41a4de59503fc6",
      "item": {
        "type": "paragraph",
        "id": "4e41a4de59503fc6",
        "text": "Keywords: [[Document Classification]]; Hierarchical feature maps; Self-organizing maps; Vectorspace model"
      },
      "date": 1674684001188
    },
    {
      "type": "edit",
      "id": "4e41a4de59503fc6",
      "item": {
        "type": "paragraph",
        "id": "4e41a4de59503fc6",
        "text": "Keywords: [[Document Classification]]; [[Hierarchical Feature Maps]]; [[Self-Organizing Maps]]; [[Vectorspace Model]]"
      },
      "date": 1674684031261
    },
    {
      "type": "edit",
      "id": "ae490f5412328277",
      "item": {
        "type": "paragraph",
        "id": "ae490f5412328277",
        "text": "In this paper we discuss ways for using self-organizing maps for document classification. Furthermore, we argue in favor of paying more attention to the fact that document collections lend themselves naturally to a hierarchical structure defined by the subject matter of the documents. We take advantage of this fact by using a hierarchically organized neural network, built up from a number of independent self-organizing maps in order to enable the true establishment of a document taxonomy. "
      },
      "date": 1674684152634
    },
    {
      "type": "add",
      "id": "48b7c1b93632e2ba",
      "item": {
        "type": "paragraph",
        "id": "48b7c1b93632e2ba",
        "text": "As a highly convenient side effect of using such an architecture, the time needed for training is reduced substantially and the user is provided with an even more intuitive metaphor for visualization. Since the single layers of self-organizing maps represent different aspects of the document collection at different levels of detail, the neural network shows the document collection in a form comparable to an atlas where the user may easily select the most appropriate degree of granularity depending on the actual focus of interest during the exploration of the document collection."
      },
      "after": "ae490f5412328277",
      "date": 1674684155476
    },
    {
      "type": "edit",
      "id": "ae490f5412328277",
      "item": {
        "type": "paragraph",
        "id": "ae490f5412328277",
        "text": "In this paper we discuss ways for using self-organizing maps for document classification. Furthermore, we argue in favor of paying more attention to the fact that document collections lend themselves naturally to a hierarchical structure defined by the subject matter of the documents. We take advantage of this fact by using a hierarchically organized neural network, built up from a number of independent self-organizing maps in order to enable the true establishment of a [[Document Taxonomy]]. "
      },
      "date": 1674684166911
    },
    {
      "type": "edit",
      "id": "86f41f3330a190e4",
      "item": {
        "type": "paragraph",
        "id": "86f41f3330a190e4",
        "text": "⇒ [[Classification]] ⇒ Text Classification ⇒ [[Self-Organizing Map]] ⇒ [[Document Classification]] ⇒ [[Information Retrieval Community]] ⇒ [[Document Taxonomy]]"
      },
      "date": 1674684226084
    },
    {
      "item": {
        "type": "factory",
        "id": "73d71023eb5f62d6"
      },
      "id": "73d71023eb5f62d6",
      "type": "add",
      "after": "86f41f3330a190e4",
      "date": 1674684371792
    },
    {
      "type": "edit",
      "id": "73d71023eb5f62d6",
      "item": {
        "type": "paragraph",
        "id": "73d71023eb5f62d6",
        "text": "Introduction"
      },
      "date": 1674684376071
    },
    {
      "item": {
        "type": "factory",
        "id": "756a18e1cc8b5150"
      },
      "id": "756a18e1cc8b5150",
      "type": "add",
      "after": "73d71023eb5f62d6",
      "date": 1674684383607
    },
    {
      "type": "edit",
      "id": "756a18e1cc8b5150",
      "item": {
        "type": "paragraph",
        "id": "756a18e1cc8b5150",
        "text": "During recent years we have witnessed the appearance of an ever increasing flood of miscellaneous written information available in computer accessible form culminating in the advent of massive digital libraries. Powerful methods for organizing, exploring, and searching collections of text documents are thus needed to deal with that mass of information. Classical methods developed by the information retrieval community for"
      },
      "date": 1674684385747
    },
    {
      "type": "edit",
      "id": "73d71023eb5f62d6",
      "item": {
        "type": "paragraph",
        "id": "73d71023eb5f62d6",
        "text": "# Introduction"
      },
      "date": 1674684388413
    },
    {
      "type": "edit",
      "id": "73d71023eb5f62d6",
      "item": {
        "type": "markdown",
        "id": "73d71023eb5f62d6",
        "text": "# Introduction"
      },
      "date": 1674684389638
    },
    {
      "type": "edit",
      "id": "756a18e1cc8b5150",
      "item": {
        "type": "paragraph",
        "id": "756a18e1cc8b5150",
        "text": "During recent years we have witnessed the appearance of an ever increasing flood of miscellaneous written information available in computer accessible form culminating in the advent of massive digital libraries. Powerful methods for organizing, exploring, and searching collections of text documents are thus needed to deal with that mass of information. Classical methods developed by the information retrieval community for searching documents are based on keywords assigned either manually or automatically by indexing the full text of the various documents. These methods may be enhanced with proximity search functionality and keyword combination according to Boole’s algebra. Other widely used approaches rather rely on document similarity measures based on a vector-space representation of the various texts. Still missing, however, are tools providing assistance for explorative search in document collections. Explorative search may be characterized as the struggle to uncover useful information when the user is unaware of appropriate keywords which could guide the search process towards relevant information. The reason for the existence of such a situation is twofold. Firstly, the user often has only limited insight in what is actually contained in the text collection and thus, has just vague expectations on what might be found. On the other hand, a usually convenient characteristic of natural language where the same fact of reality may be described in a number of di§erent ways turns out to be a hindrance in locating relevant information because the same piece of information may be represented by using di§erent sets of keywords. This is often referred to as the vocabulary problem in information retrieval literature [4]."
      },
      "date": 1674684438847
    },
    {
      "type": "edit",
      "id": "756a18e1cc8b5150",
      "item": {
        "type": "paragraph",
        "id": "756a18e1cc8b5150",
        "text": "During recent years we have witnessed the appearance of an ever increasing flood of miscellaneous written information available in computer accessible form culminating in the advent of massive digital libraries. Powerful methods for organizing, exploring, and searching collections of text documents are thus needed to deal with that mass of information. "
      },
      "date": 1674684446290
    },
    {
      "type": "add",
      "id": "319a28f52a7cc4f0",
      "item": {
        "type": "paragraph",
        "id": "319a28f52a7cc4f0",
        "text": "Classical methods developed by the information retrieval community for searching documents are based on keywords assigned either manually or automatically by indexing the full text of the various documents. These methods may be enhanced with proximity search functionality and keyword combination according to Boole’s algebra. Other widely used approaches rather rely on document similarity measures based on a vector-space representation of the various texts. Still missing, however, are tools providing assistance for explorative search in document collections. Explorative search may be characterized as the struggle to uncover useful information when the user is unaware of appropriate keywords which could guide the search process towards relevant information. The reason for the existence of such a situation is twofold. Firstly, the user often has only limited insight in what is actually contained in the text collection and thus, has just vague expectations on what might be found. On the other hand, a usually convenient characteristic of natural language where the same fact of reality may be described in a number of di§erent ways turns out to be a hindrance in locating relevant information because the same piece of information may be represented by using di§erent sets of keywords. This is often referred to as the vocabulary problem in information retrieval literature [4]."
      },
      "after": "756a18e1cc8b5150",
      "date": 1674684447365
    },
    {
      "type": "add",
      "id": "89df43f8bfec73ea",
      "item": {
        "type": "paragraph",
        "id": "89df43f8bfec73ea",
        "text": "⇒ [[Information Retrieval Community]]"
      },
      "after": "756a18e1cc8b5150",
      "date": 1674684464368
    },
    {
      "type": "edit",
      "id": "319a28f52a7cc4f0",
      "item": {
        "type": "paragraph",
        "id": "319a28f52a7cc4f0",
        "text": "Classical methods developed by the information retrieval community for searching documents are based on keywords assigned either manually or automatically by indexing the full text of the various documents. "
      },
      "date": 1674684487516
    },
    {
      "type": "add",
      "id": "267c4501a5f9a787",
      "item": {
        "type": "paragraph",
        "id": "267c4501a5f9a787",
        "text": "These methods may be enhanced with proximity search functionality and keyword combination according to Boole’s algebra. "
      },
      "after": "319a28f52a7cc4f0",
      "date": 1674684491815
    },
    {
      "type": "add",
      "id": "1ad2a2950500b5e5",
      "item": {
        "type": "paragraph",
        "id": "1ad2a2950500b5e5",
        "text": "Other widely used approaches rather rely on document similarity measures based on a vector-space representation of the various texts. "
      },
      "after": "267c4501a5f9a787",
      "date": 1674684512838
    },
    {
      "type": "add",
      "id": "8345e92ba65a69b3",
      "item": {
        "type": "paragraph",
        "id": "8345e92ba65a69b3",
        "text": "Still missing, however, are tools providing assistance for explorative search in document collections. Explorative search may be characterized as the struggle to uncover useful information when the user is unaware of appropriate keywords which could guide the search process towards relevant information. The reason for the existence of such a situation is twofold. Firstly, the user often has only limited insight in what is actually contained in the text collection and thus, has just vague expectations on what might be found. On the other hand, a usually convenient characteristic of natural language where the same fact of reality may be described in a number of different ways turns out to be a hindrance in locating relevant information because the same piece of information may be represented by using different sets of keywords. This is often referred to as the vocabulary problem in information retrieval literature [4]."
      },
      "after": "1ad2a2950500b5e5",
      "date": 1674684569965
    },
    {
      "item": {
        "type": "factory",
        "id": "db23d8873feb7374"
      },
      "id": "db23d8873feb7374",
      "type": "add",
      "after": "8345e92ba65a69b3",
      "date": 1674684594169
    },
    {
      "type": "edit",
      "id": "db23d8873feb7374",
      "item": {
        "type": "paragraph",
        "id": "db23d8873feb7374",
        "text": "vocabulary problem "
      },
      "date": 1674684598172
    },
    {
      "type": "edit",
      "id": "db23d8873feb7374",
      "item": {
        "type": "paragraph",
        "id": "db23d8873feb7374",
        "text": "⇒ [[Vocabulary Problem]]"
      },
      "date": 1674684615642
    },
    {
      "item": {
        "type": "factory",
        "id": "94c709d61eb5361a"
      },
      "id": "94c709d61eb5361a",
      "type": "add",
      "after": "db23d8873feb7374",
      "date": 1674684736722
    },
    {
      "type": "edit",
      "id": "94c709d61eb5361a",
      "item": {
        "type": "paragraph",
        "id": "94c709d61eb5361a",
        "text": "[…]"
      },
      "date": 1674684739758
    },
    {
      "type": "add",
      "id": "9bfd894c9c9a2b51",
      "item": {
        "type": "paragraph",
        "id": "9bfd894c9c9a2b51",
        "text": "Fig. 4. Architecture of a hierarchical feature map."
      },
      "after": "94c709d61eb5361a",
      "date": 1674684810105
    },
    {
      "type": "add",
      "id": "b1652f664af42f50",
      "item": {
        "type": "paragraph",
        "id": "b1652f664af42f50",
        "text": "[…]"
      },
      "after": "9bfd894c9c9a2b51",
      "date": 1674684813746
    },
    {
      "type": "edit",
      "id": "9bfd894c9c9a2b51",
      "item": {
        "type": "paragraph",
        "id": "9bfd894c9c9a2b51",
        "text": "Fig. 4. Architecture of a hierarchical feature map. ⇒ [[Hierarchical Feature Maps]]"
      },
      "date": 1674684842999
    },
    {
      "type": "edit",
      "id": "9bfd894c9c9a2b51",
      "item": {
        "type": "paragraph",
        "id": "9bfd894c9c9a2b51",
        "text": "Fig. 4. Architecture of a hierarchical feature map. ⇒ [[Hierarchical Feature Maps]] ⇒ [MultilayerGraphs.jl]]"
      },
      "date": 1674685008509
    },
    {
      "type": "edit",
      "id": "9bfd894c9c9a2b51",
      "item": {
        "type": "paragraph",
        "id": "9bfd894c9c9a2b51",
        "text": "Fig. 4. Architecture of a hierarchical feature map. ⇒ [[Hierarchical Feature Maps]] ⇒ [[MultilayerGraphs.jl]]"
      },
      "date": 1674685014051
    },
    {
      "type": "add",
      "id": "32c6fcb2fdc10ab2",
      "item": {
        "type": "paragraph",
        "id": "32c6fcb2fdc10ab2",
        "text": "p. 71 A valuable property of the hierarchical feature map is the substantial speed-up of the training process as compared to conventional self-organizing maps. An explanation that goes beyond the obvious reduction of the input data dimensions emerges from an investigation in the general properties of the self-organizing training process. In self-organizing maps, the units that are subject to adaptation are selected by means of the neighborhood kernel. It is common practice that, at the start of the training process, almost the whole map is a§ected by the presentation of an input pattern. With this strategy, the map is forced to establish initial clusters of similar input items at the outset of learning. By reducing the width of the neighborhood kernel in the course of learning, the training process is able to learn ever finer distinctions within the clusters while the overall topology of cluster arrangement is maintained. The flipside of the coin, however, is that units along the boundary between two clusters tend to be occasionally modified as belonging to either one of these clusters. This interference is the reason for the time-consuming self-organizing process. Such an interference is dramatically reduced in hierarchical feature maps. This reduction is due to the architecture of this neural network. The topology of the high-level categories is represented in the first layer of the hierarchy. Each of its sub-categories are then independently organized within separate maps at lower levels of the hierarchy. These maps in turn are free from having to represent the overall structure, as this structure is already determined by the architecture of the hierarchical feature map. In summary, much computational e§ort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its learning rule."
      },
      "after": "b1652f664af42f50",
      "date": 1674685062744
    },
    {
      "type": "add",
      "id": "8bd75e26d77e0f59",
      "item": {
        "type": "paragraph",
        "id": "8bd75e26d77e0f59",
        "text": "[…]"
      },
      "after": "32c6fcb2fdc10ab2",
      "date": 1674685067076
    },
    {
      "type": "edit",
      "id": "32c6fcb2fdc10ab2",
      "item": {
        "type": "paragraph",
        "id": "32c6fcb2fdc10ab2",
        "text": "p. 71 A valuable property of the hierarchical feature map is the substantial speed-up of the training process as compared to conventional self-organizing maps. An explanation that goes beyond the obvious reduction of the input data dimensions emerges from an investigation in the general properties of the self-organizing training process. In self-organizing maps, the units that are subject to adaptation are selected by means of the neighborhood kernel. "
      },
      "date": 1674685103734
    },
    {
      "type": "add",
      "id": "d111c2890cf3e7cf",
      "item": {
        "type": "paragraph",
        "id": "d111c2890cf3e7cf",
        "text": "It is common practice that, at the start of the training process, almost the whole map is a§ected by the presentation of an input pattern. With this strategy, the map is forced to establish initial clusters of similar input items at the outset of learning. By reducing the width of the neighborhood kernel in the course of learning, the training process is able to learn ever finer distinctions within the clusters while the overall topology of cluster arrangement is maintained. The flipside of the coin, however, is that units along the boundary between two clusters tend to be occasionally modified as belonging to either one of these clusters. This interference is the reason for the time-consuming self-organizing process. Such an interference is dramatically reduced in hierarchical feature maps. This reduction is due to the architecture of this neural network. The topology of the high-level categories is represented in the first layer of the hierarchy. Each of its sub-categories are then independently organized within separate maps at lower levels of the hierarchy. These maps in turn are free from having to represent the overall structure, as this structure is already determined by the architecture of the hierarchical feature map. In summary, much computational e§ort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its learning rule."
      },
      "after": "32c6fcb2fdc10ab2",
      "date": 1674685104923
    },
    {
      "type": "edit",
      "id": "d111c2890cf3e7cf",
      "item": {
        "type": "paragraph",
        "id": "d111c2890cf3e7cf",
        "text": "It is common practice that, at the start of the training process, almost the whole map is affected by the presentation of an input pattern. With this strategy, the map is forced to establish initial clusters of similar input items at the outset of learning. By reducing the width of the neighborhood kernel in the course of learning, the training process is able to learn ever finer distinctions within the clusters while the overall topology of cluster arrangement is maintained. The flipside of the coin, however, is that units along the boundary between two clusters tend to be occasionally modified as belonging to either one of these clusters. This interference is the reason for the time-consuming self-organizing process. Such an interference is dramatically reduced in hierarchical feature maps. This reduction is due to the architecture of this neural network. The topology of the high-level categories is represented in the first layer of the hierarchy. Each of its sub-categories are then independently organized within separate maps at lower levels of the hierarchy. These maps in turn are free from having to represent the overall structure, as this structure is already determined by the architecture of the hierarchical feature map. In summary, much computational e§ort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its learning rule."
      },
      "date": 1674685118606
    },
    {
      "type": "edit",
      "id": "d111c2890cf3e7cf",
      "item": {
        "type": "paragraph",
        "id": "d111c2890cf3e7cf",
        "text": "It is common practice that, at the start of the training process, almost the whole map is affected by the presentation of an input pattern. With this strategy, the map is forced to establish initial clusters of similar input items at the outset of learning. By reducing the width of the neighborhood kernel in the course of learning, the training process is able to learn ever finer distinctions within the clusters while the overall topology of cluster arrangement is maintained. "
      },
      "date": 1674685177825
    },
    {
      "type": "add",
      "id": "05f172f8eb216ca0",
      "item": {
        "type": "paragraph",
        "id": "05f172f8eb216ca0",
        "text": "The flipside of the coin, however, is that units along the boundary between two clusters tend to be occasionally modified as belonging to either one of these clusters. This interference is the reason for the time-consuming self-organizing process. Such an interference is dramatically reduced in hierarchical feature maps. This reduction is due to the architecture of this neural network. The topology of the high-level categories is represented in the first layer of the hierarchy. Each of its sub-categories are then independently organized within separate maps at lower levels of the hierarchy. These maps in turn are free from having to represent the overall structure, as this structure is already determined by the architecture of the hierarchical feature map. In summary, much computational e§ort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its learning rule."
      },
      "after": "d111c2890cf3e7cf",
      "date": 1674685179060
    },
    {
      "type": "edit",
      "id": "05f172f8eb216ca0",
      "item": {
        "type": "paragraph",
        "id": "05f172f8eb216ca0",
        "text": "The flipside of the coin, however, is that units along the boundary between two clusters tend to be occasionally modified as belonging to either one of these clusters. This interference is the reason for the time-consuming self-organizing process. "
      },
      "date": 1674685200338
    },
    {
      "type": "add",
      "id": "9621cb5ebecc95b7",
      "item": {
        "type": "paragraph",
        "id": "9621cb5ebecc95b7",
        "text": "Such an interference is dramatically reduced in hierarchical feature maps. This reduction is due to the architecture of this neural network. The topology of the high-level categories is represented in the first layer of the hierarchy. Each of its sub-categories are then independently organized within separate maps at lower levels of the hierarchy. These maps in turn are free from having to represent the overall structure, as this structure is already determined by the architecture of the hierarchical feature map. In summary, much computational e§ort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its learning rule."
      },
      "after": "05f172f8eb216ca0",
      "date": 1674685201454
    },
    {
      "type": "edit",
      "id": "9621cb5ebecc95b7",
      "item": {
        "type": "paragraph",
        "id": "9621cb5ebecc95b7",
        "text": "Such an interference is dramatically reduced in hierarchical feature maps. This reduction is due to the architecture of this neural network. The topology of the high-level categories is represented in the first layer of the hierarchy. Each of its sub-categories are then independently organized within separate maps at lower levels of the hierarchy. These maps in turn are free from having to represent the overall structure, as this structure is already determined by the architecture of the hierarchical feature map. In summary, much computational effort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its learning rule."
      },
      "date": 1674685231400
    },
    {
      "type": "edit",
      "id": "9621cb5ebecc95b7",
      "item": {
        "type": "paragraph",
        "id": "9621cb5ebecc95b7",
        "text": "Such an interference is dramatically reduced in hierarchical feature maps. This reduction is due to the architecture of this neural network. The topology of the high-level categories is represented in the first layer of the hierarchy. Each of its sub-categories are then independently organized within separate maps at lower levels of the hierarchy. These maps in turn are free from having to represent the overall structure, as this structure is already determined by the architecture of the hierarchical feature map. "
      },
      "date": 1674685246306
    },
    {
      "type": "add",
      "id": "df02778a7d4a5a02",
      "item": {
        "type": "paragraph",
        "id": "df02778a7d4a5a02",
        "text": "In summary, much computational effort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its learning rule."
      },
      "after": "9621cb5ebecc95b7",
      "date": 1674685247751
    },
    {
      "type": "edit",
      "id": "df02778a7d4a5a02",
      "item": {
        "type": "paragraph",
        "id": "df02778a7d4a5a02",
        "text": "In summary, much computational effort is saved due to the fact that the overall structure of clusters is determined by the architecture of the neural network rather than by its ⇒ [[Learning Rule]]."
      },
      "date": 1674685431879
    },
    {
      "item": {
        "type": "factory",
        "id": "5bd23ff47a24d508"
      },
      "id": "5bd23ff47a24d508",
      "type": "add",
      "after": "8bd75e26d77e0f59",
      "date": 1674685438915
    },
    {
      "type": "edit",
      "id": "5bd23ff47a24d508",
      "item": {
        "type": "graphviz",
        "id": "5bd23ff47a24d508",
        "text": "DOT FROM lambda-browsing"
      },
      "date": 1674685448804
    },
    {
      "type": "edit",
      "id": "4e41a4de59503fc6",
      "item": {
        "type": "paragraph",
        "id": "4e41a4de59503fc6",
        "text": "Keywords: [[Document Classification]]; [[Hierarchical Feature Maps]]; [[Self-Organizing Map]]s; [[Vectorspace Model]]"
      },
      "date": 1674685541504
    },
    {
      "type": "add",
      "id": "bdb67b6ce71eda16",
      "item": {
        "type": "paragraph",
        "id": "bdb67b6ce71eda16",
        "text": "⇒ [[Hierarchical Feature Maps]]"
      },
      "after": "df02778a7d4a5a02",
      "date": 1674686961253
    },
    {
      "type": "edit",
      "id": "bdb67b6ce71eda16",
      "item": {
        "type": "paragraph",
        "id": "bdb67b6ce71eda16",
        "text": "⇒ [[Hierarchical Feature Maps]]: The utilization of hierarchical feature maps, however, has its limitations, too. Since the architecture of the neural network in terms of the size of the various selforganizing maps and the depth of the hierarchy has to be defined prior to training some knowledge concerning the structure of the document archive is definitely necessary in order to guarantee a suitable document classification."
      },
      "date": 1674686971258
    }
  ]
}