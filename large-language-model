{
  "title": "Large Language Model",
  "story": [
    {
      "type": "paragraph",
      "id": "b022105a10192836",
      "text": "[[Murray Shanahan]], Talking About Large Language Models [https://fedi.simonwillison.net/@simon/109738632902770815 post], [https://www.arxiv-vanity.com/papers/2212.03551/ page]"
    },
    {
      "type": "paragraph",
      "id": "bd702c62e14b257c",
      "text": "⇒ [[Anthropomorphism of a Language Model]]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Large Language Model",
        "story": []
      },
      "date": 1674919639612
    },
    {
      "id": "b022105a10192836",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "b022105a10192836",
        "text": "Talking About [[Large Language Model]]s [https://fedi.simonwillison.net/@simon/109738632902770815 post], [https://www.arxiv-vanity.com/papers/2212.03551/ page]"
      },
      "date": 1674919648699
    },
    {
      "id": "46b35d893db7771b",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "46b35d893db7771b",
        "text": "Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere."
      },
      "date": 1674919665073
    },
    {
      "item": {
        "type": "factory",
        "id": "3be2fb6c87f9c0ec"
      },
      "id": "3be2fb6c87f9c0ec",
      "type": "add",
      "after": "b022105a10192836",
      "date": 1674919669264
    },
    {
      "type": "edit",
      "id": "3be2fb6c87f9c0ec",
      "item": {
        "type": "pagefold",
        "id": "3be2fb6c87f9c0ec",
        "text": "~"
      },
      "date": 1674919672768
    },
    {
      "id": "3be2fb6c87f9c0ec",
      "type": "move",
      "order": [
        "46b35d893db7771b",
        "3be2fb6c87f9c0ec",
        "b022105a10192836"
      ],
      "date": 1674919674879
    },
    {
      "type": "edit",
      "id": "b022105a10192836",
      "item": {
        "type": "paragraph",
        "id": "b022105a10192836",
        "text": "Talking About Large Language Models [https://fedi.simonwillison.net/@simon/109738632902770815 post], [https://www.arxiv-vanity.com/papers/2212.03551/ page]"
      },
      "date": 1674919684289
    },
    {
      "type": "edit",
      "id": "46b35d893db7771b",
      "item": {
        "type": "paragraph",
        "id": "46b35d893db7771b",
        "text": "Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. "
      },
      "date": 1674919702944
    },
    {
      "type": "add",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere."
      },
      "after": "46b35d893db7771b",
      "date": 1674919703438
    },
    {
      "type": "edit",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. "
      },
      "date": 1674919731039
    },
    {
      "type": "add",
      "id": "58df544defdd8a41",
      "item": {
        "type": "paragraph",
        "id": "58df544defdd8a41",
        "text": "To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere."
      },
      "after": "a1d019b92c524586",
      "date": 1674919732176
    },
    {
      "item": {
        "type": "factory",
        "id": "bd702c62e14b257c"
      },
      "id": "bd702c62e14b257c",
      "type": "add",
      "after": "b022105a10192836",
      "date": 1675058566874
    },
    {
      "type": "edit",
      "id": "bd702c62e14b257c",
      "item": {
        "type": "paragraph",
        "id": "bd702c62e14b257c",
        "text": "⇒ [[Anthropomorphism of a Language Model]]"
      },
      "date": 1675058568742
    },
    {
      "type": "edit",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[knows]”, “believes”, and “thinks”, when describing these systems. "
      },
      "date": 1675058648277
    },
    {
      "type": "edit",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[know]]s”, “believes”, and “thinks”, when describing these systems. "
      },
      "date": 1675058660059
    },
    {
      "type": "edit",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[know]]s”, “[[believe]]s”, and “thinks”, when describing these systems. "
      },
      "date": 1675073844756
    },
    {
      "type": "edit",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[know]]s”, “[[believe]]s”, and “[[think]]s”, when describing these systems. "
      },
      "date": 1675073876100
    },
    {
      "type": "edit",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are [Large Language Model]]s(LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[know]]s”, “[[believe]]s”, and “[[think]]s”, when describing these systems. "
      },
      "date": 1675073988676
    },
    {
      "type": "edit",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are [[Large Language Model]]s(LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[know]]s”, “[[believe]]s”, and “[[think]]s”, when describing these systems. "
      },
      "date": 1675073995665
    },
    {
      "type": "edit",
      "id": "a1d019b92c524586",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are [[Large Language Model]]s (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[know]]s”, “[[believe]]s”, and “[[think]]s”, when describing these systems. "
      },
      "date": 1675074000045
    },
    {
      "type": "remove",
      "id": "46b35d893db7771b",
      "date": 1675074078149
    },
    {
      "type": "remove",
      "id": "a1d019b92c524586",
      "date": 1675074080069
    },
    {
      "type": "remove",
      "id": "58df544defdd8a41",
      "date": 1675074081131
    },
    {
      "type": "remove",
      "id": "3be2fb6c87f9c0ec",
      "date": 1675074083499
    },
    {
      "id": "a4d46b1b83ce40ae",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "a4d46b1b83ce40ae",
        "text": "[[Murray Shanahan]], Talking About Large Language Models [https://fedi.simonwillison.net/@simon/109738632902770815 post], [https://www.arxiv-vanity.com/papers/2212.03551/ page]",
        "alias": "b022105a10192836"
      },
      "after": "b022105a10192836",
      "date": 1675074094081
    },
    {
      "type": "remove",
      "id": "b022105a10192836",
      "date": 1675074096421
    },
    {
      "id": "b022105a10192836",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "b022105a10192836",
        "text": "[[Murray Shanahan]], Talking About Large Language Models [https://fedi.simonwillison.net/@simon/109738632902770815 post], [https://www.arxiv-vanity.com/papers/2212.03551/ page]"
      },
      "date": 1675074110083
    },
    {
      "type": "remove",
      "id": "a4d46b1b83ce40ae",
      "date": 1675074116843
    }
  ]
}