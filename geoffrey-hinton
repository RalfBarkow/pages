{
  "title": "Geoffrey Hinton",
  "story": [
    {
      "type": "paragraph",
      "id": "d4be92ca63088e1e",
      "text": "Possible End of Humanity from AI? [[Geoffrey Hinton]] at MIT Technology Review's EmTech Digital"
    },
    {
      "type": "paragraph",
      "id": "9af705b3e995d7f8",
      "text": "[https://youtu.be/sitHS6UDMJc?t=999 16:39]\n… in a world where people have bad intentions and where the political system is so broken that we can't even decide not to give assault rifles to teenage boys um if you can't solve that problem how are you going to solve this problem?"
    },
    {
      "type": "video",
      "id": "f6021b3796bf58b2",
      "text": "YOUTUBE sitHS6UDMJc\nPossible End of Humanity from AI? Geoffrey Hinton at MIT Technology Review's EmTech Digital"
    },
    {
      "type": "markdown",
      "id": "a15ba3f4e41b4026",
      "text": "[https://youtu.be/sitHS6UDMJc?t=1057 17:34]\n… I wish it was like climate change where you could say if you've got half a brain you'd stop burning carbon um it's clear what you should do about it it's clear that's painful but has to be done uh I don't know of any solution like that to stop these things taking over from us. What we really want I don't think we're going to stop developing them because they're so useful they'll be incredibly useful in medicine and in everything else um so I don't think there's much chance of stopping development. What we want is some way of making sure that even if they're smarter than us um they're going to do things that are beneficial for us that's called the [[Alignment Problem]] …"
    },
    {
      "type": "reference",
      "id": "1f43b5b224c9e28c",
      "site": "papers.dreyeck.ch",
      "slug": "alignment-problem",
      "title": "Alignment Problem",
      "text": "The goal of building safe, human-compatible AI, including potentially AGI/ASI is known as the Alignment Problem."
    },
    {
      "type": "paragraph",
      "id": "bebc596eb482fc79",
      "text": "[https://youtu.be/sitHS6UDMJc?t=1096 18:16] but we need to try and do that in a world where there's Bad actors who want to build robot soldiers that kill people and it seems very hard to me so I'm sorry I'm I'm sounding the alarm and \nsaying we have to worry about this and I wish I had a nice simple solution I could push but I don't but I think it's very important that people get together and think hard about it and see whether there is a solution it's not clear there is a solution so I mean talk to us about"
    },
    {
      "type": "paragraph",
      "id": "c97695229b378c5f",
      "text": "No Technical Fix"
    },
    {
      "type": "paragraph",
      "id": "215c5c09360abae5",
      "text": "[https://youtu.be/sitHS6UDMJc?t=1151 19:11]\nThey write programs and suppose you give them the ability to execute those programs which we'll certainly do …"
    },
    {
      "type": "reference",
      "id": "5015742fea03f464",
      "site": "papers.dreyeck.ch",
      "slug": "establish-rules",
      "title": "Establish Rules",
      "text": "For example, the ability of computational systems to [[establish]] [[Rule]]s as genuine constraints where an analogous human legal system can only [[penalize]] violations makes possible [[Patterns of Organization]] that can only be approximated in society."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Geoffrey Hinton",
        "story": []
      },
      "date": 1685054981775
    },
    {
      "id": "d4be92ca63088e1e",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "d4be92ca63088e1e",
        "text": "Possible End of Humanity from AI? [[Geoffrey Hinton]] at MIT Technology Review's EmTech Digital"
      },
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685054986833
    },
    {
      "id": "9af705b3e995d7f8",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "9af705b3e995d7f8",
        "text": "[https://youtu.be/sitHS6UDMJc?t=999 16:39]\n… in a world where people have bad intentions and where the political system is so broken that we can't even decide not to give assault rifles to teenage boys um if you can't solve that problem how are you going to solve this problem?"
      },
      "after": "d4be92ca63088e1e",
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685054990567
    },
    {
      "id": "f6021b3796bf58b2",
      "type": "add",
      "item": {
        "type": "video",
        "id": "f6021b3796bf58b2",
        "text": "YOUTUBE sitHS6UDMJc\nPossible End of Humanity from AI? Geoffrey Hinton at MIT Technology Review's EmTech Digital"
      },
      "after": "9af705b3e995d7f8",
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685054996506
    },
    {
      "id": "a15ba3f4e41b4026",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "a15ba3f4e41b4026",
        "text": "[https://youtu.be/sitHS6UDMJc?t=1057 17:34]\n… I wish it was like climate change where you could say if you've got half a brain you'd stop burning carbon um it's clear what you should do about it it's clear that's painful but has to be done uh I don't know of any solution like that to stop these things taking over from us. What we really want I don't think we're going to stop developing them because they're so useful they'll be incredibly useful in medicine and in everything else um so I don't think there's much chance of stopping development. What we want is some way of making sure that even if they're smarter than us um they're going to do things that are beneficial for us that's called the [[Alignment Problem]] …"
      },
      "after": "f6021b3796bf58b2",
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685055004468
    },
    {
      "id": "1f43b5b224c9e28c",
      "type": "add",
      "item": {
        "type": "reference",
        "id": "1f43b5b224c9e28c",
        "site": "papers.dreyeck.ch",
        "slug": "alignment-problem",
        "title": "Alignment Problem",
        "text": "The goal of building safe, human-compatible AI, including potentially AGI/ASI is known as the Alignment Problem."
      },
      "after": "a15ba3f4e41b4026",
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685055010518
    },
    {
      "id": "bebc596eb482fc79",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "bebc596eb482fc79",
        "text": "[https://youtu.be/sitHS6UDMJc?t=1096 18:16] but we need to try and do that in a world where there's Bad actors who want to build robot soldiers that kill people and it seems very hard to me so I'm sorry I'm I'm sounding the alarm and \nsaying we have to worry about this and I wish I had a nice simple solution I could push but I don't but I think it's very important that people get together and think hard about it and see whether there is a solution it's not clear there is a solution so I mean talk to us about"
      },
      "after": "1f43b5b224c9e28c",
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685055014618
    },
    {
      "type": "edit",
      "id": "a15ba3f4e41b4026",
      "item": {
        "type": "markdown",
        "id": "a15ba3f4e41b4026",
        "text": "[https://youtu.be/sitHS6UDMJc?t=1057 17:34]\n… I wish it was like climate change where you could say if you've got half a brain you'd stop burning carbon um it's clear what you should do about it it's clear that's painful but has to be done uh I don't know of any solution like that to stop these things taking over from us. What we really want I don't think we're going to stop developing them because they're so useful they'll be incredibly useful in medicine and in everything else um so I don't think there's much chance of stopping development. What we want is some way of making sure that even if they're smarter than us um they're going to do things that are beneficial for us that's called the [[Alignment Problem]] …"
      },
      "date": 1685055045168
    },
    {
      "id": "c97695229b378c5f",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "c97695229b378c5f",
        "text": "No Technical Fix"
      },
      "after": "bebc596eb482fc79",
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685055057167
    },
    {
      "id": "215c5c09360abae5",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "215c5c09360abae5",
        "text": "[https://youtu.be/sitHS6UDMJc?t=1151 19:11]\nThey write programs and suppose you give them the ability to execute those programs which we'll certainly do …"
      },
      "after": "c97695229b378c5f",
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685055061267
    },
    {
      "id": "5015742fea03f464",
      "type": "add",
      "item": {
        "type": "reference",
        "id": "5015742fea03f464",
        "site": "papers.dreyeck.ch",
        "slug": "establish-rules",
        "title": "Establish Rules",
        "text": "For example, the ability of computational systems to [[establish]] [[Rule]]s as genuine constraints where an analogous human legal system can only [[penalize]] violations makes possible [[Patterns of Organization]] that can only be approximated in society."
      },
      "after": "215c5c09360abae5",
      "attribution": {
        "page": "2023-05-17"
      },
      "date": 1685055065232
    }
  ]
}