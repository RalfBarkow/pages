{
  "title": "Latent Semantic Analysis",
  "story": [
    {
      "type": "paragraph",
      "id": "a0fa13b8bf5478c2",
      "text": "is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by cosine similarity between any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents. [https://en.wikipedia.org/wiki/Latent_semantic_analysis wikipedia] "
    },
    {
      "type": "pagefold",
      "id": "6d3e2d102be2d701",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "638c3ab6b01dd31a",
      "text": "How to [[Create a Semantic Space]] in which documents can be compared by their cosine similarity?"
    },
    {
      "type": "paragraph",
      "id": "515faa61835f124a",
      "text": "chat.openai: To create a semantic space in which documents can be compared by their cosine similarity using Latent Semantic Analysis (LSA), the following steps are typically followed:\n\n"
    },
    {
      "type": "paragraph",
      "id": "da21bad0622fc87f",
      "text": "Preprocessing: Clean and preprocess the text data by removing stopwords, stemming, and lemmatization.\n"
    },
    {
      "type": "paragraph",
      "id": "e04a83eccaaa8d0d",
      "text": "Term-document matrix: Create a term-document matrix, where rows represent unique words and columns represent each document. Each cell in the matrix contains the number of occurrences of a given word in a given document.\n"
    },
    {
      "type": "paragraph",
      "id": "0421d873efa2497e",
      "text": "Singular value decomposition (SVD): Apply SVD to the term-document matrix, which reduces the number of rows while preserving the similarity structure among columns. The SVD decomposition results in three matrices: U, S, and V.\n"
    },
    {
      "type": "paragraph",
      "id": "934f17649d93853f",
      "text": "Dimensionality reduction: Select a number of dimensions (k) to reduce the number of rows in the matrix U. This creates a reduced dimensional space, also called a semantic space, in which the documents are represented.\n"
    },
    {
      "type": "paragraph",
      "id": "5a1a2e6fe0234001",
      "text": "Cosine similarity: Compare the similarity between any two documents in the semantic space by calculating the cosine similarity between their respective vectors in the reduced dimensional space. The cosine similarity ranges from -1 to 1, with values close to 1 indicating high similarity and values close to -1 indicating high dissimilarity.\n"
    },
    {
      "type": "paragraph",
      "id": "c0e8ad3e602d54ef",
      "text": "It's worth noting that LSA can be considered as an unsupervised technique, as it doesn't require labeled data. LSA aims to extract latent topics in the data, it is also known as [[Latent Semantic Indexing]] (LSI) as well."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Latent Semantic Analysis",
        "story": []
      },
      "date": 1674903748448
    },
    {
      "item": {
        "type": "factory",
        "id": "a0fa13b8bf5478c2"
      },
      "id": "a0fa13b8bf5478c2",
      "type": "add",
      "date": 1674903749825
    },
    {
      "item": {
        "type": "factory",
        "id": "6d3e2d102be2d701"
      },
      "id": "6d3e2d102be2d701",
      "type": "add",
      "after": "a0fa13b8bf5478c2",
      "date": 1674903750904
    },
    {
      "type": "edit",
      "id": "6d3e2d102be2d701",
      "item": {
        "type": "pagefold",
        "id": "6d3e2d102be2d701",
        "text": "~"
      },
      "date": 1674903753206
    },
    {
      "item": {
        "type": "factory",
        "id": "638c3ab6b01dd31a"
      },
      "id": "638c3ab6b01dd31a",
      "type": "add",
      "after": "6d3e2d102be2d701",
      "date": 1674903756237
    },
    {
      "type": "edit",
      "id": "638c3ab6b01dd31a",
      "item": {
        "type": "paragraph",
        "id": "638c3ab6b01dd31a",
        "text": "How to create a semantic space in which documents can be compared by their cosine similarity?"
      },
      "date": 1674903758740
    },
    {
      "item": {
        "type": "factory",
        "id": "515faa61835f124a"
      },
      "id": "515faa61835f124a",
      "type": "add",
      "after": "638c3ab6b01dd31a",
      "date": 1674903768544
    },
    {
      "type": "edit",
      "id": "515faa61835f124a",
      "item": {
        "type": "paragraph",
        "id": "515faa61835f124a",
        "text": "chat.openai: To create a semantic space in which documents can be compared by their cosine similarity using Latent Semantic Analysis (LSA), the following steps are typically followed:\n\nPreprocessing: Clean and preprocess the text data by removing stopwords, stemming, and lemmatization.\nTerm-document matrix: Create a term-document matrix, where rows represent unique words and columns represent each document. Each cell in the matrix contains the number of occurrences of a given word in a given document.\nSingular value decomposition (SVD): Apply SVD to the term-document matrix, which reduces the number of rows while preserving the similarity structure among columns. The SVD decomposition results in three matrices: U, S, and V.\nDimensionality reduction: Select a number of dimensions (k) to reduce the number of rows in the matrix U. This creates a reduced dimensional space, also called a semantic space, in which the documents are represented.\nCosine similarity: Compare the similarity between any two documents in the semantic space by calculating the cosine similarity between their respective vectors in the reduced dimensional space. The cosine similarity ranges from -1 to 1, with values close to 1 indicating high similarity and values close to -1 indicating high dissimilarity.\nIt's worth noting that LSA can be considered as an unsupervised technique, as it doesn't require labeled data. LSA aims to extract latent topics in the data, it is also known as Latent Semantic Indexing (LSI) as well."
      },
      "date": 1674903774970
    },
    {
      "type": "edit",
      "id": "515faa61835f124a",
      "item": {
        "type": "paragraph",
        "id": "515faa61835f124a",
        "text": "chat.openai: To create a semantic space in which documents can be compared by their cosine similarity using Latent Semantic Analysis (LSA), the following steps are typically followed:\n\n"
      },
      "date": 1674903785935
    },
    {
      "type": "add",
      "id": "da21bad0622fc87f",
      "item": {
        "type": "paragraph",
        "id": "da21bad0622fc87f",
        "text": "Preprocessing: Clean and preprocess the text data by removing stopwords, stemming, and lemmatization.\n"
      },
      "after": "515faa61835f124a",
      "date": 1674903787517
    },
    {
      "type": "add",
      "id": "e04a83eccaaa8d0d",
      "item": {
        "type": "paragraph",
        "id": "e04a83eccaaa8d0d",
        "text": "Term-document matrix: Create a term-document matrix, where rows represent unique words and columns represent each document. Each cell in the matrix contains the number of occurrences of a given word in a given document.\n"
      },
      "after": "da21bad0622fc87f",
      "date": 1674903789872
    },
    {
      "type": "add",
      "id": "0421d873efa2497e",
      "item": {
        "type": "paragraph",
        "id": "0421d873efa2497e",
        "text": "Singular value decomposition (SVD): Apply SVD to the term-document matrix, which reduces the number of rows while preserving the similarity structure among columns. The SVD decomposition results in three matrices: U, S, and V.\n"
      },
      "after": "e04a83eccaaa8d0d",
      "date": 1674903792617
    },
    {
      "type": "add",
      "id": "934f17649d93853f",
      "item": {
        "type": "paragraph",
        "id": "934f17649d93853f",
        "text": "Dimensionality reduction: Select a number of dimensions (k) to reduce the number of rows in the matrix U. This creates a reduced dimensional space, also called a semantic space, in which the documents are represented.\n"
      },
      "after": "0421d873efa2497e",
      "date": 1674903795836
    },
    {
      "type": "add",
      "id": "5a1a2e6fe0234001",
      "item": {
        "type": "paragraph",
        "id": "5a1a2e6fe0234001",
        "text": "Cosine similarity: Compare the similarity between any two documents in the semantic space by calculating the cosine similarity between their respective vectors in the reduced dimensional space. The cosine similarity ranges from -1 to 1, with values close to 1 indicating high similarity and values close to -1 indicating high dissimilarity.\nIt's worth noting that LSA can be considered as an unsupervised technique, as it doesn't require labeled data. LSA aims to extract latent topics in the data, it is also known as Latent Semantic Indexing (LSI) as well."
      },
      "after": "934f17649d93853f",
      "date": 1674903798783
    },
    {
      "type": "edit",
      "id": "5a1a2e6fe0234001",
      "item": {
        "type": "paragraph",
        "id": "5a1a2e6fe0234001",
        "text": "Cosine similarity: Compare the similarity between any two documents in the semantic space by calculating the cosine similarity between their respective vectors in the reduced dimensional space. The cosine similarity ranges from -1 to 1, with values close to 1 indicating high similarity and values close to -1 indicating high dissimilarity.\n"
      },
      "date": 1674903806351
    },
    {
      "type": "add",
      "id": "c0e8ad3e602d54ef",
      "item": {
        "type": "paragraph",
        "id": "c0e8ad3e602d54ef",
        "text": "It's worth noting that LSA can be considered as an unsupervised technique, as it doesn't require labeled data. LSA aims to extract latent topics in the data, it is also known as Latent Semantic Indexing (LSI) as well."
      },
      "after": "5a1a2e6fe0234001",
      "date": 1674903806864
    },
    {
      "type": "edit",
      "id": "a0fa13b8bf5478c2",
      "item": {
        "type": "paragraph",
        "id": "a0fa13b8bf5478c2",
        "text": "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by cosine similarity between any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.[1] "
      },
      "date": 1674903822241
    },
    {
      "type": "edit",
      "id": "a0fa13b8bf5478c2",
      "item": {
        "type": "paragraph",
        "id": "a0fa13b8bf5478c2",
        "text": "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by cosine similarity between any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents. [https://en.wikipedia.org/wiki/Latent_semantic_analysis wikipedia] "
      },
      "date": 1674903833099
    },
    {
      "type": "edit",
      "id": "a0fa13b8bf5478c2",
      "item": {
        "type": "paragraph",
        "id": "a0fa13b8bf5478c2",
        "text": "is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by cosine similarity between any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents. [https://en.wikipedia.org/wiki/Latent_semantic_analysis wikipedia] "
      },
      "date": 1674903842616
    },
    {
      "type": "edit",
      "id": "638c3ab6b01dd31a",
      "item": {
        "type": "paragraph",
        "id": "638c3ab6b01dd31a",
        "text": "How to [[Create a Semantic Space]] in which documents can be compared by their cosine similarity?"
      },
      "date": 1674903933884
    },
    {
      "type": "edit",
      "id": "c0e8ad3e602d54ef",
      "item": {
        "type": "paragraph",
        "id": "c0e8ad3e602d54ef",
        "text": "It's worth noting that LSA can be considered as an unsupervised technique, as it doesn't require labeled data. LSA aims to extract latent topics in the data, it is also known as [[Latent Semantic Indexing]] (LSI) as well."
      },
      "date": 1674904135932
    }
  ]
}