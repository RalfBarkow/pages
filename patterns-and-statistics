{
  "title": "Patterns and Statistics",
  "story": [
    {
      "type": "paragraph",
      "id": "bda1b11203954c7c",
      "text": "How does this \"sampling from the learned distribution of tokens\" work?"
    },
    {
      "type": "paragraph",
      "id": "6e05b8c3456d41f7",
      "text": "chat.openai: The process of sampling from the learned distribution of tokens in a [[Large Language Model]] (LLM) works by using a technique called \"[[Random Sampling]]\". The LLM is trained on a large corpus of text, which allows it to learn the patterns and structure of language. Once it is trained, it can generate new text by using the learned distribution of tokens.\n\nDuring the sampling process, the model selects a token (such as a word, character, or punctuation mark) from the learned distribution based on its probability of occurrence. The selected token is then appended to the input text and used as input for the next sampling step. The process is repeated until the desired length of generated text is reached.\n\nThis process allows the LLM to generate text that is coherent and seems meaningful, as it is based on the patterns and structure of language it learned during the training process. However, it is important to note that the generated text may not always make sense or be semantically correct, as the LLM is based on patterns and statistics rather than understanding the meaning of the text."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Patterns and Statistics",
        "story": []
      },
      "date": 1674921282903
    },
    {
      "id": "bda1b11203954c7c",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "bda1b11203954c7c",
        "text": "How does this \"sampling from the learned distribution of tokens\" work?"
      },
      "date": 1674921289593
    },
    {
      "id": "6e05b8c3456d41f7",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "6e05b8c3456d41f7",
        "text": "chat.openai: The process of sampling from the learned distribution of tokens in a large language model (LLM) works by using a technique called \"Random Sampling\". The LLM is trained on a large corpus of text, which allows it to learn the patterns and structure of language. Once it is trained, it can generate new text by using the learned distribution of tokens.\n\nDuring the sampling process, the model selects a token (such as a word, character, or punctuation mark) from the learned distribution based on its probability of occurrence. The selected token is then appended to the input text and used as input for the next sampling step. The process is repeated until the desired length of generated text is reached.\n\nThis process allows the LLM to generate text that is coherent and seems meaningful, as it is based on the patterns and structure of language it learned during the training process. However, it is important to note that the generated text may not always make sense or be semantically correct, as the LLM is based on patterns and statistics rather than understanding the meaning of the text."
      },
      "after": "bda1b11203954c7c",
      "date": 1674921312222
    },
    {
      "type": "edit",
      "id": "6e05b8c3456d41f7",
      "item": {
        "type": "paragraph",
        "id": "6e05b8c3456d41f7",
        "text": "chat.openai: The process of sampling from the learned distribution of tokens in a [[large language model]] (LLM) works by using a technique called \"Random Sampling\". The LLM is trained on a large corpus of text, which allows it to learn the patterns and structure of language. Once it is trained, it can generate new text by using the learned distribution of tokens.\n\nDuring the sampling process, the model selects a token (such as a word, character, or punctuation mark) from the learned distribution based on its probability of occurrence. The selected token is then appended to the input text and used as input for the next sampling step. The process is repeated until the desired length of generated text is reached.\n\nThis process allows the LLM to generate text that is coherent and seems meaningful, as it is based on the patterns and structure of language it learned during the training process. However, it is important to note that the generated text may not always make sense or be semantically correct, as the LLM is based on patterns and statistics rather than understanding the meaning of the text."
      },
      "date": 1674921331710
    },
    {
      "type": "edit",
      "id": "6e05b8c3456d41f7",
      "item": {
        "type": "paragraph",
        "id": "6e05b8c3456d41f7",
        "text": "chat.openai: The process of sampling from the learned distribution of tokens in a [[Large Language Model]] (LLM) works by using a technique called \"Random Sampling\". The LLM is trained on a large corpus of text, which allows it to learn the patterns and structure of language. Once it is trained, it can generate new text by using the learned distribution of tokens.\n\nDuring the sampling process, the model selects a token (such as a word, character, or punctuation mark) from the learned distribution based on its probability of occurrence. The selected token is then appended to the input text and used as input for the next sampling step. The process is repeated until the desired length of generated text is reached.\n\nThis process allows the LLM to generate text that is coherent and seems meaningful, as it is based on the patterns and structure of language it learned during the training process. However, it is important to note that the generated text may not always make sense or be semantically correct, as the LLM is based on patterns and statistics rather than understanding the meaning of the text."
      },
      "date": 1674921350715
    },
    {
      "type": "edit",
      "id": "6e05b8c3456d41f7",
      "item": {
        "type": "paragraph",
        "id": "6e05b8c3456d41f7",
        "text": "chat.openai: The process of sampling from the learned distribution of tokens in a [[Large Language Model]] (LLM) works by using a technique called \"[[Random Sampling]]\". The LLM is trained on a large corpus of text, which allows it to learn the patterns and structure of language. Once it is trained, it can generate new text by using the learned distribution of tokens.\n\nDuring the sampling process, the model selects a token (such as a word, character, or punctuation mark) from the learned distribution based on its probability of occurrence. The selected token is then appended to the input text and used as input for the next sampling step. The process is repeated until the desired length of generated text is reached.\n\nThis process allows the LLM to generate text that is coherent and seems meaningful, as it is based on the patterns and structure of language it learned during the training process. However, it is important to note that the generated text may not always make sense or be semantically correct, as the LLM is based on patterns and statistics rather than understanding the meaning of the text."
      },
      "date": 1674921363891
    }
  ]
}