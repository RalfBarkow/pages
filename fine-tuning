{
  "title": "Fine-Tuning",
  "story": [
    {
      "type": "paragraph",
      "id": "0131dc40c0114130",
      "text": "“You can take the model from the pretraining stage and kind of adapt it for whatever actual task you care about,” Ott explained. “And when you do that, you get much better results than if you had just started with your end task in the first place.”\n\nIndeed, in June of 2018, when OpenAI unveiled a neural network called [[GPT]], which included a language model pretrained on nearly a billion words (sourced from 11,038 digital books) for an entire month, its GLUE score of 72.8 immediately took the top spot on the leaderboard. Still, Sam Bowman assumed that the field had a long way to go before any system could even begin to approach human-level performance.\n\nThen [[BERT]] appeared."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Fine-Tuning",
        "story": []
      },
      "date": 1650972779869
    },
    {
      "item": {
        "type": "factory",
        "id": "0131dc40c0114130"
      },
      "id": "0131dc40c0114130",
      "type": "add",
      "date": 1650972785060
    },
    {
      "type": "edit",
      "id": "0131dc40c0114130",
      "item": {
        "type": "paragraph",
        "id": "0131dc40c0114130",
        "text": "“You can take the model from the pretraining stage and kind of adapt it for whatever actual task you care about,” Ott explained. “And when you do that, you get much better results than if you had just started with your end task in the first place.”\n\nIndeed, in June of 2018, when OpenAI unveiled a neural network called GPT, which included a language model pretrained on nearly a billion words (sourced from 11,038 digital books) for an entire month, its GLUE score of 72.8 immediately took the top spot on the leaderboard. Still, Sam Bowman assumed that the field had a long way to go before any system could even begin to approach human-level performance.\n\nThen BERT appeared."
      },
      "date": 1650972787760
    },
    {
      "type": "edit",
      "id": "0131dc40c0114130",
      "item": {
        "type": "paragraph",
        "id": "0131dc40c0114130",
        "text": "“You can take the model from the pretraining stage and kind of adapt it for whatever actual task you care about,” Ott explained. “And when you do that, you get much better results than if you had just started with your end task in the first place.”\n\nIndeed, in June of 2018, when OpenAI unveiled a neural network called [[GPT]], which included a language model pretrained on nearly a billion words (sourced from 11,038 digital books) for an entire month, its GLUE score of 72.8 immediately took the top spot on the leaderboard. Still, Sam Bowman assumed that the field had a long way to go before any system could even begin to approach human-level performance.\n\nThen [[BERT]] appeared."
      },
      "date": 1650972814328
    }
  ]
}