{
  "title": "Transformer-XL",
  "story": [
    {
      "type": "markdown",
      "id": "a2329633c4259815",
      "text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Transformer-XL",
        "story": []
      },
      "date": 1649803021389
    },
    {
      "item": {
        "type": "factory",
        "id": "a2329633c4259815"
      },
      "id": "a2329633c4259815",
      "type": "add",
      "date": 1649803048910
    },
    {
      "type": "edit",
      "id": "a2329633c4259815",
      "item": {
        "type": "paragraph",
        "id": "a2329633c4259815",
        "text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860."
      },
      "date": 1649803050939
    },
    {
      "type": "edit",
      "id": "a2329633c4259815",
      "item": {
        "type": "markdown",
        "id": "a2329633c4259815",
        "text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860."
      },
      "date": 1649803052213
    }
  ]
}