{
  "title": "Data Handling",
  "story": [
    {
      "type": "paragraph",
      "id": "2c3924fd11ce29f5",
      "text": "hnbeck â€” [https://discord.com/channels/694586717247635488/966071402526543924/982710348907941960 discord] [[Croquet Developers]] #questions"
    },
    {
      "type": "paragraph",
      "id": "189000695bba2d18",
      "text": "Ok. Honestly, the data handling is something which I've don't get in detail. Here some additional thought on this: "
    },
    {
      "type": "markdown",
      "id": "729ce7dd0bed9807",
      "text": "**Assumption A** is: duality of data and operations, meaning any data image and a given time can be described by a sequence of transformations. This would require that there is an initial data image, or the first transformation has to carry initial data.  From this, it is required that transformations can carry data \"particles\", for example parameters. Here would be the next question, is there a theoretic limit of amount of data per transformation such that this duality does not degenerate or become trivial? "
    },
    {
      "type": "markdown",
      "id": "efd6c6899f2db4e0",
      "text": "**Assumption B**: data in the Croquet network (Mikroverse) should be \"holographic\" in the following sense:  holographic properties allows to describe the content of a space with data on dimension lower. A wave front pattern recorded is 2D, the Holographic space is 3D. Therefore, some kind of rules have to be there describing how the n+1 space is filled be data from n space. Also, a subpart of the n space should be able to fill a part of the n+1 space.  For the Mikroverse this would mean the drop of some peers would keep the Mikroverse alive, maybe with some small details lost. For the Croquet, the replication of these rules would be not the problem, but how to distribute the data of the n space, and what would the n space be?  If n=1 it may one peer, the network maybe then 2D. Just thoughts, as I said. This could be developed further, that every peer got some kind of \"DNA\" or basic code as part of the snapshot, the rules (transitions) then maybe need no parameters, just activation by some event. BTW, distribution of fact data bases for example in PROLOG is an interesting problem.\n"
    },
    {
      "type": "paragraph",
      "id": "4140fe1d742e342d",
      "text": "yoshiki â€” 04.06.2022\nIt is a bit too analogy rich, but let me answer as best as I can, in an order. Incidentally, I've used tau-prolog for a project and also I've learned about Datalog, so I assume that that n -> n + 1 is something like existentional vs. intensional facts. If so, indeed there is not a single solution that works for all cases, based on the practicality of data size and computation cost and message sizes, as you alluded. If intensional facts can be generated from a smaller set of rules in not much computation (or generation of them take time as input to generate the next set of facts. either you store only the base level facts in the model and make the generated facts transient. If you do have to keep them, store them in the model as a persistent part of the model. If it is too big, yes, you might have to do inferences on one computer and send as small diffs as possible to update peers. As long as the data is serializable, there is no \"theoretical\" problem, but only practical ones. (Note, BTW, that Microverse is this particular 3D collaborative application, and in this discussion, it probably is good to separate base Croquet and Microverse.) As for distributed assertion database, the way Croquet treats participants is to consider them all bit-identical replicas. There won't be \"some data\" lost even when some participants drop out, in general. So let us know a bit more about what are the pragmatic size of things you want to deal with, and we can perhaps come up with an actionable suggestion.\n"
    },
    {
      "type": "paragraph",
      "id": "f25b937813440b64",
      "text": "hnbeck â€” 05.06.2022\nThank you very much for your answer! You pointed me to some errors in my thinking, this is great. In fact, I used \"Mikroverse\" in my posts above as \"sumerised effect of all nodes\", when effect is that of cause-effect. Now I've got that \"Mikroverse\" is a term of application or using. And yes, I was confused with \"distributed\" vs \"replicated\".  As you said, nodes have to be bit identical per definition, so what could be the same only is what I called \"DNA\", like in biological cells of a body. But there is now place where the effect on the n+1 space out of this \"DNA\" could be observable. Yes, true.  That takes me back how I could deal with the OpenDrive file problem (or USD as another relevant file format). You pointed out some possibilities, I will think about it.  THank you very much for patient discussion, sometime my phantasy is like flying horses ðŸ™‚\n"
    },
    {
      "type": "paragraph",
      "id": "50dc0aadbdb61eef",
      "text": "yoshiki â€” 05.06.2022\nGreat! If you haven't come across, this paper \"Dedalus: Datalog in Time and Space\" [https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-173.pdf pdf] may have great relevance. If n -> n + 1 thing is like generating Datalog facts, Dedalus provides a bit of special interpretation for the variable that represents the logical time so that the resulting programming model can be used for distributed computing. \n"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Data Handling",
        "story": []
      },
      "date": 1654667076745
    },
    {
      "item": {
        "type": "paragraph",
        "id": "2c3924fd11ce29f5",
        "text": "hnbeck â€” 04.06.2022\nOk. Honestly, the data handling is something which I've don't get in detail. Here some additional thought on this: Assumption A is: duality of data and operations, meaning any data image and a given time can be described by a sequence of transformations. This would require that there is an initial data image, or the first transformation has to carry initial data.  From this, it is required that transformations can carry data \"particles\", for example parameters. Here would be the next question, is there a theoretic limit of amount of data per transformation such that this duality does not degenerate or become trivial? Assumption B: data in the Croquet network (Mikroverse) should be \"holographic\" in the following sense:  holographic properties allows to describe the content of a space with data on dimension lower. A wave front pattern recorded is 2D, the Holographic space is 3D. Therefore, some kind of rules have to be there describing how the n+1 space is filled be data from n space. Also, a subpart of the n space should be able to fill a part of the n+1 space.  For the Mikroverse this would mean the drop of some peers would keep the Mikroverse alive, maybe with some small details lost. For the Croquet, the replication of these rules would be not the problem, but how to distribute the data of the n space, and what would the n space be?  If n=1 it may one peer, the network maybe then 2D. Just thoughts, as I said. This could be developed further, that every peer got some kind of \"DNA\" or basic code as part of the snapshot, the rules (transitions) then maybe need no parameters, just activation by some event. BTW, distribution of fact data bases for example in PROLOG is an interesting problem.\n"
      },
      "id": "2c3924fd11ce29f5",
      "type": "add",
      "date": 1654667091015
    },
    {
      "type": "edit",
      "id": "2c3924fd11ce29f5",
      "item": {
        "type": "paragraph",
        "id": "2c3924fd11ce29f5",
        "text": "hnbeck â€” 04.06.2022\nOk. Honestly, the data handling is something which I've don't get in detail. Here some additional thought on this: Assumption A is: duality of data and operations, meaning any data image and a given time can be described by a sequence of transformations. This would require that there is an initial data image, or the first transformation has to carry initial data.  From this, it is required that transformations can carry data \"particles\", for example parameters. Here would be the next question, is there a theoretic limit of amount of data per transformation such that this duality does not degenerate or become trivial? Assumption B: data in the Croquet network (Mikroverse) should be \"holographic\" in the following sense:  holographic properties allows to describe the content of a space with data on dimension lower. A wave front pattern recorded is 2D, the Holographic space is 3D. Therefore, some kind of rules have to be there describing how the n+1 space is filled be data from n space. Also, a subpart of the n space should be able to fill a part of the n+1 space.  For the Mikroverse this would mean the drop of some peers would keep the Mikroverse alive, maybe with some small details lost. For the Croquet, the replication of these rules would be not the problem, but how to distribute the data of the n space, and what would the n space be?  If n=1 it may one peer, the network maybe then 2D. Just thoughts, as I said. This could be developed further, that every peer got some kind of \"DNA\" or basic code as part of the snapshot, the rules (transitions) then maybe need no parameters, just activation by some event. BTW, distribution of fact data bases for example in PROLOG is an interesting problem.\n"
      },
      "date": 1654667102565
    },
    {
      "type": "add",
      "id": "4140fe1d742e342d",
      "item": {
        "type": "paragraph",
        "id": "4140fe1d742e342d",
        "text": "yoshiki â€” 04.06.2022\nIt is a bit too analogy rich, but let me answer as best as I can, in an order. Incidentally, I've used tau-prolog for a project and also I've learned about Datalog, so I assume that that n -> n + 1 is something like existentional vs. intensional facts. If so, indeed there is not a single solution that works for all cases, based on the practicality of data size and computation cost and message sizes, as you alluded. If intensional facts can be generated from a smaller set of rules in not much computation (or generation of them take time as input to generate the next set of facts. either you store only the base level facts in the model and make the generated facts transient. If you do have to keep them, store them in the model as a persistent part of the model. If it is too big, yes, you might have to do inferences on one computer and send as small diffs as possible to update peers. As long as the data is serializable, there is no \"theoretical\" problem, but only practical ones. (Note, BTW, that Microverse is this particular 3D collaborative application, and in this discussion, it probably is good to separate base Croquet and Microverse.) As for distributed assertion database, the way Croquet treats participants is to consider them all bit-identical replicas. There won't be \"some data\" lost even when some participants drop out, in general. So let us know a bit more about what are the pragmatic size of things you want to deal with, and we can perhaps come up with an actionable suggestion.\n"
      },
      "after": "2c3924fd11ce29f5",
      "date": 1654667104465,
      "error": {
        "type": "error",
        "msg": ""
      }
    },
    {
      "type": "fork",
      "date": 1654667115527
    },
    {
      "type": "edit",
      "id": "2c3924fd11ce29f5",
      "item": {
        "type": "paragraph",
        "id": "2c3924fd11ce29f5",
        "text": "hnbeck â€” [https://discord.com/channels/694586717247635488/966071402526543924/982710348907941960 discord] [[Croquet Developers]] #questions"
      },
      "date": 1654667232938
    },
    {
      "type": "add",
      "id": "189000695bba2d18",
      "item": {
        "type": "paragraph",
        "id": "189000695bba2d18",
        "text": "Ok. Honestly, the data handling is something which I've don't get in detail. Here some additional thought on this: Assumption A is: duality of data and operations, meaning any data image and a given time can be described by a sequence of transformations. This would require that there is an initial data image, or the first transformation has to carry initial data.  From this, it is required that transformations can carry data \"particles\", for example parameters. Here would be the next question, is there a theoretic limit of amount of data per transformation such that this duality does not degenerate or become trivial? Assumption B: data in the Croquet network (Mikroverse) should be \"holographic\" in the following sense:  holographic properties allows to describe the content of a space with data on dimension lower. A wave front pattern recorded is 2D, the Holographic space is 3D. Therefore, some kind of rules have to be there describing how the n+1 space is filled be data from n space. Also, a subpart of the n space should be able to fill a part of the n+1 space.  For the Mikroverse this would mean the drop of some peers would keep the Mikroverse alive, maybe with some small details lost. For the Croquet, the replication of these rules would be not the problem, but how to distribute the data of the n space, and what would the n space be?  If n=1 it may one peer, the network maybe then 2D. Just thoughts, as I said. This could be developed further, that every peer got some kind of \"DNA\" or basic code as part of the snapshot, the rules (transitions) then maybe need no parameters, just activation by some event. BTW, distribution of fact data bases for example in PROLOG is an interesting problem.\n"
      },
      "after": "2c3924fd11ce29f5",
      "date": 1654667233675
    },
    {
      "type": "edit",
      "id": "189000695bba2d18",
      "item": {
        "type": "paragraph",
        "id": "189000695bba2d18",
        "text": "Ok. Honestly, the data handling is something which I've don't get in detail. Here some additional thought on this: "
      },
      "date": 1654667273425
    },
    {
      "type": "add",
      "id": "729ce7dd0bed9807",
      "item": {
        "type": "paragraph",
        "id": "729ce7dd0bed9807",
        "text": "**Assumption A is**: duality of data and operations, meaning any data image and a given time can be described by a sequence of transformations. This would require that there is an initial data image, or the first transformation has to carry initial data.  From this, it is required that transformations can carry data \"particles\", for example parameters. Here would be the next question, is there a theoretic limit of amount of data per transformation such that this duality does not degenerate or become trivial? Assumption B: data in the Croquet network (Mikroverse) should be \"holographic\" in the following sense:  holographic properties allows to describe the content of a space with data on dimension lower. A wave front pattern recorded is 2D, the Holographic space is 3D. Therefore, some kind of rules have to be there describing how the n+1 space is filled be data from n space. Also, a subpart of the n space should be able to fill a part of the n+1 space.  For the Mikroverse this would mean the drop of some peers would keep the Mikroverse alive, maybe with some small details lost. For the Croquet, the replication of these rules would be not the problem, but how to distribute the data of the n space, and what would the n space be?  If n=1 it may one peer, the network maybe then 2D. Just thoughts, as I said. This could be developed further, that every peer got some kind of \"DNA\" or basic code as part of the snapshot, the rules (transitions) then maybe need no parameters, just activation by some event. BTW, distribution of fact data bases for example in PROLOG is an interesting problem.\n"
      },
      "after": "189000695bba2d18",
      "date": 1654667282519
    },
    {
      "type": "edit",
      "id": "729ce7dd0bed9807",
      "item": {
        "type": "paragraph",
        "id": "729ce7dd0bed9807",
        "text": "**Assumption A is**: duality of data and operations, meaning any data image and a given time can be described by a sequence of transformations. This would require that there is an initial data image, or the first transformation has to carry initial data.  From this, it is required that transformations can carry data \"particles\", for example parameters. Here would be the next question, is there a theoretic limit of amount of data per transformation such that this duality does not degenerate or become trivial? "
      },
      "date": 1654667297150
    },
    {
      "type": "add",
      "id": "efd6c6899f2db4e0",
      "item": {
        "type": "paragraph",
        "id": "efd6c6899f2db4e0",
        "text": "Assumption B: data in the Croquet network (Mikroverse) should be \"holographic\" in the following sense:  holographic properties allows to describe the content of a space with data on dimension lower. A wave front pattern recorded is 2D, the Holographic space is 3D. Therefore, some kind of rules have to be there describing how the n+1 space is filled be data from n space. Also, a subpart of the n space should be able to fill a part of the n+1 space.  For the Mikroverse this would mean the drop of some peers would keep the Mikroverse alive, maybe with some small details lost. For the Croquet, the replication of these rules would be not the problem, but how to distribute the data of the n space, and what would the n space be?  If n=1 it may one peer, the network maybe then 2D. Just thoughts, as I said. This could be developed further, that every peer got some kind of \"DNA\" or basic code as part of the snapshot, the rules (transitions) then maybe need no parameters, just activation by some event. BTW, distribution of fact data bases for example in PROLOG is an interesting problem.\n"
      },
      "after": "729ce7dd0bed9807",
      "date": 1654667299719
    },
    {
      "type": "edit",
      "id": "729ce7dd0bed9807",
      "item": {
        "type": "paragraph",
        "id": "729ce7dd0bed9807",
        "text": "**Assumption A** is: duality of data and operations, meaning any data image and a given time can be described by a sequence of transformations. This would require that there is an initial data image, or the first transformation has to carry initial data.  From this, it is required that transformations can carry data \"particles\", for example parameters. Here would be the next question, is there a theoretic limit of amount of data per transformation such that this duality does not degenerate or become trivial? "
      },
      "date": 1654667307853
    },
    {
      "type": "edit",
      "id": "729ce7dd0bed9807",
      "item": {
        "type": "markdown",
        "id": "729ce7dd0bed9807",
        "text": "**Assumption A** is: duality of data and operations, meaning any data image and a given time can be described by a sequence of transformations. This would require that there is an initial data image, or the first transformation has to carry initial data.  From this, it is required that transformations can carry data \"particles\", for example parameters. Here would be the next question, is there a theoretic limit of amount of data per transformation such that this duality does not degenerate or become trivial? "
      },
      "date": 1654667308860
    },
    {
      "type": "edit",
      "id": "efd6c6899f2db4e0",
      "item": {
        "type": "paragraph",
        "id": "efd6c6899f2db4e0",
        "text": "**Assumption B**: data in the Croquet network (Mikroverse) should be \"holographic\" in the following sense:  holographic properties allows to describe the content of a space with data on dimension lower. A wave front pattern recorded is 2D, the Holographic space is 3D. Therefore, some kind of rules have to be there describing how the n+1 space is filled be data from n space. Also, a subpart of the n space should be able to fill a part of the n+1 space.  For the Mikroverse this would mean the drop of some peers would keep the Mikroverse alive, maybe with some small details lost. For the Croquet, the replication of these rules would be not the problem, but how to distribute the data of the n space, and what would the n space be?  If n=1 it may one peer, the network maybe then 2D. Just thoughts, as I said. This could be developed further, that every peer got some kind of \"DNA\" or basic code as part of the snapshot, the rules (transitions) then maybe need no parameters, just activation by some event. BTW, distribution of fact data bases for example in PROLOG is an interesting problem.\n"
      },
      "date": 1654667369234
    },
    {
      "type": "edit",
      "id": "efd6c6899f2db4e0",
      "item": {
        "type": "markdown",
        "id": "efd6c6899f2db4e0",
        "text": "**Assumption B**: data in the Croquet network (Mikroverse) should be \"holographic\" in the following sense:  holographic properties allows to describe the content of a space with data on dimension lower. A wave front pattern recorded is 2D, the Holographic space is 3D. Therefore, some kind of rules have to be there describing how the n+1 space is filled be data from n space. Also, a subpart of the n space should be able to fill a part of the n+1 space.  For the Mikroverse this would mean the drop of some peers would keep the Mikroverse alive, maybe with some small details lost. For the Croquet, the replication of these rules would be not the problem, but how to distribute the data of the n space, and what would the n space be?  If n=1 it may one peer, the network maybe then 2D. Just thoughts, as I said. This could be developed further, that every peer got some kind of \"DNA\" or basic code as part of the snapshot, the rules (transitions) then maybe need no parameters, just activation by some event. BTW, distribution of fact data bases for example in PROLOG is an interesting problem.\n"
      },
      "date": 1654667370779
    },
    {
      "item": {
        "type": "factory",
        "id": "f25b937813440b64"
      },
      "id": "f25b937813440b64",
      "type": "add",
      "after": "4140fe1d742e342d",
      "date": 1654667476131
    },
    {
      "type": "edit",
      "id": "f25b937813440b64",
      "item": {
        "type": "paragraph",
        "id": "f25b937813440b64",
        "text": "hnbeck â€” 05.06.2022\nThank you very much for your answer! You pointed me to some errors in my thinking, this is great. In fact, I used \"Mikroverse\" in my posts above as \"sumerised effect of all nodes\", when effect is that of cause-effect. Now I've got that \"Mikroverse\" is a term of application or using. And yes, I was confused with \"distributed\" vs \"replicated\".  As you said, nodes have to be bit identical per definition, so what could be the same only is what I called \"DNA\", like in biological cells of a body. But there is now place where the effect on the n+1 space out of this \"DNA\" could be observable. Yes, true.  That takes me back how I could deal with the OpenDrive file problem (or USD as another relevant file format). You pointed out some possibilities, I will think about it.  THank you very much for patient discussion, sometime my phantasy is like flying horses ðŸ™‚\nyoshiki â€” 05.06.2022\nGreat! If you haven't come across, this paper \"Dedalus: Datalog in Time and Space\" https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-173.pdf may have great relevance. If n -> n + 1 thing is like generating Datalog facts, Dedalus provides a bit of special interpretation for the variable that represents the logical time so that the resulting programming model can be used for distributed computing. \n"
      },
      "date": 1654667478365
    },
    {
      "type": "edit",
      "id": "f25b937813440b64",
      "item": {
        "type": "paragraph",
        "id": "f25b937813440b64",
        "text": "hnbeck â€” 05.06.2022\nThank you very much for your answer! You pointed me to some errors in my thinking, this is great. In fact, I used \"Mikroverse\" in my posts above as \"sumerised effect of all nodes\", when effect is that of cause-effect. Now I've got that \"Mikroverse\" is a term of application or using. And yes, I was confused with \"distributed\" vs \"replicated\".  As you said, nodes have to be bit identical per definition, so what could be the same only is what I called \"DNA\", like in biological cells of a body. But there is now place where the effect on the n+1 space out of this \"DNA\" could be observable. Yes, true.  That takes me back how I could deal with the OpenDrive file problem (or USD as another relevant file format). You pointed out some possibilities, I will think about it.  THank you very much for patient discussion, sometime my phantasy is like flying horses ðŸ™‚\n"
      },
      "date": 1654667487347
    },
    {
      "type": "add",
      "id": "50dc0aadbdb61eef",
      "item": {
        "type": "paragraph",
        "id": "50dc0aadbdb61eef",
        "text": "yoshiki â€” 05.06.2022\nGreat! If you haven't come across, this paper \"Dedalus: Datalog in Time and Space\" [https://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-173.pdf pdf] may have great relevance. If n -> n + 1 thing is like generating Datalog facts, Dedalus provides a bit of special interpretation for the variable that represents the logical time so that the resulting programming model can be used for distributed computing. \n"
      },
      "after": "f25b937813440b64",
      "date": 1654667498631
    },
    {
      "type": "fork",
      "date": 1654681717509
    }
  ]
}