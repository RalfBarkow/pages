{
  "title": "Generative Modeling",
  "story": [
    {
      "type": "paragraph",
      "id": "002c28cd1023bfdb",
      "text": "“Generative modeling” is a broad area of machine learning which deals with models of distributions P(X), defined over datapoints X in some potentially high-dimensional space X. "
    },
    {
      "type": "paragraph",
      "id": "cd7f2c9a0e3cfd94",
      "text": "For instance, images are a popular kind of data for which we might create generative models. Each “datapoint” (image) has thousands or millions of dimensions (pixels), and the generative model’s job is to somehow capture the dependencies between pixels, e.g., that nearby pixels have similar color, and are organized into objects. Exactly what it means to “capture” these dependencies depends on exactly what we want to do with the model. One straightforward kind of generative model simply allows us to compute P(X) numerically."
    },
    {
      "type": "paragraph",
      "id": "7e8ebac27dfed2d1",
      "text": "In the case of images, X values which look like real images should get high probability, whereas images that look like random noise should get low probability. However, models like this are not necessarily useful: knowing that one image is unlikely does not help us synthesize one that is likely."
    },
    {
      "type": "paragraph",
      "id": "931440f05777392d",
      "text": "Instead, one often cares about producing more examples that are like those already in a database, but not exactly the same. We could start with a database of raw images and synthesize new, unseen images. We might take in a database of 3D models of something like plants and produce more of them to fill a forest in a video game. We could take handwritten text and try to produce more handwritten text. Tools like this might actually be useful for graphic designers. We can formalize this setup by saying that we get examples X distributed according to some unknown distribution Pgt(X), and our goal is to learn a model P which we can sample from, such that P is as similar as possible to Pgt."
    },
    {
      "type": "paragraph",
      "id": "fd52e6c11792c7f2",
      "text": "Training this type of model has been a long-standing problem in the machine learning community, and classically, most approaches have had one of three serious drawbacks. First, they might require strong assumptions about the structure in the data. Second, they might make severe approximations, leading to sub-optimal models. Or third, they might rely on computationally expensive inference procedures like Markov Chain Monte Carlo. "
    },
    {
      "type": "paragraph",
      "id": "f993abde16e85c63",
      "text": "More recently, some works have made tremendous progress in training neural networks as powerful function approximators through backpropagation [9]. These advances have given rise to promising frameworks which can use backpropagation-based function approximators to build generative models."
    },
    {
      "type": "paragraph",
      "id": "33b46e33c991f53e",
      "text": "One of the most popular such frameworks is the [[Variational Autoencoder]] [1, 3], the subject of this tutorial. The assumptions of this model are weak, and training is fast via backpropagation. VAEs do make an approximation, but the error introduced by this approximation is arguably small given high-capacity models. These characteristics have contributed to a quick rise in their popularity."
    },
    {
      "type": "paragraph",
      "id": "12494e7096d0fca9",
      "text": "This tutorial is intended to be an informal introduction to VAEs, and not a formal scientific paper about them. It is aimed at people who might have uses for generative models, but might not have a strong background in the variatonal Bayesian methods and “minimum description length” coding models on which VAEs are based. This tutorial began its life as a presentation for computer vision reading groups at UC Berkeley and Carnegie Mellon, and hence has a bias toward a vision audience. Suggestions for improvement are appreciated. [https://github.com/cdoersch/vae_tutorial github]"
    },
    {
      "type": "pagefold",
      "id": "ffc886e3ad77a94a",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "13131692f28ff748",
      "text": "DOERSCH, Carl, 2021. Tutorial on Variational Autoencoders. Online. 3 January 2021. arXiv. arXiv:1606.05908. [Accessed 22 March 2023]. Available from: http://arxiv.org/abs/1606.05908 [https://arxiv.org/pdf/1606.05908.pdf pdf]"
    },
    {
      "type": "paragraph",
      "id": "ac526278a0980339",
      "text": "In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], CIFAR images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed. arXiv:1606.05908 [cs, stat]\n"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Generative Modeling",
        "story": []
      },
      "date": 1679485302658
    },
    {
      "item": {
        "type": "factory",
        "id": "002c28cd1023bfdb"
      },
      "id": "002c28cd1023bfdb",
      "type": "add",
      "date": 1679485319825
    },
    {
      "type": "edit",
      "id": "002c28cd1023bfdb",
      "item": {
        "type": "paragraph",
        "id": "002c28cd1023bfdb",
        "text": "“Generative modeling” is a broad area of machine learning which deals with models of distributions P(X), defined over datapoints X in some potentially high-dimensional space X . For instance, images are a popular kind of data for which we might create generative models. Each “datapoint” (image) has thousands or millions of dimensions (pixels), and the generative model’s job is to somehow capture the dependencies between pixels, e.g., that nearby pixels have similar color, and are organized into objects. Exactly what it means to “capture” these dependencies depends on exactly what we want to do with the model. One straightforward kind of generative model simply allows us to compute P(X) numerically."
      },
      "date": 1679485321669
    },
    {
      "item": {
        "type": "factory",
        "id": "ffc886e3ad77a94a"
      },
      "id": "ffc886e3ad77a94a",
      "type": "add",
      "after": "002c28cd1023bfdb",
      "date": 1679485330431
    },
    {
      "type": "edit",
      "id": "ffc886e3ad77a94a",
      "item": {
        "type": "pagefold",
        "id": "ffc886e3ad77a94a",
        "text": "~"
      },
      "date": 1679485332779
    },
    {
      "item": {
        "type": "factory",
        "id": "13131692f28ff748"
      },
      "id": "13131692f28ff748",
      "type": "add",
      "after": "ffc886e3ad77a94a",
      "date": 1679485334506
    },
    {
      "type": "edit",
      "id": "13131692f28ff748",
      "item": {
        "type": "paragraph",
        "id": "13131692f28ff748",
        "text": "\nDOERSCH, Carl, 2021. Tutorial on Variational Autoencoders. Online. 3 January 2021. arXiv. arXiv:1606.05908. [Accessed 22 March 2023]. Available from: http://arxiv.org/abs/1606.05908In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], CIFAR images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.arXiv:1606.05908 [cs, stat]\n"
      },
      "date": 1679485338547
    },
    {
      "type": "edit",
      "id": "13131692f28ff748",
      "item": {
        "type": "paragraph",
        "id": "13131692f28ff748",
        "text": "DOERSCH, Carl, 2021. Tutorial on Variational Autoencoders. Online. 3 January 2021. arXiv. arXiv:1606.05908. [Accessed 22 March 2023]. Available from: http://arxiv.org/abs/1606.05908"
      },
      "date": 1679485353048
    },
    {
      "type": "add",
      "id": "ac526278a0980339",
      "item": {
        "type": "paragraph",
        "id": "ac526278a0980339",
        "text": "In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], CIFAR images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.arXiv:1606.05908 [cs, stat]\n"
      },
      "after": "13131692f28ff748",
      "date": 1679485354232
    },
    {
      "type": "edit",
      "id": "13131692f28ff748",
      "item": {
        "type": "paragraph",
        "id": "13131692f28ff748",
        "text": "DOERSCH, Carl, 2021. Tutorial on Variational Autoencoders. Online. 3 January 2021. arXiv. arXiv:1606.05908. [Accessed 22 March 2023]. Available from: http://arxiv.org/abs/1606.05908 [https://arxiv.org/pdf/1606.05908.pdf pdf]"
      },
      "date": 1679485355945
    },
    {
      "type": "edit",
      "id": "ac526278a0980339",
      "item": {
        "type": "paragraph",
        "id": "ac526278a0980339",
        "text": "In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], CIFAR images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed. arXiv:1606.05908 [cs, stat]\n"
      },
      "date": 1679485394122
    },
    {
      "type": "edit",
      "id": "002c28cd1023bfdb",
      "item": {
        "type": "paragraph",
        "id": "002c28cd1023bfdb",
        "text": "“Generative modeling” is a broad area of machine learning which deals with models of distributions P(X), defined over datapoints X in some potentially high-dimensional space X. "
      },
      "date": 1679485409459
    },
    {
      "type": "add",
      "id": "cd7f2c9a0e3cfd94",
      "item": {
        "type": "paragraph",
        "id": "cd7f2c9a0e3cfd94",
        "text": "For instance, images are a popular kind of data for which we might create generative models. Each “datapoint” (image) has thousands or millions of dimensions (pixels), and the generative model’s job is to somehow capture the dependencies between pixels, e.g., that nearby pixels have similar color, and are organized into objects. Exactly what it means to “capture” these dependencies depends on exactly what we want to do with the model. One straightforward kind of generative model simply allows us to compute P(X) numerically."
      },
      "after": "002c28cd1023bfdb",
      "date": 1679485410722
    },
    {
      "type": "add",
      "id": "7e8ebac27dfed2d1",
      "item": {
        "type": "paragraph",
        "id": "7e8ebac27dfed2d1",
        "text": "In the case of images, X values"
      },
      "after": "cd7f2c9a0e3cfd94",
      "date": 1679485458149
    },
    {
      "type": "edit",
      "id": "7e8ebac27dfed2d1",
      "item": {
        "type": "paragraph",
        "id": "7e8ebac27dfed2d1",
        "text": "In the case of images, X values which look like real images should get high probability, whereas images that look like random noise should get low probability. However, models like this are not necessarily useful: knowing that one image is unlikely does not help us synthesize one that is likely."
      },
      "date": 1679485466147
    },
    {
      "type": "add",
      "id": "931440f05777392d",
      "item": {
        "type": "paragraph",
        "id": "931440f05777392d",
        "text": "Instead, one often cares about producing more examples that are like those already in a database, but not exactly the same. We could start with a database of raw images and synthesize new, unseen images. We might take in a database of 3D models of something like plants and produce more of them to fill a forest in a video game. We could take handwritten text and try to produce more handwritten text. Tools like this might actually be useful for graphic designers. We can formalize this setup by saying that we get examples X distributed according to some unknown distribution Pgt(X), and our goal is to learn a model P which we can sample from, such that P is as similar as possible to Pgt."
      },
      "after": "7e8ebac27dfed2d1",
      "date": 1679485487251
    },
    {
      "type": "add",
      "id": "fd52e6c11792c7f2",
      "item": {
        "type": "paragraph",
        "id": "fd52e6c11792c7f2",
        "text": "Training this type of model has been a long-standing problem in the machine learning community, and classically, most approaches have had one of three serious drawbacks. First, they might require strong assumptions about the structure in the data. Second, they might make severe approximations, leading to sub-optimal models. Or third, they might rely on computationally expensive inference procedures like Markov Chain Monte Carlo. More recently, some works have made tremendous progress in training neural networks as powerful function approximators through backpropagation [9]. These advances have given rise to promising frameworks which can use backpropagation-based function approximators to build generative models."
      },
      "after": "931440f05777392d",
      "date": 1679485530227
    },
    {
      "type": "edit",
      "id": "fd52e6c11792c7f2",
      "item": {
        "type": "paragraph",
        "id": "fd52e6c11792c7f2",
        "text": "Training this type of model has been a long-standing problem in the machine learning community, and classically, most approaches have had one of three serious drawbacks. First, they might require strong assumptions about the structure in the data. Second, they might make severe approximations, leading to sub-optimal models. Or third, they might rely on computationally expensive inference procedures like Markov Chain Monte Carlo. "
      },
      "date": 1679485554017
    },
    {
      "type": "add",
      "id": "f993abde16e85c63",
      "item": {
        "type": "paragraph",
        "id": "f993abde16e85c63",
        "text": "More recently, some works have made tremendous progress in training neural networks as powerful function approximators through backpropagation [9]. These advances have given rise to promising frameworks which can use backpropagation-based function approximators to build generative models."
      },
      "after": "fd52e6c11792c7f2",
      "date": 1679485555280
    },
    {
      "type": "add",
      "id": "33b46e33c991f53e",
      "item": {
        "type": "paragraph",
        "id": "33b46e33c991f53e",
        "text": "One of the most popular such frameworks is the [[Variational Autoencoder]] [1, 3], the subject of this tutorial. The assumptions of this model are weak, and training is fast via backpropagation. VAEs do make an approximation, but the error introduced by this approximation is arguably small given high-capacity models. These characteristics have contributed to a quick rise in their popularity."
      },
      "after": "f993abde16e85c63",
      "date": 1679485582303
    },
    {
      "type": "add",
      "id": "12494e7096d0fca9",
      "item": {
        "type": "paragraph",
        "id": "12494e7096d0fca9",
        "text": "This tutorial is intended to be an informal introduction to VAEs, and not a formal scientific paper about them. It is aimed at people who might have uses for generative models, but might not have a strong background in the variatonal Bayesian methods and “minimum description length” coding models on which VAEs are based. This tutorial began its life as a presentation for computer vision reading groups at UC Berkeley and Carnegie Mellon, and hence has a bias toward a vision audience. Suggestions for improvement are appreciated."
      },
      "after": "33b46e33c991f53e",
      "date": 1679485606197
    },
    {
      "type": "edit",
      "id": "12494e7096d0fca9",
      "item": {
        "type": "paragraph",
        "id": "12494e7096d0fca9",
        "text": "This tutorial is intended to be an informal introduction to VAEs, and not a formal scientific paper about them. It is aimed at people who might have uses for generative models, but might not have a strong background in the variatonal Bayesian methods and “minimum description length” coding models on which VAEs are based. This tutorial began its life as a presentation for computer vision reading groups at UC Berkeley and Carnegie Mellon, and hence has a bias toward a vision audience. Suggestions for improvement are appreciated. [https://github.com/cdoersch/vae_tutorial github]"
      },
      "date": 1679485731557
    }
  ]
}