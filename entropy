{
  "title": "Entropy",
  "story": [
    {
      "type": "markdown",
      "id": "d5611fb30f40bbb2",
      "text": "is a measure of the [[Disorder]] or decay of order"
    },
    {
      "type": "markdown",
      "id": "4a7c947543b3db02",
      "text": "Work, such as that performed by thermal energy, causes a change that in turn depends on a distinction between one state and another. In an analogous process, a message involves a selection and differentiation of a particular message."
    },
    {
      "type": "video",
      "id": "026e4139100eb43a",
      "text": "START 4958\nYOUTUBE XWuUdJo9ubM\nEntropy and [[Before and After]]"
    },
    {
      "type": "markdown",
      "id": "a1012e26702456a2",
      "text": "Entropy measures the decay of the differentiation which permits work to be done\nor messages articulated. Because of its unidirectional progress, the most probable state is one where differentiation has been relaxed. ’Thus,\" Entropy is a measure of a system's inexorable tendency to move from a less to a more probable state\". Entropy can be described in general or precise terms based on the area of application. An organization requires new infusions of effort to maintain its vitality and 1% offset its tendency to run down. An animal needs a continuous supply of food to maintain itself. Without it, it can no longer maintain its order and it moves toward maximum entropy - and death. The perception of disorder in a system varies according to the observer's definition offee system and its purposes. For example, in one organization, order may be perceived in terms of hierarchy while in another it will be seen in terms of a network of small self-organizing units.\n\nAs a measure of disorder in a system, the conception of entropy and the ways it is measured differ somewhat depending on the system under consideration."
    },
    {
      "type": "markdown",
      "id": "ebd8e0ff436c0901",
      "text": "In mathematics, entropy is expressed as the logarithm of the probability of a certain state.\n\nIn thermodynamics, entropy is the subject of the Second Law which says that heat in a system will tend to even out until none is available for doing work. If work is to continue, heat must be added to the system to offset the effect of entropy."
    },
    {
      "type": "markdown",
      "id": "5afcbae48d137ef2",
      "text": "In statistical mechanics, entropy is a measure of disorder in the arrangement of atoms. The maximum disorder and the most probable-state is one of random distribution. The entropy increases as the degree of randomness increases.\n\nIn cybernetics and information theory, the amount of information corresponds to the degree of order in the system and the entropy to the amount of disorder. Thus, entropy and information are seen as opposing concepts with entropy defined as information with a negative sign. A system maintains its order through the addition of information. Entropy may be seen in some contexts as a measure of uncertainty. In information theory, entropy may be calculated for a given source and subtracted from its maidmwm entropy* This gives its relative entropy. The redundancy of the message is figured at one minus relative entropy.\n# SOURCE\nFor Thermodynamics, see Clausius; for statistical mechanics, see Boltzmann and Gibbs; For cybernetics and information theory see \nWiener, N. (1961). Cybernetics; or, Control and communication in the animal and the\nmachine. New York: M.I.T. Press.\nShannon, C. E., & Weaver, W. (1964). The\nmathematical theory of communication. Urbana: University of Illinois Press.\n# EXAMPLES\n• on the way from the delicatessen to your desk, the hot coffee gets luke warm and your ice cream begins to melt Before you ordered them, electricity supplied the energy to keep the coffee hot in the urn and the ice cream cold in the freezer\n• holiday traffic is backed up for miles waiting for a backlog at the bridge to be cleared. The once orderly movement of cars is transformed to a collection of Frisbee games, picnics, people sitting on fenders, etc.\n• a puppy overturns the garbage and messes up the yard.\n• an untended garden becomes overgrown\n• a sports team's plays become sloppy if they have not had enough practice\n# NON-EXAMPLES\n• the members of a community organize themselves to maintain essential services after a flood has cut them off from normal sources of supply\n• a message is transmitted with a given degree of accuracy a system grows and becomes more specialized • an organization maintains and enhances its identity through participative planning sessions and training\nprobable Error\n• viewing the progress of decay and disorder ilia system as an aberration, neglecting to consider the allocation of resources to maintain system order\n# SEE\nInformation; Self-organization; Autopoeisis; Uncertainty"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Entropy",
        "story": []
      },
      "date": 1605667103807
    },
    {
      "item": {
        "type": "factory",
        "id": "d5611fb30f40bbb2"
      },
      "id": "d5611fb30f40bbb2",
      "type": "add",
      "date": 1605667105408
    },
    {
      "type": "edit",
      "id": "d5611fb30f40bbb2",
      "item": {
        "type": "markdown",
        "id": "d5611fb30f40bbb2",
        "text": " Entropy is a measure ol disorder or of the decay of order. Work., such as is accomplished by heat energy, causes a change to occur which in turn depends on a distinction between the one state and the other- In an analogous process a message involves a choice and the distinction of one particular message.\nEntropy measures the decayof the differentiation which permits work to be done\nor messages articulated. Because of its unidirectional progress, the most probable state is one where differentiation has been relaxed. ’Thus,\" Entropy is a measure of a system's inexorable tendency to move from a less to a more probable state\". Entropy can be described in general or precise terms based on the area of application. An organization requires new infusions of effort to maintain its vitality and 1% offset its tendency to run down. An animal\nneeds a continuous supply of food to maintain itself. Without it, it can no longer maintainits order and it moves toward maximum entropy - and death. The perception of disorder in a system varies according to the observer's definition offee system and its purposes. For example, in one organization, order may be perceived in terms of hierarchy while in\nanother it will be seen in terms of a network of small self-organizing units.\nAs a measure of disorder in a system, the conception of entropy and the ways it is measured differ somewhat depending on the system under consideration.\nIn mathematics, entropy is expressed as the logarithm of the probability of a certain state.\nIn thermodynamics, entropy is the subject of the Second Law which says that heat in a system will tend to even out until none is available for doing work. If work is to continue, heat must be added to the system to offset the effect of entropy."
      },
      "date": 1605667116799
    },
    {
      "type": "edit",
      "id": "d5611fb30f40bbb2",
      "item": {
        "type": "markdown",
        "id": "d5611fb30f40bbb2",
        "text": " Entropy is a measure ol disorder or of the decay of order. Work., such as is accomplished by heat energy, causes a change to occur which in turn depends on a distinction between the one state and the other- In an analogous process a message involves a choice and the distinction of one particular message.\n\nEntropy measures the decay of the differentiation which permits work to be done\nor messages articulated. Because of its unidirectional progress, the most probable state is one where differentiation has been relaxed. ’Thus,\" Entropy is a measure of a system's inexorable tendency to move from a less to a more probable state\". Entropy can be described in general or precise terms based on the area of application. An organization requires new infusions of effort to maintain its vitality and 1% offset its tendency to run down. An animal needs a continuous supply of food to maintain itself. Without it, it can no longer maintain its order and it moves toward maximum entropy - and death. The perception of disorder in a system varies according to the observer's definition offee system and its purposes. For example, in one organization, order may be perceived in terms of hierarchy while in\nanother it will be seen in terms of a network of small self-organizing units.\n\nAs a measure of disorder in a system, the conception of entropy and the ways it is measured differ somewhat depending on the system under consideration.\nIn mathematics, entropy is expressed as the logarithm of the probability of a certain state.\nIn thermodynamics, entropy is the subject of the Second Law which says that heat in a system will tend to even out until none is available for doing work. If work is to continue, heat must be added to the system to offset the effect of entropy."
      },
      "date": 1605667150299
    },
    {
      "type": "add",
      "id": "5afcbae48d137ef2",
      "item": {
        "type": "markdown",
        "id": "5afcbae48d137ef2",
        "text": "In statistical mechanics, entropy is a measure of disorder in the arrangement of atoms. The maximum disorder and the most probable-state is one of random distribution. The entropy increases as the degree of randomness increases.\n\nIn cybernetics and information theory, the amount of information corresponds to the degree of order in the system and the entropy to the amount of disorder. Thus, entropy and information are seen as opposing concepts with entropy defined as information with a negative sign. A system maintains its order through the addition of information. Entropy may be seen in some contexts as a measure of uncertainty. In information theory, entropy may be calculated fora given source and subtracted from its maidmwm entropy* This gives its relative entropy. The redundancy of the message is figured at one minus relative entropy.\n# SOURCE\nFor Thermodynamics, see Clausius; for statistical mechanics, see Boltzmann and Gibbs; For cybernetics and information theory see Wiener, N. (1961). Cybernetics; or, Control and\ncommunication in the animal and the\nmachine. New York: M.I.T. Press. Shannon, C. E., & Weaver, W. (1964). The\nmathematical theory of communication. Urbana: University of Illinois Press.\n# EXAMPLES\n• on the way from the delicatessen to your desk, the hot coffee gets luke warm and your ice cream begins to melt Before you ordered them, electricity supplied the energy to keep the coffee hot in the urn and the ice cream cold in the freezer\n• holiday traffic is backed up for miles waiting for a backlog at the bridge to be cleared. The once orderly movement of cars is transformed to a collection of Frisbee games, picnics, people sitting on fenders, etc.\n• a puppy overturns the garbage and messes up the yard."
      },
      "after": "d5611fb30f40bbb2",
      "date": 1605667215205
    },
    {
      "type": "edit",
      "id": "d5611fb30f40bbb2",
      "item": {
        "type": "markdown",
        "id": "d5611fb30f40bbb2",
        "text": " Entropy is a measure ol disorder or of the decay of order. Work., such as is accomplished by heat energy, causes a change to occur which in turn depends on a distinction between the one state and the other- In an analogous process a message involves a choice and the distinction of one particular message.\n\nEntropy measures the decay of the differentiation which permits work to be done\nor messages articulated. Because of its unidirectional progress, the most probable state is one where differentiation has been relaxed. ’Thus,\" Entropy is a measure of a system's inexorable tendency to move from a less to a more probable state\". Entropy can be described in general or precise terms based on the area of application. An organization requires new infusions of effort to maintain its vitality and 1% offset its tendency to run down. An animal needs a continuous supply of food to maintain itself. Without it, it can no longer maintain its order and it moves toward maximum entropy - and death. The perception of disorder in a system varies according to the observer's definition offee system and its purposes. For example, in one organization, order may be perceived in terms of hierarchy while in another it will be seen in terms of a network of small self-organizing units.\n\nAs a measure of disorder in a system, the conception of entropy and the ways it is measured differ somewhat depending on the system under consideration."
      },
      "date": 1605667246135
    },
    {
      "type": "add",
      "id": "ebd8e0ff436c0901",
      "item": {
        "type": "markdown",
        "id": "ebd8e0ff436c0901",
        "text": "In mathematics, entropy is expressed as the logarithm of the probability of a certain state.\n\nIn thermodynamics, entropy is the subject of the Second Law which says that heat in a system will tend to even out until none is available for doing work. If work is to continue, heat must be added to the system to offset the effect of entropy."
      },
      "after": "d5611fb30f40bbb2",
      "date": 1605667248585
    },
    {
      "type": "edit",
      "id": "5afcbae48d137ef2",
      "item": {
        "type": "markdown",
        "id": "5afcbae48d137ef2",
        "text": "In statistical mechanics, entropy is a measure of disorder in the arrangement of atoms. The maximum disorder and the most probable-state is one of random distribution. The entropy increases as the degree of randomness increases.\n\nIn cybernetics and information theory, the amount of information corresponds to the degree of order in the system and the entropy to the amount of disorder. Thus, entropy and information are seen as opposing concepts with entropy defined as information with a negative sign. A system maintains its order through the addition of information. Entropy may be seen in some contexts as a measure of uncertainty. In information theory, entropy may be calculated for a given source and subtracted from its maidmwm entropy* This gives its relative entropy. The redundancy of the message is figured at one minus relative entropy.\n# SOURCE\nFor Thermodynamics, see Clausius; for statistical mechanics, see Boltzmann and Gibbs; For cybernetics and information theory see Wiener, N. (1961). Cybernetics; or, Control and communication in the animal and the\nmachine. New York: M.I.T. Press. Shannon, C. E., & Weaver, W. (1964). The\nmathematical theory of communication. Urbana: University of Illinois Press.\n# EXAMPLES\n• on the way from the delicatessen to your desk, the hot coffee gets luke warm and your ice cream begins to melt Before you ordered them, electricity supplied the energy to keep the coffee hot in the urn and the ice cream cold in the freezer\n• holiday traffic is backed up for miles waiting for a backlog at the bridge to be cleared. The once orderly movement of cars is transformed to a collection of Frisbee games, picnics, people sitting on fenders, etc.\n• a puppy overturns the garbage and messes up the yard."
      },
      "date": 1605667292476
    },
    {
      "type": "edit",
      "id": "5afcbae48d137ef2",
      "item": {
        "type": "markdown",
        "id": "5afcbae48d137ef2",
        "text": "In statistical mechanics, entropy is a measure of disorder in the arrangement of atoms. The maximum disorder and the most probable-state is one of random distribution. The entropy increases as the degree of randomness increases.\n\nIn cybernetics and information theory, the amount of information corresponds to the degree of order in the system and the entropy to the amount of disorder. Thus, entropy and information are seen as opposing concepts with entropy defined as information with a negative sign. A system maintains its order through the addition of information. Entropy may be seen in some contexts as a measure of uncertainty. In information theory, entropy may be calculated for a given source and subtracted from its maidmwm entropy* This gives its relative entropy. The redundancy of the message is figured at one minus relative entropy.\n# SOURCE\nFor Thermodynamics, see Clausius; for statistical mechanics, see Boltzmann and Gibbs; For cybernetics and information theory see \nWiener, N. (1961). Cybernetics; or, Control and communication in the animal and the\nmachine. New York: M.I.T. Press. "
      },
      "date": 1605667348701
    },
    {
      "type": "edit",
      "id": "5afcbae48d137ef2",
      "item": {
        "type": "markdown",
        "id": "5afcbae48d137ef2",
        "text": "In statistical mechanics, entropy is a measure of disorder in the arrangement of atoms. The maximum disorder and the most probable-state is one of random distribution. The entropy increases as the degree of randomness increases.\n\nIn cybernetics and information theory, the amount of information corresponds to the degree of order in the system and the entropy to the amount of disorder. Thus, entropy and information are seen as opposing concepts with entropy defined as information with a negative sign. A system maintains its order through the addition of information. Entropy may be seen in some contexts as a measure of uncertainty. In information theory, entropy may be calculated for a given source and subtracted from its maidmwm entropy* This gives its relative entropy. The redundancy of the message is figured at one minus relative entropy.\n# SOURCE\nFor Thermodynamics, see Clausius; for statistical mechanics, see Boltzmann and Gibbs; For cybernetics and information theory see \nWiener, N. (1961). Cybernetics; or, Control and communication in the animal and the\nmachine. New York: M.I.T. Press.\nShannon, C. E., & Weaver, W. (1964). The\nmathematical theory of communication. Urbana: University of Illinois Press.\n# EXAMPLES\n• on the way from the delicatessen to your desk, the hot coffee gets luke warm and your ice cream begins to melt Before you ordered them, electricity supplied the energy to keep the coffee hot in the urn and the ice cream cold in the freezer\n• holiday traffic is backed up for miles waiting for a backlog at the bridge to be cleared. The once orderly movement of cars is transformed to a collection of Frisbee games, picnics, people sitting on fenders, etc.\n• a puppy overturns the garbage and messes up the yard."
      },
      "date": 1605667369194
    },
    {
      "item": {
        "type": "factory",
        "id": "9d560ab619866a93"
      },
      "id": "9d560ab619866a93",
      "type": "add",
      "after": "5afcbae48d137ef2",
      "date": 1605713701614
    },
    {
      "type": "remove",
      "id": "9d560ab619866a93",
      "date": 1605713747410
    },
    {
      "type": "edit",
      "id": "5afcbae48d137ef2",
      "item": {
        "type": "markdown",
        "id": "5afcbae48d137ef2",
        "text": "In statistical mechanics, entropy is a measure of disorder in the arrangement of atoms. The maximum disorder and the most probable-state is one of random distribution. The entropy increases as the degree of randomness increases.\n\nIn cybernetics and information theory, the amount of information corresponds to the degree of order in the system and the entropy to the amount of disorder. Thus, entropy and information are seen as opposing concepts with entropy defined as information with a negative sign. A system maintains its order through the addition of information. Entropy may be seen in some contexts as a measure of uncertainty. In information theory, entropy may be calculated for a given source and subtracted from its maidmwm entropy* This gives its relative entropy. The redundancy of the message is figured at one minus relative entropy.\n# SOURCE\nFor Thermodynamics, see Clausius; for statistical mechanics, see Boltzmann and Gibbs; For cybernetics and information theory see \nWiener, N. (1961). Cybernetics; or, Control and communication in the animal and the\nmachine. New York: M.I.T. Press.\nShannon, C. E., & Weaver, W. (1964). The\nmathematical theory of communication. Urbana: University of Illinois Press.\n# EXAMPLES\n• on the way from the delicatessen to your desk, the hot coffee gets luke warm and your ice cream begins to melt Before you ordered them, electricity supplied the energy to keep the coffee hot in the urn and the ice cream cold in the freezer\n• holiday traffic is backed up for miles waiting for a backlog at the bridge to be cleared. The once orderly movement of cars is transformed to a collection of Frisbee games, picnics, people sitting on fenders, etc.\n• a puppy overturns the garbage and messes up the yard.\n• an untended garden becomes overgrown\n• a sports team's plays become sloppy if they have not had enough practice\n# NON-EXAMPLES\n• the members of a community organize themselves to maintain essential services after a flood has cut them off from normal sources of supply\n• a message is transmitted with a given degree of accuracy a system grows and becomes more specialized • an organization maintains and enhances its identity through participative planning sessions and training\nprobable Error\n• viewing the progress of decay and disorder ilia system as an aberration, neglecting to consider the allocation of resources to maintain system order\n# SEE\nInformation; Self-organization; Autopoeisis; Uncertainty"
      },
      "date": 1605713751938
    },
    {
      "type": "fork",
      "site": "marc.tries.fed.wiki",
      "date": 1661079416534
    },
    {
      "type": "edit",
      "id": "d5611fb30f40bbb2",
      "item": {
        "type": "markdown",
        "id": "d5611fb30f40bbb2",
        "text": "is a measure ol disorder or of the decay of order. Work., such as is accomplished by heat energy, causes a change to occur which in turn depends on a distinction between the one state and the other- In an analogous process a message involves a choice and the distinction of one particular message."
      },
      "date": 1695454692628
    },
    {
      "type": "add",
      "id": "a1012e26702456a2",
      "item": {
        "type": "markdown",
        "id": "a1012e26702456a2",
        "text": "Entropy measures the decay of the differentiation which permits work to be done\nor messages articulated. Because of its unidirectional progress, the most probable state is one where differentiation has been relaxed. ’Thus,\" Entropy is a measure of a system's inexorable tendency to move from a less to a more probable state\". Entropy can be described in general or precise terms based on the area of application. An organization requires new infusions of effort to maintain its vitality and 1% offset its tendency to run down. An animal needs a continuous supply of food to maintain itself. Without it, it can no longer maintain its order and it moves toward maximum entropy - and death. The perception of disorder in a system varies according to the observer's definition offee system and its purposes. For example, in one organization, order may be perceived in terms of hierarchy while in another it will be seen in terms of a network of small self-organizing units.\n\nAs a measure of disorder in a system, the conception of entropy and the ways it is measured differ somewhat depending on the system under consideration."
      },
      "after": "d5611fb30f40bbb2",
      "date": 1695454693630
    },
    {
      "id": "026e4139100eb43a",
      "type": "add",
      "item": {
        "type": "video",
        "id": "026e4139100eb43a",
        "text": "START 4958\nYOUTUBE XWuUdJo9ubM"
      },
      "after": "d5611fb30f40bbb2",
      "attribution": {
        "page": "scratch"
      },
      "date": 1707924055778
    },
    {
      "type": "edit",
      "id": "026e4139100eb43a",
      "item": {
        "type": "video",
        "id": "026e4139100eb43a",
        "text": "START 4958\nYOUTUBE XWuUdJo9ubM\nEntropy"
      },
      "date": 1707924069333
    },
    {
      "type": "edit",
      "id": "d5611fb30f40bbb2",
      "item": {
        "type": "markdown",
        "id": "d5611fb30f40bbb2",
        "text": "is a measure of disorder or of the decay of order. Work., such as is accomplished by heat energy, causes a change to occur which in turn depends on a distinction between the one state and the other. In an analogous process a message involves a choice and the distinction of one particular message."
      },
      "date": 1707924206873
    },
    {
      "type": "edit",
      "id": "d5611fb30f40bbb2",
      "item": {
        "type": "markdown",
        "id": "d5611fb30f40bbb2",
        "text": "is a measure of the disorder or decay of order"
      },
      "date": 1707924255974
    },
    {
      "type": "add",
      "id": "4a7c947543b3db02",
      "item": {
        "type": "markdown",
        "id": "4a7c947543b3db02",
        "text": "Work, such as that performed by thermal energy, causes a change that in turn depends on a distinction between one state and another. In an analogous process, a message involves a selection and differentiation of a particular message."
      },
      "after": "d5611fb30f40bbb2",
      "date": 1707924258310
    },
    {
      "type": "edit",
      "id": "026e4139100eb43a",
      "item": {
        "type": "video",
        "id": "026e4139100eb43a",
        "text": "START 4958\nYOUTUBE XWuUdJo9ubM\nEntropy and [[Before and After]]"
      },
      "date": 1707924526825
    },
    {
      "type": "edit",
      "id": "d5611fb30f40bbb2",
      "item": {
        "type": "markdown",
        "id": "d5611fb30f40bbb2",
        "text": "is a measure of the [[Disorder]] or decay of order"
      },
      "date": 1707926183860
    }
  ]
}