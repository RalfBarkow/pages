{
  "title": "Anthropomorphism of a Language Model",
  "story": [
    {
      "type": "paragraph",
      "id": "46b35d893db7771b",
      "text": "Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. "
    },
    {
      "type": "paragraph",
      "id": "a1d019b92c524586",
      "text": "Sitting squarely at the centre of this intersection are [[Large Language Model]]s (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[know]]s”, “[[believe]]s”, and “[[think]]s”, when describing these systems. "
    },
    {
      "type": "paragraph",
      "id": "58df544defdd8a41",
      "text": "To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere."
    },
    {
      "type": "pagefold",
      "id": "cfeb6c163e330c7e",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "609c23b10bb0ebdf",
      "text": "SHANAHAN, Murray, 2023. Talking About [[Large Language Model]]s. Online. 25 January 2023. arXiv. arXiv:2212.03551. [Accessed 28 January 2023]. "
    },
    {
      "type": "paragraph",
      "id": "2ddca56d32ab6538",
      "text": "⇒ [[Prompt Programming]]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Anthropomorphism of a Language Model",
        "story": []
      },
      "date": 1675058488394
    },
    {
      "id": "6c25d934cb0a6a73",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "6c25d934cb0a6a73",
        "text": "Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.arXiv:2212.03551 [cs]\n"
      },
      "date": 1675058490275
    },
    {
      "item": {
        "type": "factory",
        "id": "cfeb6c163e330c7e"
      },
      "id": "cfeb6c163e330c7e",
      "type": "add",
      "after": "6c25d934cb0a6a73",
      "date": 1675058492063
    },
    {
      "type": "edit",
      "id": "cfeb6c163e330c7e",
      "item": {
        "type": "pagefold",
        "id": "cfeb6c163e330c7e",
        "text": "~"
      },
      "date": 1675058494624
    },
    {
      "id": "609c23b10bb0ebdf",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "609c23b10bb0ebdf",
        "text": "\nSHANAHAN, Murray, 2023. Talking About Large Language Models. Online. 25 January 2023. arXiv. arXiv:2212.03551. [Accessed 28 January 2023]. "
      },
      "after": "cfeb6c163e330c7e",
      "date": 1675058497913
    },
    {
      "type": "edit",
      "id": "609c23b10bb0ebdf",
      "item": {
        "type": "paragraph",
        "id": "609c23b10bb0ebdf",
        "text": "SHANAHAN, Murray, 2023. Talking About [[Large Language Model]]s. Online. 25 January 2023. arXiv. arXiv:2212.03551. [Accessed 28 January 2023]. "
      },
      "date": 1675058537223
    },
    {
      "item": {
        "type": "factory",
        "id": "cae43d45adcd6272"
      },
      "id": "cae43d45adcd6272",
      "type": "add",
      "after": "609c23b10bb0ebdf",
      "date": 1675058762778
    },
    {
      "type": "edit",
      "id": "cae43d45adcd6272",
      "item": {
        "type": "paragraph",
        "id": "cae43d45adcd6272",
        "text": "\nREYNOLDS, Laria and MCDONELL, Kyle, 2021. Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. In: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Online. New York, NY, USA: Association for Computing Machinery. 8 May 2021. p. 1–7. [Accessed 29 January 2023]. CHI EA ’21. ISBN 978-1-4503-8095-9. DOI 10.1145/3411763.3451760. Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.\n"
      },
      "date": 1675058765520
    },
    {
      "type": "edit",
      "id": "cae43d45adcd6272",
      "item": {
        "type": "paragraph",
        "id": "cae43d45adcd6272",
        "text": "REYNOLDS, Laria and MCDONELL, Kyle, 2021. Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. In: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Online. New York, NY, USA: Association for Computing Machinery. 8 May 2021. p. 1–7. [Accessed 29 January 2023]. CHI EA ’21. ISBN 978-1-4503-8095-9. DOI 10.1145/3411763.3451760. "
      },
      "date": 1675058774313
    },
    {
      "type": "add",
      "id": "3ce0b3514b096c62",
      "item": {
        "type": "paragraph",
        "id": "3ce0b3514b096c62",
        "text": "Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. We discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.\n"
      },
      "after": "cae43d45adcd6272",
      "date": 1675058774601
    },
    {
      "type": "add",
      "id": "2ddca56d32ab6538",
      "item": {
        "type": "paragraph",
        "id": "2ddca56d32ab6538",
        "text": "⇒ [[Prompt Programming]]"
      },
      "after": "609c23b10bb0ebdf",
      "date": 1675058892389
    },
    {
      "id": "3ce0b3514b096c62",
      "type": "remove",
      "date": 1675058898270
    },
    {
      "id": "cae43d45adcd6272",
      "type": "remove",
      "date": 1675058909126
    },
    {
      "type": "edit",
      "id": "6c25d934cb0a6a73",
      "item": {
        "type": "paragraph",
        "id": "6c25d934cb0a6a73",
        "text": "Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. "
      },
      "date": 1675073269918
    },
    {
      "type": "add",
      "id": "2fe76a60bf3a1f76",
      "item": {
        "type": "paragraph",
        "id": "2fe76a60bf3a1f76",
        "text": "Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.arXiv:2212.03551 [cs]\n"
      },
      "after": "6c25d934cb0a6a73",
      "date": 1675073270707
    },
    {
      "type": "edit",
      "id": "2fe76a60bf3a1f76",
      "item": {
        "type": "paragraph",
        "id": "2fe76a60bf3a1f76",
        "text": "Sitting squarely at the centre of this intersection are [[large language model]]s (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.arXiv:2212.03551 [cs]\n"
      },
      "date": 1675073292662
    },
    {
      "type": "edit",
      "id": "2fe76a60bf3a1f76",
      "item": {
        "type": "paragraph",
        "id": "2fe76a60bf3a1f76",
        "text": "Sitting squarely at the centre of this intersection are [[Large Language Model]]s (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.arXiv:2212.03551 [cs]\n"
      },
      "date": 1675073306544
    },
    {
      "type": "edit",
      "id": "2fe76a60bf3a1f76",
      "item": {
        "type": "paragraph",
        "id": "2fe76a60bf3a1f76",
        "text": "Sitting squarely at the centre of this intersection are [[Large Language Model]]s (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.arXiv:2212.03551 [cs]\n"
      },
      "date": 1675073339547
    },
    {
      "type": "edit",
      "id": "2fe76a60bf3a1f76",
      "item": {
        "type": "paragraph",
        "id": "2fe76a60bf3a1f76",
        "text": "Sitting squarely at the centre of this intersection are [[Large Language Model]]s (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. "
      },
      "date": 1675073961625
    },
    {
      "type": "add",
      "id": "148000c68e22ef66",
      "item": {
        "type": "paragraph",
        "id": "148000c68e22ef66",
        "text": "To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.arXiv:2212.03551 [cs]\n"
      },
      "after": "2fe76a60bf3a1f76",
      "date": 1675073962629
    },
    {
      "id": "a1d019b92c524586",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "a1d019b92c524586",
        "text": "Sitting squarely at the centre of this intersection are [[Large Language Model]]s (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to [[Anthropomorphism]], to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “[[know]]s”, “[[believe]]s”, and “[[think]]s”, when describing these systems. "
      },
      "after": "6c25d934cb0a6a73",
      "date": 1675074009880
    },
    {
      "type": "remove",
      "id": "2fe76a60bf3a1f76",
      "date": 1675074012207
    },
    {
      "id": "46b35d893db7771b",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "46b35d893db7771b",
        "text": "Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. "
      },
      "after": "6c25d934cb0a6a73",
      "date": 1675074031221
    },
    {
      "type": "remove",
      "id": "6c25d934cb0a6a73",
      "date": 1675074032883
    },
    {
      "id": "58df544defdd8a41",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "58df544defdd8a41",
        "text": "To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere."
      },
      "after": "a1d019b92c524586",
      "date": 1675074041946
    },
    {
      "type": "remove",
      "id": "148000c68e22ef66",
      "date": 1675074043645
    }
  ]
}