{
  "title": "Distributed Representations of Sentences and Documents",
  "story": [
    {
      "type": "paragraph",
      "id": "87136d81fe3de0e3",
      "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the ordering of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a [[Dense Vector]] which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. [https://cs.stanford.edu/~quocle/paragraph_vector.pdf pdf] [https://arxiv.org/abs/1405.4053 arxiv] [https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4?gi=6ec3a115db69 post]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Distributed Representations of Sentences and Documents",
        "story": []
      },
      "date": 1711001033371
    },
    {
      "item": {
        "type": "factory",
        "id": "87136d81fe3de0e3"
      },
      "id": "87136d81fe3de0e3",
      "type": "add",
      "date": 1711001034930
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the order-\ning of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks"
      },
      "date": 1711001107496
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the order-\ning of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. "
      },
      "date": 1711001118198
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the order-\ning of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. [https://cs.stanford.edu/~quocle/paragraph_vector.pdf pdf]"
      },
      "date": 1711001126406
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the ordering of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. [https://cs.stanford.edu/~quocle/paragraph_vector.pdf pdf]"
      },
      "date": 1711001158296
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the ordering of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a [[Dense Vector]] which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. [https://cs.stanford.edu/~quocle/paragraph_vector.pdf pdf]"
      },
      "date": 1711001202285
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the ordering of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a [[Dense Vector]] which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. [https://cs.stanford.edu/~quocle/paragraph_vector.pdf pdf] []"
      },
      "date": 1711001240760
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the ordering of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a [[Dense Vector]] which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. [https://cs.stanford.edu/~quocle/paragraph_vector.pdf pdf] [https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4?gi=6ec3a115db69]"
      },
      "date": 1711001268447
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the ordering of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a [[Dense Vector]] which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. [https://cs.stanford.edu/~quocle/paragraph_vector.pdf pdf] [https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4?gi=6ec3a115db69 post]"
      },
      "date": 1711001277694
    },
    {
      "type": "edit",
      "id": "87136d81fe3de0e3",
      "item": {
        "type": "paragraph",
        "id": "87136d81fe3de0e3",
        "text": "Many machine learning algorithms require the\ninput to be represented as a fixed-length feature\nvector. When it comes to texts, one of the most\ncommon fixed-length features is bag-of-words.\nDespite their popularity, bag-of-words features\nhave two major weaknesses: they lose the ordering of the words and they also ignore semantics\nof the words. For example, “powerful,” “strong”\nand “Paris” are equally distant. In this paper, we\npropose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a [[Dense Vector]] which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks. [https://cs.stanford.edu/~quocle/paragraph_vector.pdf pdf] [https://arxiv.org/abs/1405.4053 arxiv] [https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4?gi=6ec3a115db69 post]"
      },
      "date": 1711002690921
    }
  ]
}