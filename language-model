{
  "title": "Language Model",
  "story": [
    {
      "type": "paragraph",
      "id": "1d0d14386674582b",
      "text": "is a statistical probability distribution over a set of words."
    },
    {
      "type": "pagefold",
      "id": "ee2d48161b68dba8",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "e200d4419c09834a",
      "text": "HORISHNY, Elizabeth, 2022. Romantic-Computing. Online. 1 June 2022. arXiv. arXiv:2206.11864. [Accessed 16 February 2023]. "
    },
    {
      "type": "paragraph",
      "id": "4ca98fa7be5344bc",
      "text": "In this paper we compare various text generation models’ ability to write poetry in the style of early English Romanticism. These models include: Character-Level Recurrent Neural Networks with Long Short-Term Memory, Hugging Face’s GPT-2, OpenAI’s GPT-3, and EleutherAI’s GPT-NEO. Quality was measured based syllable count and coherence with the automatic evaluation metric GRUEN. Character-Level Recurrent Neural Networks performed far worse compared to transformer models. And, as parameter-size increased, the quality of transformer models’ poems improved. These models are typically not compared in a creative context, and we are happy to contribute.arXiv:2206.11864 [cs]\n"
    },
    {
      "type": "reference",
      "id": "026ff42c149f6327",
      "site": "wiki.ralfbarkow.ch",
      "slug": "probabilities",
      "title": "Probabilities",
      "text": "\"What if you replace [[probabilities]] with [[possibilities]]?\" –– [[Tai-Danae Bradley]], [[Algebra + Statistics = ?]]."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Language Model",
        "story": []
      },
      "date": 1676544157053
    },
    {
      "item": {
        "type": "factory",
        "id": "1d0d14386674582b"
      },
      "id": "1d0d14386674582b",
      "type": "add",
      "date": 1676544159340
    },
    {
      "type": "edit",
      "id": "1d0d14386674582b",
      "item": {
        "type": "paragraph",
        "id": "1d0d14386674582b",
        "text": "is a statistical probability distribution over a set of words."
      },
      "date": 1676544163079
    },
    {
      "item": {
        "type": "factory",
        "id": "e200d4419c09834a"
      },
      "id": "e200d4419c09834a",
      "type": "add",
      "after": "1d0d14386674582b",
      "date": 1676544173850
    },
    {
      "type": "edit",
      "id": "e200d4419c09834a",
      "item": {
        "type": "paragraph",
        "id": "e200d4419c09834a",
        "text": "\nHORISHNY, Elizabeth, 2022. Romantic-Computing. Online. 1 June 2022. arXiv. arXiv:2206.11864. [Accessed 16 February 2023]. In this paper we compare various text generation models’ ability to write poetry in the style of early English Romanticism. These models include: Character-Level Recurrent Neural Networks with Long Short-Term Memory, Hugging Face’s GPT-2, OpenAI’s GPT-3, and EleutherAI’s GPT-NEO. Quality was measured based syllable count and coherence with the automatic evaluation metric GRUEN. Character-Level Recurrent Neural Networks performed far worse compared to transformer models. And, as parameter-size increased, the quality of transformer models’ poems improved. These models are typically not compared in a creative context, and we are happy to contribute.arXiv:2206.11864 [cs]\n"
      },
      "date": 1676544175837
    },
    {
      "type": "edit",
      "id": "e200d4419c09834a",
      "item": {
        "type": "paragraph",
        "id": "e200d4419c09834a",
        "text": "HORISHNY, Elizabeth, 2022. Romantic-Computing. Online. 1 June 2022. arXiv. arXiv:2206.11864. [Accessed 16 February 2023]. "
      },
      "date": 1676544184786
    },
    {
      "type": "add",
      "id": "4ca98fa7be5344bc",
      "item": {
        "type": "paragraph",
        "id": "4ca98fa7be5344bc",
        "text": "In this paper we compare various text generation models’ ability to write poetry in the style of early English Romanticism. These models include: Character-Level Recurrent Neural Networks with Long Short-Term Memory, Hugging Face’s GPT-2, OpenAI’s GPT-3, and EleutherAI’s GPT-NEO. Quality was measured based syllable count and coherence with the automatic evaluation metric GRUEN. Character-Level Recurrent Neural Networks performed far worse compared to transformer models. And, as parameter-size increased, the quality of transformer models’ poems improved. These models are typically not compared in a creative context, and we are happy to contribute.arXiv:2206.11864 [cs]\n"
      },
      "after": "e200d4419c09834a",
      "date": 1676544185461
    },
    {
      "item": {
        "type": "factory",
        "id": "ee2d48161b68dba8"
      },
      "id": "ee2d48161b68dba8",
      "type": "add",
      "after": "4ca98fa7be5344bc",
      "date": 1676544189185
    },
    {
      "type": "edit",
      "id": "ee2d48161b68dba8",
      "item": {
        "type": "pagefold",
        "id": "ee2d48161b68dba8",
        "text": "~"
      },
      "date": 1676544191730
    },
    {
      "id": "ee2d48161b68dba8",
      "type": "move",
      "order": [
        "1d0d14386674582b",
        "ee2d48161b68dba8",
        "e200d4419c09834a",
        "4ca98fa7be5344bc"
      ],
      "date": 1676544194362
    },
    {
      "item": {
        "type": "factory",
        "id": "026ff42c149f6327"
      },
      "id": "026ff42c149f6327",
      "type": "add",
      "after": "4ca98fa7be5344bc",
      "date": 1676544221642
    },
    {
      "type": "edit",
      "id": "026ff42c149f6327",
      "item": {
        "type": "reference",
        "id": "026ff42c149f6327",
        "site": "wiki.ralfbarkow.ch",
        "slug": "probabilities",
        "title": "Probabilities",
        "text": "\"What if you replace [[probabilities]] with [[possibilities]]?\" –– [[Tai-Danae Bradley]], [[Algebra + Statistics = ?]]."
      },
      "date": 1676544225226
    }
  ]
}