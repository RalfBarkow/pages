{
  "title": "Vanishing Gradient Problem",
  "story": [
    {
      "type": "paragraph",
      "id": "42c2a0f754dd1950",
      "text": "In contrast with conventional neural networks, recurrent neural networks (RNN) contain cycles within the network which allow reference of previous outputs. "
    },
    {
      "type": "paragraph",
      "id": "f5a7815ca57df4fe",
      "text": "In other words, RNN allow neural networks to [[remember]]. Unfortunately, on its own, a RNN’s memory, or context, becomes over-saturated with information. Over time, output recycled back into the hidden layer will either explode or become hidden. This problem is commonly referred to as the ’vanishing gradient’ problem. Long short-term memory, or LSTM, is one of many ’attention-mechanisms’ built to prevent this problem. During each time-step, a hidden ‘short-term’ state is composed, retaining information which could have been lost. Essentially, at each time-step or token (think each word in a sentence) an LSTM unit will compute what should be remembered, and consequently, what should be forgotten."
    },
    {
      "type": "pagefold",
      "id": "ee2d48161b68dba8",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "e200d4419c09834a",
      "text": "HORISHNY, Elizabeth, 2022. Romantic-Computing. Online. 1 June 2022. arXiv. arXiv:2206.11864. [Accessed 16 February 2023]. "
    },
    {
      "type": "reference",
      "id": "6ebbb310ea2d7d07",
      "site": "wiki.ralfbarkow.ch",
      "slug": "berlegung",
      "title": "Überlegung",
      "text": "Then one finds out that with the word \"Überlegung\" nothing else is meant. \"Reflecting\", radically carried out, can mean nothing else than folding the course of thoughts into a Möbius strip. And the Möbius dizziness is experienced by everyone who tries to think about something (whatever) radically. Maybe this is connected with the fact that we think with two hands, back and forth between the right and the left one. But the two hands can only be brought to coincide with each other in the fourth dimension, by a möbius-like rotation."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Vanishing Gradient Problem",
        "story": []
      },
      "date": 1676544691277
    },
    {
      "item": {
        "type": "factory",
        "id": "42c2a0f754dd1950"
      },
      "id": "42c2a0f754dd1950",
      "type": "add",
      "date": 1676544693824
    },
    {
      "type": "edit",
      "id": "42c2a0f754dd1950",
      "item": {
        "type": "paragraph",
        "id": "42c2a0f754dd1950",
        "text": "In contrast with conventional neural networks, recurrent neural networks (RNN) contain cycles within the network which allow reference of previous outputs. In other words, RNN allow neural networks to remember. Unfortunately, on its own, a RNN’s memory, or context, becomes over-saturated with information. Over time, output recycled back into the hidden layer will either explode or become hidden. This problem is commonly referred to as the ’vanishing gradient’ problem. Long short-term memory, or LSTM, is one of many ’attention-mechanisms’ built to prevent this problem. During each time-step, a hidden ‘short-term’ state is composed, retaining information which could have been lost. Essentially, at each time-step or token (think each word in a sentence) an LSTM unit will compute what should be remembered, and consequently, what should be forgotten."
      },
      "date": 1676544695801
    },
    {
      "id": "ee2d48161b68dba8",
      "type": "add",
      "item": {
        "type": "pagefold",
        "id": "ee2d48161b68dba8",
        "text": "~"
      },
      "after": "42c2a0f754dd1950",
      "attribution": {
        "page": "Language Model"
      },
      "date": 1676544703499
    },
    {
      "id": "e200d4419c09834a",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "e200d4419c09834a",
        "text": "HORISHNY, Elizabeth, 2022. Romantic-Computing. Online. 1 June 2022. arXiv. arXiv:2206.11864. [Accessed 16 February 2023]. "
      },
      "after": "ee2d48161b68dba8",
      "attribution": {
        "page": "Language Model"
      },
      "date": 1676544706637
    },
    {
      "type": "edit",
      "id": "42c2a0f754dd1950",
      "item": {
        "type": "paragraph",
        "id": "42c2a0f754dd1950",
        "text": "In contrast with conventional neural networks, recurrent neural networks (RNN) contain cycles within the network which allow reference of previous outputs. "
      },
      "date": 1676544721629
    },
    {
      "type": "add",
      "id": "f5a7815ca57df4fe",
      "item": {
        "type": "paragraph",
        "id": "f5a7815ca57df4fe",
        "text": "In other words, RNN allow neural networks to remember. Unfortunately, on its own, a RNN’s memory, or context, becomes over-saturated with information. Over time, output recycled back into the hidden layer will either explode or become hidden. This problem is commonly referred to as the ’vanishing gradient’ problem. Long short-term memory, or LSTM, is one of many ’attention-mechanisms’ built to prevent this problem. During each time-step, a hidden ‘short-term’ state is composed, retaining information which could have been lost. Essentially, at each time-step or token (think each word in a sentence) an LSTM unit will compute what should be remembered, and consequently, what should be forgotten."
      },
      "after": "42c2a0f754dd1950",
      "date": 1676544723122
    },
    {
      "type": "edit",
      "id": "f5a7815ca57df4fe",
      "item": {
        "type": "paragraph",
        "id": "f5a7815ca57df4fe",
        "text": "In other words, RNN allow neural networks to [[remember]]. Unfortunately, on its own, a RNN’s memory, or context, becomes over-saturated with information. Over time, output recycled back into the hidden layer will either explode or become hidden. This problem is commonly referred to as the ’vanishing gradient’ problem. Long short-term memory, or LSTM, is one of many ’attention-mechanisms’ built to prevent this problem. During each time-step, a hidden ‘short-term’ state is composed, retaining information which could have been lost. Essentially, at each time-step or token (think each word in a sentence) an LSTM unit will compute what should be remembered, and consequently, what should be forgotten."
      },
      "date": 1676544741552
    },
    {
      "item": {
        "type": "factory",
        "id": "6ebbb310ea2d7d07"
      },
      "id": "6ebbb310ea2d7d07",
      "type": "add",
      "after": "e200d4419c09834a",
      "date": 1676544830689
    },
    {
      "type": "edit",
      "id": "6ebbb310ea2d7d07",
      "item": {
        "type": "reference",
        "id": "6ebbb310ea2d7d07",
        "site": "wiki.ralfbarkow.ch",
        "slug": "berlegung",
        "title": "Überlegung",
        "text": "Then one finds out that with the word \"Überlegung\" nothing else is meant. \"Reflecting\", radically carried out, can mean nothing else than folding the course of thoughts into a Möbius strip. And the Möbius dizziness is experienced by everyone who tries to think about something (whatever) radically. Maybe this is connected with the fact that we think with two hands, back and forth between the right and the left one. But the two hands can only be brought to coincide with each other in the fourth dimension, by a möbius-like rotation."
      },
      "date": 1676544833601
    }
  ]
}