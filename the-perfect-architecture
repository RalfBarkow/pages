{
  "title": "The Perfect Architecture",
  "story": [
    {
      "type": "markdown",
      "id": "5269699d19b856f9",
      "text": "Kay, Alan, quoted in Cade Metz, “The Perfect Architecture.” PC Magazine, September 4, 2001, [https://web.archive.org/web/20031004030504/https://www.pcmag.com/print_article/0,3048,a=10175,00.asp archive]."
    },
    {
      "type": "markdown",
      "id": "eea24b120f243973",
      "text": "He inspired the most important innovation in the history of software, and no one knows his name. He was a programmer stationed at Denver's Randolph Air Force Base sometime in the late 1950s, writing assembly language code and feeding it on spools of magnetic tape to a Burroughs 220 vacuum tube computer.\n\nThat worked well enough. What was virtually impossible, though, was to move data from the Burroughs machine at the base to computers at other air training command installations. No operating systems or standard file formats existed, so a person couldn't e-mail a Microsoft Word document or hand an Excel file on a disk to a colleague and expect it to be readable by any machine. Raw data could be copied from the Burroughs onto tape, but other computers wouldn't be able to make sense of the jumble of ones and zeros."
    },
    {
      "type": "markdown",
      "id": "7ab598ccd4e21209",
      "text": "The anonymous programmer came up with an ingenious solution. He transformed the Burroughs data files into miniprograms, little self-sufficient modules of code. Inside each of these, he included both the data and the procedures needed to manipulate that data. He then hid data and code behind a simple interface, which let a computer understand the module without knowing anything about how the inside was formatted. In this way, a module could supply not only data to disparate machines but also a means of decoding it; all the computer needed to know was how to decipher the simple outer interface.\n\nWhen Alan Kay first used this file system as a new recruit at Randolph in 1961, he knew it was clever, but he didn't completely appreciate its importance until several years had passed, well after the programmer's name had been lost to history. \"I didn't get the big grok until '66,\" says Kay, now well known in computing circles as a cofounder of Xerox's seminal Palo Alto Research Center (PARC), where the overlapping-windows interface, Ethernet, laser printing, and the laptop were invented. Kay caught on, though, when he realized that this innovative file system could not only open up the lines of communication between machines but also improve the way entire software applications were designed.\n\nWithin six years, in Kay's hands at Xerox PARC, this idea evolved into object-oriented programming, the revolutionary computing concept that is the foundation for most of today's high-level software development languages, including C++ and Java. Mimicking the Randolph system, this new approach involved dividing a program into a series of modules, which Kay called objects, each one a little nugget of data and instructions that performs a specific task, and each fronted by a simple interface.\n\nLots of objects plugged together formed complete applications. One object never knew what went on inside another, but through the interfaces, objects collectively completed tasks they couldn't on their own. Thanks to object-oriented programming, a friend can send you an Excel spreadsheet as an e-mail attachment and your computer knows what to do with it.\n\n\n"
    },
    {
      "type": "markdown",
      "id": "85df99a093b08850",
      "text": "# The Biological Imperative\n\nKay's favorite metaphor for his programming language, which eventually was named Smalltalk, is a biological system. The most obvious parallel is the human body, which is divided into trillions of cells, each performing its own specialized task. Like objects in software produced with object-oriented programming, human cells don't know what goes on inside one another, but they can communicate nevertheless, working together to perform more complex tasks. \"This is an almost foolproof way of operating,\" Kay says.\n\nBy mimicking biology in this way, we can minimize many of the problems inherent to the construction of a complex computing system. A developer can focus on one simple module at a time, making sure it works properly, and move on to the next. Not only is building a system this way easier, but the system will be much more reliable. And when it does break down, it's simpler to fix, because problems are typically contained within individual modules, which can be repaired or replaced quickly.\n\nBy contrast, a monolithic system is like a massive mechanical clock containing innumerable turning gears, none of which has its own internal logic or communicates information. Each gear functions unintelligently and only in relation to other gears. That design is hopelessly flawed. \"When you're building gear clocks, eventually you reach a certain level of complexity and it falls in on itself,\" says Kay.\n\nThat he would view his programming language in terms of molecular sciences and not electronic engineering is apt, because that comparison in many ways sums up the remarkable development of computing in the past 60 years. Although it may not be immediately apparent, from the days of the first computer to now, developers have made greater and greater use of biological concepts.\n\nWhether the first room-filling machines, minicomputers, personal computers, PDAs, Microsoft Word, or the Internet, each generation has created ever more complex systems by piecing together ever simpler subsets.\n\n\n"
    },
    {
      "type": "markdown",
      "id": "6f57d0c5b953bbd3",
      "text": "# A 30-Ton Experiment\n\nWell before the unnamed programmer designed his file system, the same basic division-of-labor idea was used to create the 30-ton ENIAC (Electronic Numerical Integrator and Computer), arguably the world's first computer. Completed in 1945 at the University of Pennsylvania with funding from the Army's Ballistic Research Laboratory, the ENIAC included 20 different \"accumulators\" that handled addition and subtraction, with a separate module for multiplication and yet another for division and square roots.\n\nThese modules worked independently but could also be strung together, transforming output from one into input for another. \"Using the ENIAC was like operating the old telephone systems,\" says Kay Antonelli, the widow of ENIAC designer John Mauchly, and herself a mathematician who worked on the ENIAC project. \"All these units were connected using cables and a plug board.\"\n\nIt worked magnificently. In an era when most computers rarely ran for more than an hour at a time, the ENIAC, thanks to its modular design, could compute for hours on end. Originally designed to determine missile trajectories, the ENIAC was soon handling wholly unrelated calculations for mathematicians and physicists around the globe. In late 1945, Edward Teller and his Los Alamos team arrived at the university, putting the ENIAC to work on the H-bomb project.\n\n\n"
    },
    {
      "type": "markdown",
      "id": "c94890392d126c91",
      "text": "# Modular Inventions\n\nThe biological parallel made its biggest impact in 1958, when scientists at Texas Instruments and Fairchild Semiconductor Corp. invented the integrated circuit, a self-contained electronic device capable of housing dozens, hundreds, and eventually millions of electronic components. The silicon chip, which is the ultimate integrated circuit, is the basis of nearly every computer made since the late 1960s. Building a computer has become a matter of piecing chips together.\n\nJust as software programmers can use the same software objects repeatedly and even share them with other programmers, manufacturers can easily mass-produce integrated circuits and sell them to other hardware manufacturers.\n\nThe original IBM PC, completed in less than a year, was built this way. \"We bought the programmed I/O, interrupt controllers, timer chip, DMA controllers, and three different operating systems. Then we put them all together with an Intel microprocessor,\" says IBM's David Bradley, who designed the machine's BIOS, the microcode that allows for communication between hardware and software.\n\n\n"
    },
    {
      "type": "markdown",
      "id": "1d3af6ed76632281",
      "text": "After the rise of object-oriented programming, virtually every major new language adopted a modular architecture. Interaction among languages was eventually facilitated by Microsoft's OLE (Object Linking and Embedding) technology, now called COM (Component Object Model), and a competing standard known as CORBA (Common Object Request Broker Architecture). These are merely modern versions of the Randolph file system. They let disparate object-oriented systems talk to each other and transform data files into objects that can be easily passed from one machine to another.\n\nThe computer-as-a-biological-system metaphor reached a new level with the development of the network, where the concept was applied on a much larger scale. In full-scale networks, disparate hardware components performing disparate tasks are given common ways of talking to one another. To join a network, a system doesn't have to know how the others work; it just needs to know how to talk to them. Computers themselves become modules, strung together on local area networks, on wide area networks, and on the Internet.\n\nThe Internet is, at least by today's standards, the most perfect network and perhaps the most perfect parallel to a biological system. It has yet to match the 100 trillion cells that make up the human body, but the Internet now includes several hundred million computers that have gradually become more specialized: Some store data, some broadcast information, some download music and video, and some control the quality of pages, to name a few functions. And the Web is expanding relentlessly to encompass TVs, wireless phones, notebooks, and handhelds. \"Put a chip in a radio or a TV and it becomes part of this growing network,\" says Larry Roberts, who in 1965 developed the ARPAnet, the Internet's precursor. He is now chairman of router maker Caspian Networks.\n\nThe next step in extending the biological system to new technologies is distributed computing, the modularization of software applications running across the Internet. With the original object-oriented languages, all objects used in a program have to reside initially on the same machine and be written in the same language. With distributed computing, best embodied by IBM and Microsoft's Web Services initiative, you can build an application by connecting objects from machines anyplace on the Internet, without knowing anything about how they were written.\n\nTake, for example, a clothing business that wants to develop an e-tailing site. In the future, such a company will use almost nothing but modules to piece its site together. To handle acquisition of raw materials, the company will tie into a module built by a cloth seller. To handle customer payments, it will tie into a module of a credit card company. To arrange for shipping, it will tie into a module of FedEx or UPS. \"It's just about gathering pieces and putting them together,\" says Mani Chandy, a professor of computer science at the California Institute of Technology and cofounder of iSpheres, a company that develops software components for businesses. Sounds a lot like the work of that anonymous Air Force programmer."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "The Perfect Architecture",
        "story": []
      },
      "date": 1633177829137
    },
    {
      "item": {
        "type": "factory",
        "id": "5269699d19b856f9"
      },
      "id": "5269699d19b856f9",
      "type": "add",
      "date": 1633177831039
    },
    {
      "type": "edit",
      "id": "5269699d19b856f9",
      "item": {
        "type": "markdown",
        "id": "5269699d19b856f9",
        "text": "Kay, Alan, quoted in Cade Metz, “The Perfect Architecture.” PC Magazine, September 4, 2001, http://www.pcmag.com/print_article/0,3048,a=10175,00.asp."
      },
      "date": 1633177846928
    },
    {
      "item": {
        "type": "factory",
        "id": "eea24b120f243973"
      },
      "id": "eea24b120f243973",
      "type": "add",
      "after": "5269699d19b856f9",
      "date": 1633177891575
    },
    {
      "type": "edit",
      "id": "5269699d19b856f9",
      "item": {
        "type": "markdown",
        "id": "5269699d19b856f9",
        "text": "Kay, Alan, quoted in Cade Metz, “The Perfect Architecture.” PC Magazine, September 4, 2001, [https://web.archive.org/web/20031004030504/https://www.pcmag.com/print_article/0,3048,a=10175,00.asp archive]"
      },
      "date": 1633177910339
    },
    {
      "type": "edit",
      "id": "5269699d19b856f9",
      "item": {
        "type": "markdown",
        "id": "5269699d19b856f9",
        "text": "Kay, Alan, quoted in Cade Metz, “The Perfect Architecture.” PC Magazine, September 4, 2001, [https://web.archive.org/web/20031004030504/https://www.pcmag.com/print_article/0,3048,a=10175,00.asp archive]."
      },
      "date": 1633177915647
    },
    {
      "item": {
        "type": "factory",
        "id": "13f3caeba75f8c9d"
      },
      "id": "13f3caeba75f8c9d",
      "type": "add",
      "after": "eea24b120f243973",
      "date": 1633177945314
    },
    {
      "type": "remove",
      "id": "13f3caeba75f8c9d",
      "date": 1633177950921
    },
    {
      "type": "edit",
      "id": "eea24b120f243973",
      "item": {
        "type": "markdown",
        "id": "eea24b120f243973",
        "text": "He inspired the most important innovation in the history of software, and no one knows his name. He was a programmer stationed at Denver's Randolph Air Force Base sometime in the late 1950s, writing assembly language code and feeding it on spools of magnetic tape to a Burroughs 220 vacuum tube computer.\n\nThat worked well enough. What was virtually impossible, though, was to move data from the Burroughs machine at the base to computers at other air training command installations. No operating systems or standard file formats existed, so a person couldn't e-mail a Microsoft Word document or hand an Excel file on a disk to a colleague and expect it to be readable by any machine. Raw data could be copied from the Burroughs onto tape, but other computers wouldn't be able to make sense of the jumble of ones and zeros."
      },
      "date": 1633177952635
    },
    {
      "item": {
        "type": "factory",
        "id": "7ab598ccd4e21209"
      },
      "id": "7ab598ccd4e21209",
      "type": "add",
      "after": "eea24b120f243973",
      "date": 1633178017040
    },
    {
      "type": "edit",
      "id": "7ab598ccd4e21209",
      "item": {
        "type": "markdown",
        "id": "7ab598ccd4e21209",
        "text": "The anonymous programmer came up with an ingenious solution. He transformed the Burroughs data files into miniprograms, little self-sufficient modules of code. Inside each of these, he included both the data and the procedures needed to manipulate that data. He then hid data and code behind a simple interface, which let a computer understand the module without knowing anything about how the inside was formatted. In this way, a module could supply not only data to disparate machines but also a means of decoding it; all the computer needed to know was how to decipher the simple outer interface.\n\nWhen Alan Kay first used this file system as a new recruit at Randolph in 1961, he knew it was clever, but he didn't completely appreciate its importance until several years had passed, well after the programmer's name had been lost to history. \"I didn't get the big grok until '66,\" says Kay, now well known in computing circles as a cofounder of Xerox's seminal Palo Alto Research Center (PARC), where the overlapping-windows interface, Ethernet, laser printing, and the laptop were invented. Kay caught on, though, when he realized that this innovative file system could not only open up the lines of communication between machines but also improve the way entire software applications were designed.\n\nWithin six years, in Kay's hands at Xerox PARC, this idea evolved into object-oriented programming, the revolutionary computing concept that is the foundation for most of today's high-level software development languages, including C++ and Java. Mimicking the Randolph system, this new approach involved dividing a program into a series of modules, which Kay called objects, each one a little nugget of data and instructions that performs a specific task, and each fronted by a simple interface.\n\nLots of objects plugged together formed complete applications. One object never knew what went on inside another, but through the interfaces, objects collectively completed tasks they couldn't on their own. Thanks to object-oriented programming, a friend can send you an Excel spreadsheet as an e-mail attachment and your computer knows what to do with it.\n\nThe Biological Imperative\n\nKay's favorite metaphor for his programming language, which eventually was named Smalltalk, is a biological system. The most obvious parallel is the human body, which is divided into trillions of cells, each performing its own specialized task. Like objects in software produced with object-oriented programming, human cells don't know what goes on inside one another, but they can communicate nevertheless, working together to perform more complex tasks. \"This is an almost foolproof way of operating,\" Kay says.\n\nBy mimicking biology in this way, we can minimize many of the problems inherent to the construction of a complex computing system. A developer can focus on one simple module at a time, making sure it works properly, and move on to the next. Not only is building a system this way easier, but the system will be much more reliable. And when it does break down, it's simpler to fix, because problems are typically contained within individual modules, which can be repaired or replaced quickly.\n\nBy contrast, a monolithic system is like a massive mechanical clock containing innumerable turning gears, none of which has its own internal logic or communicates information. Each gear functions unintelligently and only in relation to other gears. That design is hopelessly flawed. \"When you're building gear clocks, eventually you reach a certain level of complexity and it falls in on itself,\" says Kay.\n\nThat he would view his programming language in terms of molecular sciences and not electronic engineering is apt, because that comparison in many ways sums up the remarkable development of computing in the past 60 years. Although it may not be immediately apparent, from the days of the first computer to now, developers have made greater and greater use of biological concepts.\n\nWhether the first room-filling machines, minicomputers, personal computers, PDAs, Microsoft Word, or the Internet, each generation has created ever more complex systems by piecing together ever simpler subsets.\n\nA 30-Ton Experiment\n\nWell before the unnamed programmer designed his file system, the same basic division-of-labor idea was used to create the 30-ton ENIAC (Electronic Numerical Integrator and Computer), arguably the world's first computer. Completed in 1945 at the University of Pennsylvania with funding from the Army's Ballistic Research Laboratory, the ENIAC included 20 different \"accumulators\" that handled addition and subtraction, with a separate module for multiplication and yet another for division and square roots.\n\nThese modules worked independently but could also be strung together, transforming output from one into input for another. \"Using the ENIAC was like operating the old telephone systems,\" says Kay Antonelli, the widow of ENIAC designer John Mauchly, and herself a mathematician who worked on the ENIAC project. \"All these units were connected using cables and a plug board.\"\n\nIt worked magnificently. In an era when most computers rarely ran for more than an hour at a time, the ENIAC, thanks to its modular design, could compute for hours on end. Originally designed to determine missile trajectories, the ENIAC was soon handling wholly unrelated calculations for mathematicians and physicists around the globe. In late 1945, Edward Teller and his Los Alamos team arrived at the university, putting the ENIAC to work on the H-bomb project.\n\nModular Inventions\n\nThe biological parallel made its biggest impact in 1958, when scientists at Texas Instruments and Fairchild Semiconductor Corp. invented the integrated circuit, a self-contained electronic device capable of housing dozens, hundreds, and eventually millions of electronic components. The silicon chip, which is the ultimate integrated circuit, is the basis of nearly every computer made since the late 1960s. Building a computer has become a matter of piecing chips together.\n\nJust as software programmers can use the same software objects repeatedly and even share them with other programmers, manufacturers can easily mass-produce integrated circuits and sell them to other hardware manufacturers.\n\nThe original IBM PC, completed in less than a year, was built this way. \"We bought the programmed I/O, interrupt controllers, timer chip, DMA controllers, and three different operating systems. Then we put them all together with an Intel microprocessor,\" says IBM's David Bradley, who designed the machine's BIOS, the microcode that allows for communication between hardware and software.\n\nAfter the rise of object-oriented programming, virtually every major new language adopted a modular architecture. Interaction among lan- guages was eventually facilitated by Microsoft's OLE (Object Linking and Embedding) technology, now called COM (Component Object Model), and a competing standard known as CORBA (Common Object Request Broker Architecture). These are merely modern versions of the Randolph file system. They let disparate object-oriented systems talk to each other and transform data files into objects that can be easily passed from one machine to another.\n\nThe computer-as-a-biological-system metaphor reached a new level with the development of the network, where the concept was applied on a much larger scale. In full-scale networks, disparate hardware components performing disparate tasks are given common ways of talking to one another. To join a network, a system doesn't have to know how the others work; it just needs to know how to talk to them. Computers themselves become modules, strung together on local area networks, on wide area networks, and on the Internet.\n\nThe Internet is, at least by today's standards, the most perfect network and perhaps the most perfect parallel to a biological system. It has yet to match the 100 trillion cells that make up the human body, but the Internet now includes several hundred million computers that have gradually become more specialized: Some store data, some broadcast information, some download music and video, and some control the quality of pages, to name a few functions. And the Web is expanding relentlessly to encompass TVs, wireless phones, notebooks, and handhelds. \"Put a chip in a radio or a TV and it becomes part of this growing network,\" says Larry Roberts, who in 1965 developed the ARPAnet, the Internet's precursor. He is now chairman of router maker Caspian Networks.\n\nThe next step in extending the biological system to new technologies is distributed computing, the modularization of software applications running across the Internet. With the original object-oriented languages, all objects used in a program have to reside initially on the same machine and be written in the same language. With distributed computing, best embodied by IBM and Microsoft's Web Services initiative, you can build an application by connecting objects from machines anyplace on the Internet, without knowing anything about how they were written.\n\nTake, for example, a clothing business that wants to develop an e-tailing site. In the future, such a company will use almost nothing but modules to piece its site together. To handle acquisition of raw materials, the company will tie into a module built by a cloth seller. To handle customer payments, it will tie into a module of a credit card company. To arrange for shipping, it will tie into a module of FedEx or UPS. \"It's just about gathering pieces and putting them together,\" says Mani Chandy, a professor of computer science at the California Institute of Technology and cofounder of iSpheres, a company that develops software components for businesses. Sounds a lot like the work of that anonymous Air Force programmer."
      },
      "date": 1633178019878
    },
    {
      "type": "edit",
      "id": "7ab598ccd4e21209",
      "item": {
        "type": "markdown",
        "id": "7ab598ccd4e21209",
        "text": "The anonymous programmer came up with an ingenious solution. He transformed the Burroughs data files into miniprograms, little self-sufficient modules of code. Inside each of these, he included both the data and the procedures needed to manipulate that data. He then hid data and code behind a simple interface, which let a computer understand the module without knowing anything about how the inside was formatted. In this way, a module could supply not only data to disparate machines but also a means of decoding it; all the computer needed to know was how to decipher the simple outer interface.\n\nWhen Alan Kay first used this file system as a new recruit at Randolph in 1961, he knew it was clever, but he didn't completely appreciate its importance until several years had passed, well after the programmer's name had been lost to history. \"I didn't get the big grok until '66,\" says Kay, now well known in computing circles as a cofounder of Xerox's seminal Palo Alto Research Center (PARC), where the overlapping-windows interface, Ethernet, laser printing, and the laptop were invented. Kay caught on, though, when he realized that this innovative file system could not only open up the lines of communication between machines but also improve the way entire software applications were designed.\n\nWithin six years, in Kay's hands at Xerox PARC, this idea evolved into object-oriented programming, the revolutionary computing concept that is the foundation for most of today's high-level software development languages, including C++ and Java. Mimicking the Randolph system, this new approach involved dividing a program into a series of modules, which Kay called objects, each one a little nugget of data and instructions that performs a specific task, and each fronted by a simple interface.\n\nLots of objects plugged together formed complete applications. One object never knew what went on inside another, but through the interfaces, objects collectively completed tasks they couldn't on their own. Thanks to object-oriented programming, a friend can send you an Excel spreadsheet as an e-mail attachment and your computer knows what to do with it.\n\n\n"
      },
      "date": 1633178137072
    },
    {
      "type": "add",
      "id": "85df99a093b08850",
      "item": {
        "type": "markdown",
        "id": "85df99a093b08850",
        "text": "# The Biological Imperative\n\nKay's favorite metaphor for his programming language, which eventually was named Smalltalk, is a biological system. The most obvious parallel is the human body, which is divided into trillions of cells, each performing its own specialized task. Like objects in software produced with object-oriented programming, human cells don't know what goes on inside one another, but they can communicate nevertheless, working together to perform more complex tasks. \"This is an almost foolproof way of operating,\" Kay says.\n\nBy mimicking biology in this way, we can minimize many of the problems inherent to the construction of a complex computing system. A developer can focus on one simple module at a time, making sure it works properly, and move on to the next. Not only is building a system this way easier, but the system will be much more reliable. And when it does break down, it's simpler to fix, because problems are typically contained within individual modules, which can be repaired or replaced quickly.\n\nBy contrast, a monolithic system is like a massive mechanical clock containing innumerable turning gears, none of which has its own internal logic or communicates information. Each gear functions unintelligently and only in relation to other gears. That design is hopelessly flawed. \"When you're building gear clocks, eventually you reach a certain level of complexity and it falls in on itself,\" says Kay.\n\nThat he would view his programming language in terms of molecular sciences and not electronic engineering is apt, because that comparison in many ways sums up the remarkable development of computing in the past 60 years. Although it may not be immediately apparent, from the days of the first computer to now, developers have made greater and greater use of biological concepts.\n\nWhether the first room-filling machines, minicomputers, personal computers, PDAs, Microsoft Word, or the Internet, each generation has created ever more complex systems by piecing together ever simpler subsets.\n\nA 30-Ton Experiment\n\nWell before the unnamed programmer designed his file system, the same basic division-of-labor idea was used to create the 30-ton ENIAC (Electronic Numerical Integrator and Computer), arguably the world's first computer. Completed in 1945 at the University of Pennsylvania with funding from the Army's Ballistic Research Laboratory, the ENIAC included 20 different \"accumulators\" that handled addition and subtraction, with a separate module for multiplication and yet another for division and square roots.\n\nThese modules worked independently but could also be strung together, transforming output from one into input for another. \"Using the ENIAC was like operating the old telephone systems,\" says Kay Antonelli, the widow of ENIAC designer John Mauchly, and herself a mathematician who worked on the ENIAC project. \"All these units were connected using cables and a plug board.\"\n\nIt worked magnificently. In an era when most computers rarely ran for more than an hour at a time, the ENIAC, thanks to its modular design, could compute for hours on end. Originally designed to determine missile trajectories, the ENIAC was soon handling wholly unrelated calculations for mathematicians and physicists around the globe. In late 1945, Edward Teller and his Los Alamos team arrived at the university, putting the ENIAC to work on the H-bomb project.\n\nModular Inventions\n\nThe biological parallel made its biggest impact in 1958, when scientists at Texas Instruments and Fairchild Semiconductor Corp. invented the integrated circuit, a self-contained electronic device capable of housing dozens, hundreds, and eventually millions of electronic components. The silicon chip, which is the ultimate integrated circuit, is the basis of nearly every computer made since the late 1960s. Building a computer has become a matter of piecing chips together.\n\nJust as software programmers can use the same software objects repeatedly and even share them with other programmers, manufacturers can easily mass-produce integrated circuits and sell them to other hardware manufacturers.\n\nThe original IBM PC, completed in less than a year, was built this way. \"We bought the programmed I/O, interrupt controllers, timer chip, DMA controllers, and three different operating systems. Then we put them all together with an Intel microprocessor,\" says IBM's David Bradley, who designed the machine's BIOS, the microcode that allows for communication between hardware and software.\n\nAfter the rise of object-oriented programming, virtually every major new language adopted a modular architecture. Interaction among lan- guages was eventually facilitated by Microsoft's OLE (Object Linking and Embedding) technology, now called COM (Component Object Model), and a competing standard known as CORBA (Common Object Request Broker Architecture). These are merely modern versions of the Randolph file system. They let disparate object-oriented systems talk to each other and transform data files into objects that can be easily passed from one machine to another.\n\nThe computer-as-a-biological-system metaphor reached a new level with the development of the network, where the concept was applied on a much larger scale. In full-scale networks, disparate hardware components performing disparate tasks are given common ways of talking to one another. To join a network, a system doesn't have to know how the others work; it just needs to know how to talk to them. Computers themselves become modules, strung together on local area networks, on wide area networks, and on the Internet.\n\nThe Internet is, at least by today's standards, the most perfect network and perhaps the most perfect parallel to a biological system. It has yet to match the 100 trillion cells that make up the human body, but the Internet now includes several hundred million computers that have gradually become more specialized: Some store data, some broadcast information, some download music and video, and some control the quality of pages, to name a few functions. And the Web is expanding relentlessly to encompass TVs, wireless phones, notebooks, and handhelds. \"Put a chip in a radio or a TV and it becomes part of this growing network,\" says Larry Roberts, who in 1965 developed the ARPAnet, the Internet's precursor. He is now chairman of router maker Caspian Networks.\n\nThe next step in extending the biological system to new technologies is distributed computing, the modularization of software applications running across the Internet. With the original object-oriented languages, all objects used in a program have to reside initially on the same machine and be written in the same language. With distributed computing, best embodied by IBM and Microsoft's Web Services initiative, you can build an application by connecting objects from machines anyplace on the Internet, without knowing anything about how they were written.\n\nTake, for example, a clothing business that wants to develop an e-tailing site. In the future, such a company will use almost nothing but modules to piece its site together. To handle acquisition of raw materials, the company will tie into a module built by a cloth seller. To handle customer payments, it will tie into a module of a credit card company. To arrange for shipping, it will tie into a module of FedEx or UPS. \"It's just about gathering pieces and putting them together,\" says Mani Chandy, a professor of computer science at the California Institute of Technology and cofounder of iSpheres, a company that develops software components for businesses. Sounds a lot like the work of that anonymous Air Force programmer."
      },
      "after": "7ab598ccd4e21209",
      "date": 1633178140295
    },
    {
      "type": "edit",
      "id": "85df99a093b08850",
      "item": {
        "type": "markdown",
        "id": "85df99a093b08850",
        "text": "# The Biological Imperative\n\nKay's favorite metaphor for his programming language, which eventually was named Smalltalk, is a biological system. The most obvious parallel is the human body, which is divided into trillions of cells, each performing its own specialized task. Like objects in software produced with object-oriented programming, human cells don't know what goes on inside one another, but they can communicate nevertheless, working together to perform more complex tasks. \"This is an almost foolproof way of operating,\" Kay says.\n\nBy mimicking biology in this way, we can minimize many of the problems inherent to the construction of a complex computing system. A developer can focus on one simple module at a time, making sure it works properly, and move on to the next. Not only is building a system this way easier, but the system will be much more reliable. And when it does break down, it's simpler to fix, because problems are typically contained within individual modules, which can be repaired or replaced quickly.\n\nBy contrast, a monolithic system is like a massive mechanical clock containing innumerable turning gears, none of which has its own internal logic or communicates information. Each gear functions unintelligently and only in relation to other gears. That design is hopelessly flawed. \"When you're building gear clocks, eventually you reach a certain level of complexity and it falls in on itself,\" says Kay.\n\nThat he would view his programming language in terms of molecular sciences and not electronic engineering is apt, because that comparison in many ways sums up the remarkable development of computing in the past 60 years. Although it may not be immediately apparent, from the days of the first computer to now, developers have made greater and greater use of biological concepts.\n\nWhether the first room-filling machines, minicomputers, personal computers, PDAs, Microsoft Word, or the Internet, each generation has created ever more complex systems by piecing together ever simpler subsets.\n\n\n"
      },
      "date": 1633178184407
    },
    {
      "type": "add",
      "id": "6f57d0c5b953bbd3",
      "item": {
        "type": "markdown",
        "id": "6f57d0c5b953bbd3",
        "text": "# A 30-Ton Experiment\n\nWell before the unnamed programmer designed his file system, the same basic division-of-labor idea was used to create the 30-ton ENIAC (Electronic Numerical Integrator and Computer), arguably the world's first computer. Completed in 1945 at the University of Pennsylvania with funding from the Army's Ballistic Research Laboratory, the ENIAC included 20 different \"accumulators\" that handled addition and subtraction, with a separate module for multiplication and yet another for division and square roots.\n\nThese modules worked independently but could also be strung together, transforming output from one into input for another. \"Using the ENIAC was like operating the old telephone systems,\" says Kay Antonelli, the widow of ENIAC designer John Mauchly, and herself a mathematician who worked on the ENIAC project. \"All these units were connected using cables and a plug board.\"\n\nIt worked magnificently. In an era when most computers rarely ran for more than an hour at a time, the ENIAC, thanks to its modular design, could compute for hours on end. Originally designed to determine missile trajectories, the ENIAC was soon handling wholly unrelated calculations for mathematicians and physicists around the globe. In late 1945, Edward Teller and his Los Alamos team arrived at the university, putting the ENIAC to work on the H-bomb project.\n\nModular Inventions\n\nThe biological parallel made its biggest impact in 1958, when scientists at Texas Instruments and Fairchild Semiconductor Corp. invented the integrated circuit, a self-contained electronic device capable of housing dozens, hundreds, and eventually millions of electronic components. The silicon chip, which is the ultimate integrated circuit, is the basis of nearly every computer made since the late 1960s. Building a computer has become a matter of piecing chips together.\n\nJust as software programmers can use the same software objects repeatedly and even share them with other programmers, manufacturers can easily mass-produce integrated circuits and sell them to other hardware manufacturers.\n\nThe original IBM PC, completed in less than a year, was built this way. \"We bought the programmed I/O, interrupt controllers, timer chip, DMA controllers, and three different operating systems. Then we put them all together with an Intel microprocessor,\" says IBM's David Bradley, who designed the machine's BIOS, the microcode that allows for communication between hardware and software.\n\nAfter the rise of object-oriented programming, virtually every major new language adopted a modular architecture. Interaction among lan- guages was eventually facilitated by Microsoft's OLE (Object Linking and Embedding) technology, now called COM (Component Object Model), and a competing standard known as CORBA (Common Object Request Broker Architecture). These are merely modern versions of the Randolph file system. They let disparate object-oriented systems talk to each other and transform data files into objects that can be easily passed from one machine to another.\n\nThe computer-as-a-biological-system metaphor reached a new level with the development of the network, where the concept was applied on a much larger scale. In full-scale networks, disparate hardware components performing disparate tasks are given common ways of talking to one another. To join a network, a system doesn't have to know how the others work; it just needs to know how to talk to them. Computers themselves become modules, strung together on local area networks, on wide area networks, and on the Internet.\n\nThe Internet is, at least by today's standards, the most perfect network and perhaps the most perfect parallel to a biological system. It has yet to match the 100 trillion cells that make up the human body, but the Internet now includes several hundred million computers that have gradually become more specialized: Some store data, some broadcast information, some download music and video, and some control the quality of pages, to name a few functions. And the Web is expanding relentlessly to encompass TVs, wireless phones, notebooks, and handhelds. \"Put a chip in a radio or a TV and it becomes part of this growing network,\" says Larry Roberts, who in 1965 developed the ARPAnet, the Internet's precursor. He is now chairman of router maker Caspian Networks.\n\nThe next step in extending the biological system to new technologies is distributed computing, the modularization of software applications running across the Internet. With the original object-oriented languages, all objects used in a program have to reside initially on the same machine and be written in the same language. With distributed computing, best embodied by IBM and Microsoft's Web Services initiative, you can build an application by connecting objects from machines anyplace on the Internet, without knowing anything about how they were written.\n\nTake, for example, a clothing business that wants to develop an e-tailing site. In the future, such a company will use almost nothing but modules to piece its site together. To handle acquisition of raw materials, the company will tie into a module built by a cloth seller. To handle customer payments, it will tie into a module of a credit card company. To arrange for shipping, it will tie into a module of FedEx or UPS. \"It's just about gathering pieces and putting them together,\" says Mani Chandy, a professor of computer science at the California Institute of Technology and cofounder of iSpheres, a company that develops software components for businesses. Sounds a lot like the work of that anonymous Air Force programmer."
      },
      "after": "85df99a093b08850",
      "date": 1633178187273
    },
    {
      "type": "edit",
      "id": "6f57d0c5b953bbd3",
      "item": {
        "type": "markdown",
        "id": "6f57d0c5b953bbd3",
        "text": "# A 30-Ton Experiment\n\nWell before the unnamed programmer designed his file system, the same basic division-of-labor idea was used to create the 30-ton ENIAC (Electronic Numerical Integrator and Computer), arguably the world's first computer. Completed in 1945 at the University of Pennsylvania with funding from the Army's Ballistic Research Laboratory, the ENIAC included 20 different \"accumulators\" that handled addition and subtraction, with a separate module for multiplication and yet another for division and square roots.\n\nThese modules worked independently but could also be strung together, transforming output from one into input for another. \"Using the ENIAC was like operating the old telephone systems,\" says Kay Antonelli, the widow of ENIAC designer John Mauchly, and herself a mathematician who worked on the ENIAC project. \"All these units were connected using cables and a plug board.\"\n\nIt worked magnificently. In an era when most computers rarely ran for more than an hour at a time, the ENIAC, thanks to its modular design, could compute for hours on end. Originally designed to determine missile trajectories, the ENIAC was soon handling wholly unrelated calculations for mathematicians and physicists around the globe. In late 1945, Edward Teller and his Los Alamos team arrived at the university, putting the ENIAC to work on the H-bomb project.\n\n\n"
      },
      "date": 1633178248275
    },
    {
      "type": "add",
      "id": "c94890392d126c91",
      "item": {
        "type": "markdown",
        "id": "c94890392d126c91",
        "text": "# Modular Inventions\n\nThe biological parallel made its biggest impact in 1958, when scientists at Texas Instruments and Fairchild Semiconductor Corp. invented the integrated circuit, a self-contained electronic device capable of housing dozens, hundreds, and eventually millions of electronic components. The silicon chip, which is the ultimate integrated circuit, is the basis of nearly every computer made since the late 1960s. Building a computer has become a matter of piecing chips together.\n\nJust as software programmers can use the same software objects repeatedly and even share them with other programmers, manufacturers can easily mass-produce integrated circuits and sell them to other hardware manufacturers.\n\nThe original IBM PC, completed in less than a year, was built this way. \"We bought the programmed I/O, interrupt controllers, timer chip, DMA controllers, and three different operating systems. Then we put them all together with an Intel microprocessor,\" says IBM's David Bradley, who designed the machine's BIOS, the microcode that allows for communication between hardware and software.\n\nAfter the rise of object-oriented programming, virtually every major new language adopted a modular architecture. Interaction among lan- guages was eventually facilitated by Microsoft's OLE (Object Linking and Embedding) technology, now called COM (Component Object Model), and a competing standard known as CORBA (Common Object Request Broker Architecture). These are merely modern versions of the Randolph file system. They let disparate object-oriented systems talk to each other and transform data files into objects that can be easily passed from one machine to another.\n\nThe computer-as-a-biological-system metaphor reached a new level with the development of the network, where the concept was applied on a much larger scale. In full-scale networks, disparate hardware components performing disparate tasks are given common ways of talking to one another. To join a network, a system doesn't have to know how the others work; it just needs to know how to talk to them. Computers themselves become modules, strung together on local area networks, on wide area networks, and on the Internet.\n\nThe Internet is, at least by today's standards, the most perfect network and perhaps the most perfect parallel to a biological system. It has yet to match the 100 trillion cells that make up the human body, but the Internet now includes several hundred million computers that have gradually become more specialized: Some store data, some broadcast information, some download music and video, and some control the quality of pages, to name a few functions. And the Web is expanding relentlessly to encompass TVs, wireless phones, notebooks, and handhelds. \"Put a chip in a radio or a TV and it becomes part of this growing network,\" says Larry Roberts, who in 1965 developed the ARPAnet, the Internet's precursor. He is now chairman of router maker Caspian Networks.\n\nThe next step in extending the biological system to new technologies is distributed computing, the modularization of software applications running across the Internet. With the original object-oriented languages, all objects used in a program have to reside initially on the same machine and be written in the same language. With distributed computing, best embodied by IBM and Microsoft's Web Services initiative, you can build an application by connecting objects from machines anyplace on the Internet, without knowing anything about how they were written.\n\nTake, for example, a clothing business that wants to develop an e-tailing site. In the future, such a company will use almost nothing but modules to piece its site together. To handle acquisition of raw materials, the company will tie into a module built by a cloth seller. To handle customer payments, it will tie into a module of a credit card company. To arrange for shipping, it will tie into a module of FedEx or UPS. \"It's just about gathering pieces and putting them together,\" says Mani Chandy, a professor of computer science at the California Institute of Technology and cofounder of iSpheres, a company that develops software components for businesses. Sounds a lot like the work of that anonymous Air Force programmer."
      },
      "after": "6f57d0c5b953bbd3",
      "date": 1633178250829
    },
    {
      "type": "edit",
      "id": "c94890392d126c91",
      "item": {
        "type": "markdown",
        "id": "c94890392d126c91",
        "text": "# Modular Inventions\n\nThe biological parallel made its biggest impact in 1958, when scientists at Texas Instruments and Fairchild Semiconductor Corp. invented the integrated circuit, a self-contained electronic device capable of housing dozens, hundreds, and eventually millions of electronic components. The silicon chip, which is the ultimate integrated circuit, is the basis of nearly every computer made since the late 1960s. Building a computer has become a matter of piecing chips together.\n\nJust as software programmers can use the same software objects repeatedly and even share them with other programmers, manufacturers can easily mass-produce integrated circuits and sell them to other hardware manufacturers.\n\nThe original IBM PC, completed in less than a year, was built this way. \"We bought the programmed I/O, interrupt controllers, timer chip, DMA controllers, and three different operating systems. Then we put them all together with an Intel microprocessor,\" says IBM's David Bradley, who designed the machine's BIOS, the microcode that allows for communication between hardware and software.\n\n\n"
      },
      "date": 1633178318043
    },
    {
      "type": "add",
      "id": "1d3af6ed76632281",
      "item": {
        "type": "markdown",
        "id": "1d3af6ed76632281",
        "text": "After the rise of object-oriented programming, virtually every major new language adopted a modular architecture. Interaction among languages was eventually facilitated by Microsoft's OLE (Object Linking and Embedding) technology, now called COM (Component Object Model), and a competing standard known as CORBA (Common Object Request Broker Architecture). These are merely modern versions of the Randolph file system. They let disparate object-oriented systems talk to each other and transform data files into objects that can be easily passed from one machine to another.\n\nThe computer-as-a-biological-system metaphor reached a new level with the development of the network, where the concept was applied on a much larger scale. In full-scale networks, disparate hardware components performing disparate tasks are given common ways of talking to one another. To join a network, a system doesn't have to know how the others work; it just needs to know how to talk to them. Computers themselves become modules, strung together on local area networks, on wide area networks, and on the Internet.\n\nThe Internet is, at least by today's standards, the most perfect network and perhaps the most perfect parallel to a biological system. It has yet to match the 100 trillion cells that make up the human body, but the Internet now includes several hundred million computers that have gradually become more specialized: Some store data, some broadcast information, some download music and video, and some control the quality of pages, to name a few functions. And the Web is expanding relentlessly to encompass TVs, wireless phones, notebooks, and handhelds. \"Put a chip in a radio or a TV and it becomes part of this growing network,\" says Larry Roberts, who in 1965 developed the ARPAnet, the Internet's precursor. He is now chairman of router maker Caspian Networks.\n\nThe next step in extending the biological system to new technologies is distributed computing, the modularization of software applications running across the Internet. With the original object-oriented languages, all objects used in a program have to reside initially on the same machine and be written in the same language. With distributed computing, best embodied by IBM and Microsoft's Web Services initiative, you can build an application by connecting objects from machines anyplace on the Internet, without knowing anything about how they were written.\n\nTake, for example, a clothing business that wants to develop an e-tailing site. In the future, such a company will use almost nothing but modules to piece its site together. To handle acquisition of raw materials, the company will tie into a module built by a cloth seller. To handle customer payments, it will tie into a module of a credit card company. To arrange for shipping, it will tie into a module of FedEx or UPS. \"It's just about gathering pieces and putting them together,\" says Mani Chandy, a professor of computer science at the California Institute of Technology and cofounder of iSpheres, a company that develops software components for businesses. Sounds a lot like the work of that anonymous Air Force programmer."
      },
      "after": "c94890392d126c91",
      "date": 1633178323268
    }
  ]
}