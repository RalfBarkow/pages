{
  "title": "Virtual Reality Interface",
  "story": [
    {
      "type": "paragraph",
      "id": "bc56fe296368e1f7",
      "text": "The motivation to develop a tool is given by its interface and the purpose of facilitating the interaction between a user programmer [⇒ [[User-Programmer Unity]]] and a [[Manipulator]] robot in the task of creating trajectories that will be executed by the robot, one of the aids to implement for the elaboration of this project, 3D models are applied in virtual reality technology that has been welcomed by many sectors, among them the academic and the industrial. "
    },
    {
      "type": "paragraph",
      "id": "c197d5bb4e4a1d58",
      "text": "An example of this is the developed application, a 3D virtual environment for robotics and the purpose of the project to evaluate robotic and virtual reality techniques, implementing a simulator in robotic education"
    },
    {
      "type": "pagefold",
      "id": "90c685b402a4c6e5",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "f8d7642eebb1c602",
      "text": "RODRÍGUEZ HOYOS, Daniel Santiago, TUMIALÁN BORJA, José Antonio and VELASCO PEÑA, Hugo Fernando, 2021. Virtual Reality Interface for Assist in Programming of Tasks of a Robotic Manipulator. In: CORTES TOBAR, Dario Fernando, HOANG DUY, Vo and TRONG DAO, Tran (eds.), AETA 2019 - Recent Advances in Electrical Engineering and Related Sciences: Theory and Application. Cham: Springer International Publishing. 2021. p. 328–335. Lecture Notes in Electrical Engineering. ISBN 978-3-030-53021-1. DOI 10.1007/978-3-030-53021-1_33. "
    },
    {
      "type": "paragraph",
      "id": "0a7fa8eac820fa8e",
      "text": "The creation of interfaces that contribute to the interaction between a programmer and an industrial robot are based on implementing adequate procedures that facilitate people without knowledge of robotics and programming of robotic manipulators to generate tasks or activities that the robotic arm executes. In this article, it describes the creation of a programming interface using virtual reality, as an immersion tool in the work environment of the yaskawa motoman HP20D robot. This virtual environment was developed from 3D models, later exported to the Unity 3D video game engine, in this interface the functions are programmed that allow the user to interact with the robot according to the movements and trajectories performed by the user in the work environment. "
    },
    {
      "type": "paragraph",
      "id": "d94d5186d695c511",
      "text": "A [[MYO Armband]] sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses [⇒ Focals [[Smart Glass]]es by North [https://www.wired.com/review/focals-by-north-smart-glasses/ wired] ] are used to visualize the entire space, with a first-person perspective. "
    },
    {
      "type": "pagefold",
      "id": "4bb1a3b63350bf4f",
      "text": "~"
    },
    {
      "type": "video",
      "id": "1da9c8d0f31b122d",
      "text": "YOUTUBE jOEcsNmTk7g\nThe Myo Armband: The Future of Gesture Control"
    },
    {
      "type": "paragraph",
      "id": "2f440413db90837c",
      "text": "The Myo wristband from the US company Thalmic Labs came onto the market in 2015 as the result of a crowdfunding project. Touted as revolutionary at the time, the device was intended to enable natural and everyday gesture control for the first time. The core element is the electromyographic sensors, which are supplemented by normal motion sensor technology. At least 50,000 units of the device were sold, but we do not have exact figures. [https://kinemic.com/de/band/kinemic-band-im-vergleich/thalmic-labs-myo-kinemic-band/ kinemic] (de), [https://wearabletech.io/myo-bracelet/ wearabletech], [https://kinemic.com/en/ kinemic]"
    },
    {
      "type": "paragraph",
      "id": "66d9e959c3738b6a",
      "text": "After successful investment rounds, the company focused on the development of a [[Smart Glass]], which was launched in 2018 under the name Focals by North. [https://www.wired.com/review/focals-by-north-smart-glasses/ wired]"
    },
    {
      "type": "pagefold",
      "id": "27e32c5bb0d04792",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "c752c15e3b1599d0",
      "text": "As a result, a computational tool is obtained that allows generating trajectories, recording them, simulating them and generating the script or programming code to be implement in the robotic manipulator controller.\n"
    },
    {
      "type": "paragraph",
      "id": "9e4df6c6567b3c2f",
      "text": "[…] "
    },
    {
      "type": "image",
      "id": "802d3758ccef2b55",
      "text": "Gestures recognized by MYO Armband",
      "size": "wide",
      "width": 416,
      "height": 107,
      "url": "/assets/plugins/image/4f866495ac2b6e5a4a2c7169d81437e0.jpg"
    },
    {
      "type": "paragraph",
      "id": "79ccb921036bf521",
      "text": "The gestures are used as interface functionality, giving a specific action to 2 of these gestures."
    },
    {
      "type": "markdown",
      "id": "f56c4b80f471e965",
      "text": "* Make a fist: Activation of movement control on the end effector of the robot. \n* Wave Right: Registration and storage of the current point of the final effector."
    },
    {
      "type": "paragraph",
      "id": "04263cb14ab3cde8",
      "text": "Once the movement control is activated, it is necessary to obtain the reference for the movement of the virtual arm which the user observes in order to relate the movement of the user’s arm to that of the robot, giving the user control over the final effector. To solve the control of the target point, the [[Compass]] of the device is used, which transmits the relative rotation information of the three axes of the MYO Armband and will be used to control the rotation of the forearm point of the 3D model of the arm (See Fig. 3)."
    },
    {
      "type": "image",
      "id": "6620e52cc27071a7",
      "text": "Fig. 3. Dynamic of virtual upper extremity",
      "size": "wide",
      "width": 416,
      "height": 342,
      "url": "/assets/plugins/image/8c44c386b09ce8778b6ece6c652aff44.jpg"
    },
    {
      "type": "paragraph",
      "id": "15c0a14bca86aaca",
      "text": "The orientation of the MYO Armband applied to the forearm is obtained through a programming script."
    },
    {
      "type": "paragraph",
      "id": "5226df25b25de0f9",
      "text": "⇒ [[Implementation of Inverse Kinematics]]"
    },
    {
      "type": "pagefold",
      "id": "a657fb7bd064b775",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "6d3947ce8516b46c",
      "text": "⇒ [[Trajectories]] ⇒ [[Interaction]] ⇒ [[Interaction (Interaktion)]] ⇒ [[Path]] ⇒ [[Animation with Captured Gestures]]"
    },
    {
      "type": "graphviz",
      "id": "5c2f794fc29ba96d",
      "text": "DOT FROM lambda-browsing"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Virtual Reality Interface",
        "story": []
      },
      "date": 1676566549177
    },
    {
      "item": {
        "type": "factory",
        "id": "bc56fe296368e1f7"
      },
      "id": "bc56fe296368e1f7",
      "type": "add",
      "date": 1676566559462
    },
    {
      "type": "edit",
      "id": "bc56fe296368e1f7",
      "item": {
        "type": "paragraph",
        "id": "bc56fe296368e1f7",
        "text": "The motivation to develop a tool is given by its interface and the purpose of facilitating the interaction between a user programmer and a manipulator robot in the task of creating trajectories that will be executed by the robot, one of the aids to implement for the elaboration of this project, 3D models are applied in virtual reality technology that has been welcomed by many sectors, among them the academic and the industrial. An example of this is the developed application, a 3D virtual environment for robotics and the purpose of the project to evaluate robotic and virtual reality techniques, implementing a simulator in robotic education"
      },
      "date": 1676566561132
    },
    {
      "type": "edit",
      "id": "bc56fe296368e1f7",
      "item": {
        "type": "paragraph",
        "id": "bc56fe296368e1f7",
        "text": "The motivation to develop a tool is given by its interface and the purpose of facilitating the interaction between a user programmer [⇒ [[User-Programmer Unity]] and a manipulator robot in the task of creating trajectories that will be executed by the robot, one of the aids to implement for the elaboration of this project, 3D models are applied in virtual reality technology that has been welcomed by many sectors, among them the academic and the industrial. An example of this is the developed application, a 3D virtual environment for robotics and the purpose of the project to evaluate robotic and virtual reality techniques, implementing a simulator in robotic education"
      },
      "date": 1676566615995
    },
    {
      "item": {
        "type": "factory",
        "id": "90c685b402a4c6e5"
      },
      "id": "90c685b402a4c6e5",
      "type": "add",
      "after": "bc56fe296368e1f7",
      "date": 1676566633948
    },
    {
      "type": "edit",
      "id": "90c685b402a4c6e5",
      "item": {
        "type": "pagefold",
        "id": "90c685b402a4c6e5",
        "text": "~"
      },
      "date": 1676566638949
    },
    {
      "item": {
        "type": "factory",
        "id": "f8d7642eebb1c602"
      },
      "id": "f8d7642eebb1c602",
      "type": "add",
      "after": "90c685b402a4c6e5",
      "date": 1676566640975
    },
    {
      "type": "edit",
      "id": "f8d7642eebb1c602",
      "item": {
        "type": "paragraph",
        "id": "f8d7642eebb1c602",
        "text": "\nRODRÍGUEZ HOYOS, Daniel Santiago, TUMIALÁN BORJA, José Antonio and VELASCO PEÑA, Hugo Fernando, 2021. Virtual Reality Interface for Assist in Programming of Tasks of a Robotic Manipulator. In: CORTES TOBAR, Dario Fernando, HOANG DUY, Vo and TRONG DAO, Tran (eds.), AETA 2019 - Recent Advances in Electrical Engineering and Related Sciences: Theory and Application. Cham: Springer International Publishing. 2021. p. 328–335. Lecture Notes in Electrical Engineering. ISBN 978-3-030-53021-1. DOI 10.1007/978-3-030-53021-1_33. "
      },
      "date": 1676566654006
    },
    {
      "type": "add",
      "id": "0a7fa8eac820fa8e",
      "item": {
        "type": "paragraph",
        "id": "0a7fa8eac820fa8e",
        "text": "The creation of interfaces that contribute to the interaction between a programmer and an industrial robot are based on implementing adequate procedures that facilitate people without knowledge of robotics and programming of robotic manipulators to generate tasks or activities that the robotic arm executes. In this article, it describes the creation of a programming interface using virtual reality, as an immersion tool in the work environment of the yaskawa motoman HP20D robot. This virtual environment was developed from 3D models, later exported to the Unity 3D video game engine, in this interface the functions are programmed that allow the user to interact with the robot according to the movements and trajectories performed by the user in the work environment. A MYO Armband sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses are used to visualize the entire space, with a first-person perspective. As a result, a computational tool is obtained that allows generating trajectories, recording them, simulating them and generating the script or programming code to be implement in the robotic manipulator controller.\n"
      },
      "after": "f8d7642eebb1c602",
      "date": 1676566654436
    },
    {
      "type": "edit",
      "id": "bc56fe296368e1f7",
      "item": {
        "type": "paragraph",
        "id": "bc56fe296368e1f7",
        "text": "The motivation to develop a tool is given by its interface and the purpose of facilitating the interaction between a user programmer [⇒ [[User-Programmer Unity]] and a manipulator robot in the task of creating trajectories that will be executed by the robot, one of the aids to implement for the elaboration of this project, 3D models are applied in virtual reality technology that has been welcomed by many sectors, among them the academic and the industrial. "
      },
      "date": 1676566676671
    },
    {
      "type": "add",
      "id": "c197d5bb4e4a1d58",
      "item": {
        "type": "paragraph",
        "id": "c197d5bb4e4a1d58",
        "text": "An example of this is the developed application, a 3D virtual environment for robotics and the purpose of the project to evaluate robotic and virtual reality techniques, implementing a simulator in robotic education"
      },
      "after": "bc56fe296368e1f7",
      "date": 1676566677989
    },
    {
      "item": {
        "type": "factory",
        "id": "9e4df6c6567b3c2f"
      },
      "id": "9e4df6c6567b3c2f",
      "type": "add",
      "after": "0a7fa8eac820fa8e",
      "date": 1676566778458
    },
    {
      "type": "edit",
      "id": "9e4df6c6567b3c2f",
      "item": {
        "type": "paragraph",
        "id": "9e4df6c6567b3c2f",
        "text": "[…] Fig. 2. Gestures recognized by MYO Armband"
      },
      "date": 1676566784112
    },
    {
      "item": {
        "type": "factory",
        "id": "802d3758ccef2b55"
      },
      "id": "802d3758ccef2b55",
      "type": "add",
      "after": "9e4df6c6567b3c2f",
      "date": 1676566849644
    },
    {
      "type": "edit",
      "id": "802d3758ccef2b55",
      "item": {
        "type": "image",
        "id": "802d3758ccef2b55",
        "text": "Uploaded image",
        "size": "wide",
        "width": 416,
        "height": 107,
        "url": "/assets/plugins/image/4f866495ac2b6e5a4a2c7169d81437e0.jpg"
      },
      "date": 1676566861381
    },
    {
      "type": "edit",
      "id": "802d3758ccef2b55",
      "item": {
        "type": "image",
        "id": "802d3758ccef2b55",
        "text": "Gestures recognized by MYO Armband",
        "size": "wide",
        "width": 416,
        "height": 107,
        "url": "/assets/plugins/image/4f866495ac2b6e5a4a2c7169d81437e0.jpg"
      },
      "date": 1676566890330
    },
    {
      "type": "edit",
      "id": "9e4df6c6567b3c2f",
      "item": {
        "type": "paragraph",
        "id": "9e4df6c6567b3c2f",
        "text": "[…] "
      },
      "date": 1676566900499
    },
    {
      "id": "6d3947ce8516b46c",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "6d3947ce8516b46c",
        "text": "⇒ [[Trajectories]] ⇒ [[Interaction]] ⇒ [[Interaction (Interaktion)]] ⇒ [[Path]] ⇒ [[Animation with Captured Gestures]]"
      },
      "after": "802d3758ccef2b55",
      "attribution": {
        "page": "User-Programmer Unity"
      },
      "date": 1676566925399
    },
    {
      "item": {
        "type": "factory",
        "id": "5c2f794fc29ba96d"
      },
      "id": "5c2f794fc29ba96d",
      "type": "add",
      "after": "6d3947ce8516b46c",
      "date": 1676566933445
    },
    {
      "type": "edit",
      "id": "5c2f794fc29ba96d",
      "item": {
        "type": "graphviz",
        "id": "5c2f794fc29ba96d",
        "text": "DOT FROM lambda-browsing"
      },
      "date": 1676566942359
    },
    {
      "item": {
        "type": "factory",
        "id": "a657fb7bd064b775"
      },
      "id": "a657fb7bd064b775",
      "type": "add",
      "after": "5c2f794fc29ba96d",
      "date": 1676567049453
    },
    {
      "type": "edit",
      "id": "a657fb7bd064b775",
      "item": {
        "type": "pagefold",
        "id": "a657fb7bd064b775",
        "text": "~"
      },
      "date": 1676567053754
    },
    {
      "id": "a657fb7bd064b775",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676567059823
    },
    {
      "item": {
        "type": "factory",
        "id": "79ccb921036bf521"
      },
      "id": "79ccb921036bf521",
      "type": "add",
      "after": "5c2f794fc29ba96d",
      "date": 1676567064213
    },
    {
      "id": "79ccb921036bf521",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "79ccb921036bf521",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676567066666
    },
    {
      "type": "edit",
      "id": "79ccb921036bf521",
      "item": {
        "type": "paragraph",
        "id": "79ccb921036bf521",
        "text": "The gestures are used as interface functionality, giving a specific action to 2 of these gestures."
      },
      "date": 1676567069153
    },
    {
      "type": "add",
      "id": "04263cb14ab3cde8",
      "item": {
        "type": "paragraph",
        "id": "04263cb14ab3cde8",
        "text": "[…]"
      },
      "after": "79ccb921036bf521",
      "date": 1676567150917
    },
    {
      "item": {
        "type": "factory",
        "id": "6620e52cc27071a7"
      },
      "id": "6620e52cc27071a7",
      "type": "add",
      "after": "5c2f794fc29ba96d",
      "date": 1676567155306
    },
    {
      "id": "6620e52cc27071a7",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "79ccb921036bf521",
        "04263cb14ab3cde8",
        "6620e52cc27071a7",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676567157843
    },
    {
      "type": "edit",
      "id": "6620e52cc27071a7",
      "item": {
        "type": "image",
        "id": "6620e52cc27071a7",
        "text": "Fig. 3. Dynamic of virtual upper extremity",
        "size": "wide",
        "width": 416,
        "height": 342,
        "url": "/assets/plugins/image/8c44c386b09ce8778b6ece6c652aff44.jpg"
      },
      "date": 1676567166908
    },
    {
      "type": "add",
      "id": "f56c4b80f471e965",
      "item": {
        "type": "paragraph",
        "id": "f56c4b80f471e965",
        "text": "• Make a fist: Activation of movement control on the end effector of the robot. • Wave Right: Registration and storage of the current point of the final effector."
      },
      "after": "79ccb921036bf521",
      "date": 1676567208545
    },
    {
      "type": "edit",
      "id": "f56c4b80f471e965",
      "item": {
        "type": "markdown",
        "id": "f56c4b80f471e965",
        "text": "• Make a fist: Activation of movement control on the end effector of the robot. • Wave Right: Registration and storage of the current point of the final effector."
      },
      "date": 1676567210686
    },
    {
      "type": "edit",
      "id": "f56c4b80f471e965",
      "item": {
        "type": "markdown",
        "id": "f56c4b80f471e965",
        "text": "* Make a fist: Activation of movement control on the end effector of the robot. • Wave Right: Registration and storage of the current point of the final effector."
      },
      "date": 1676567216766
    },
    {
      "type": "edit",
      "id": "f56c4b80f471e965",
      "item": {
        "type": "markdown",
        "id": "f56c4b80f471e965",
        "text": "* Make a fist: Activation of movement control on the end effector of the robot. \n* Wave Right: Registration and storage of the current point of the final effector."
      },
      "date": 1676567224307
    },
    {
      "type": "edit",
      "id": "04263cb14ab3cde8",
      "item": {
        "type": "paragraph",
        "id": "04263cb14ab3cde8",
        "text": "Once the movement control is activated, it is necessary to obtain the reference for the movement of the virtual arm which the user observes in order to relate the movement of the user’s arm to that of the robot, giving the user control over the final effector. To solve the control of the target point, the compass of the device is used, which transmits"
      },
      "date": 1676567248227
    },
    {
      "type": "add",
      "id": "c587d2c30e5d726f",
      "item": {
        "type": "paragraph",
        "id": "c587d2c30e5d726f",
        "text": "[…]"
      },
      "after": "04263cb14ab3cde8",
      "date": 1676567248908
    },
    {
      "type": "remove",
      "id": "c587d2c30e5d726f",
      "date": 1676567262741
    },
    {
      "type": "edit",
      "id": "04263cb14ab3cde8",
      "item": {
        "type": "paragraph",
        "id": "04263cb14ab3cde8",
        "text": "Once the movement control is activated, it is necessary to obtain the reference for the movement of the virtual arm which the user observes in order to relate the movement of the user’s arm to that of the robot, giving the user control over the final effector. To solve the control of the target point, the compass of the device is used, which transmitsthe relative rotation information of the three axes of the MYO Armband and will be used to control the rotation of the forearm point of the 3D model of the arm (See Fig. 3)."
      },
      "date": 1676567264645
    },
    {
      "type": "edit",
      "id": "04263cb14ab3cde8",
      "item": {
        "type": "paragraph",
        "id": "04263cb14ab3cde8",
        "text": "Once the movement control is activated, it is necessary to obtain the reference for the movement of the virtual arm which the user observes in order to relate the movement of the user’s arm to that of the robot, giving the user control over the final effector. To solve the control of the target point, the [[Compass]] of the device is used, which transmits the relative rotation information of the three axes of the MYO Armband and will be used to control the rotation of the forearm point of the 3D model of the arm (See Fig. 3)."
      },
      "date": 1676567292125
    },
    {
      "type": "add",
      "id": "15c0a14bca86aaca",
      "item": {
        "type": "paragraph",
        "id": "15c0a14bca86aaca",
        "text": "The orientation of the MYO Armband applied to the forearm is obtained through a programming script."
      },
      "after": "04263cb14ab3cde8",
      "date": 1676567384534
    },
    {
      "id": "15c0a14bca86aaca",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "79ccb921036bf521",
        "f56c4b80f471e965",
        "04263cb14ab3cde8",
        "6620e52cc27071a7",
        "15c0a14bca86aaca",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676567390280
    },
    {
      "type": "add",
      "id": "5226df25b25de0f9",
      "item": {
        "type": "paragraph",
        "id": "5226df25b25de0f9",
        "text": "⇒ [[Implementation of Inverse Kinematics]]"
      },
      "after": "15c0a14bca86aaca",
      "date": 1676567561375
    },
    {
      "type": "edit",
      "id": "0a7fa8eac820fa8e",
      "item": {
        "type": "paragraph",
        "id": "0a7fa8eac820fa8e",
        "text": "The creation of interfaces that contribute to the interaction between a programmer and an industrial robot are based on implementing adequate procedures that facilitate people without knowledge of robotics and programming of robotic manipulators to generate tasks or activities that the robotic arm executes. In this article, it describes the creation of a programming interface using virtual reality, as an immersion tool in the work environment of the yaskawa motoman HP20D robot. This virtual environment was developed from 3D models, later exported to the Unity 3D video game engine, in this interface the functions are programmed that allow the user to interact with the robot according to the movements and trajectories performed by the user in the work environment. A [[MYO Armband]] sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses are used to visualize the entire space, with a first-person perspective. As a result, a computational tool is obtained that allows generating trajectories, recording them, simulating them and generating the script or programming code to be implement in the robotic manipulator controller.\n"
      },
      "date": 1676568619898
    },
    {
      "type": "edit",
      "id": "0a7fa8eac820fa8e",
      "item": {
        "type": "paragraph",
        "id": "0a7fa8eac820fa8e",
        "text": "The creation of interfaces that contribute to the interaction between a programmer and an industrial robot are based on implementing adequate procedures that facilitate people without knowledge of robotics and programming of robotic manipulators to generate tasks or activities that the robotic arm executes. In this article, it describes the creation of a programming interface using virtual reality, as an immersion tool in the work environment of the yaskawa motoman HP20D robot. This virtual environment was developed from 3D models, later exported to the Unity 3D video game engine, in this interface the functions are programmed that allow the user to interact with the robot according to the movements and trajectories performed by the user in the work environment. "
      },
      "date": 1676569114667
    },
    {
      "type": "add",
      "id": "d94d5186d695c511",
      "item": {
        "type": "paragraph",
        "id": "d94d5186d695c511",
        "text": "A [[MYO Armband]] sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses are used to visualize the entire space, with a first-person perspective. As a result, a computational tool is obtained that allows generating trajectories, recording them, simulating them and generating the script or programming code to be implement in the robotic manipulator controller.\n"
      },
      "after": "0a7fa8eac820fa8e",
      "date": 1676569116674
    },
    {
      "id": "1da9c8d0f31b122d",
      "type": "add",
      "item": {
        "type": "video",
        "id": "1da9c8d0f31b122d",
        "text": "YOUTUBE jOEcsNmTk7g\nThe Myo Armband: The Future of Gesture Control"
      },
      "after": "0a7fa8eac820fa8e",
      "attribution": {
        "page": "MYO Armband"
      },
      "date": 1676569127903
    },
    {
      "type": "edit",
      "id": "d94d5186d695c511",
      "item": {
        "type": "paragraph",
        "id": "d94d5186d695c511",
        "text": "A [[MYO Armband]] sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses [⇒ [Focals Smart Glasses by North] ]are used to visualize the entire space, with a first-person perspective. As a result, a computational tool is obtained that allows generating trajectories, recording them, simulating them and generating the script or programming code to be implement in the robotic manipulator controller.\n"
      },
      "date": 1676569258974
    },
    {
      "type": "edit",
      "id": "d94d5186d695c511",
      "item": {
        "type": "paragraph",
        "id": "d94d5186d695c511",
        "text": "A [[MYO Armband]] sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses [⇒ Focals Smart Glasses by North [https://www.wired.com/review/focals-by-north-smart-glasses/ wired] ]are used to visualize the entire space, with a first-person perspective. As a result, a computational tool is obtained that allows generating trajectories, recording them, simulating them and generating the script or programming code to be implement in the robotic manipulator controller.\n"
      },
      "date": 1676569279293
    },
    {
      "type": "edit",
      "id": "d94d5186d695c511",
      "item": {
        "type": "paragraph",
        "id": "d94d5186d695c511",
        "text": "A [[MYO Armband]] sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses [⇒ Focals Smart Glasses by North [https://www.wired.com/review/focals-by-north-smart-glasses/ wired] ] are used to visualize the entire space, with a first-person perspective. As a result, a computational tool is obtained that allows generating trajectories, recording them, simulating them and generating the script or programming code to be implement in the robotic manipulator controller.\n"
      },
      "date": 1676569290228
    },
    {
      "item": {
        "type": "factory",
        "id": "27e32c5bb0d04792"
      },
      "id": "27e32c5bb0d04792",
      "type": "add",
      "after": "5c2f794fc29ba96d",
      "date": 1676569315120
    },
    {
      "type": "edit",
      "id": "27e32c5bb0d04792",
      "item": {
        "type": "pagefold",
        "id": "27e32c5bb0d04792",
        "text": "~"
      },
      "date": 1676569318161
    },
    {
      "id": "27e32c5bb0d04792",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "1da9c8d0f31b122d",
        "27e32c5bb0d04792",
        "d94d5186d695c511",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "79ccb921036bf521",
        "f56c4b80f471e965",
        "04263cb14ab3cde8",
        "6620e52cc27071a7",
        "15c0a14bca86aaca",
        "5226df25b25de0f9",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676569326335
    },
    {
      "type": "edit",
      "id": "d94d5186d695c511",
      "item": {
        "type": "paragraph",
        "id": "d94d5186d695c511",
        "text": "A [[MYO Armband]] sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses [⇒ Focals Smart Glasses by North [https://www.wired.com/review/focals-by-north-smart-glasses/ wired] ] are used to visualize the entire space, with a first-person perspective. "
      },
      "date": 1676569334913
    },
    {
      "type": "add",
      "id": "c752c15e3b1599d0",
      "item": {
        "type": "paragraph",
        "id": "c752c15e3b1599d0",
        "text": "As a result, a computational tool is obtained that allows generating trajectories, recording them, simulating them and generating the script or programming code to be implement in the robotic manipulator controller.\n"
      },
      "after": "d94d5186d695c511",
      "date": 1676569335609
    },
    {
      "id": "d94d5186d695c511",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "d94d5186d695c511",
        "1da9c8d0f31b122d",
        "27e32c5bb0d04792",
        "c752c15e3b1599d0",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "79ccb921036bf521",
        "f56c4b80f471e965",
        "04263cb14ab3cde8",
        "6620e52cc27071a7",
        "15c0a14bca86aaca",
        "5226df25b25de0f9",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676569337481
    },
    {
      "item": {
        "type": "factory",
        "id": "4bb1a3b63350bf4f"
      },
      "id": "4bb1a3b63350bf4f",
      "type": "add",
      "after": "5c2f794fc29ba96d",
      "date": 1676569348848
    },
    {
      "type": "edit",
      "id": "4bb1a3b63350bf4f",
      "item": {
        "type": "pagefold",
        "id": "4bb1a3b63350bf4f",
        "text": "~"
      },
      "date": 1676569351689
    },
    {
      "id": "4bb1a3b63350bf4f",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "d94d5186d695c511",
        "4bb1a3b63350bf4f",
        "1da9c8d0f31b122d",
        "27e32c5bb0d04792",
        "c752c15e3b1599d0",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "79ccb921036bf521",
        "f56c4b80f471e965",
        "04263cb14ab3cde8",
        "6620e52cc27071a7",
        "15c0a14bca86aaca",
        "5226df25b25de0f9",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676569359679
    },
    {
      "id": "2f440413db90837c",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "2f440413db90837c",
        "text": "The Myo wristband from the US company Thalmic Labs came onto the market in 2015 as the result of a crowdfunding project. Touted as revolutionary at the time, the device was intended to enable natural and everyday gesture control for the first time. The core element is the electromyographic sensors, which are supplemented by normal motion sensor technology. At least 50,000 units of the device were sold, but we do not have exact figures. After successful investment rounds, the company focused on the development of a smartglass, which was launched in 2018 under the name Focals by North. [https://www.wired.com/review/focals-by-north-smart-glasses/ wired]"
      },
      "after": "d94d5186d695c511",
      "attribution": {
        "page": "MYO Armband"
      },
      "date": 1676569363064
    },
    {
      "id": "2f440413db90837c",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "d94d5186d695c511",
        "4bb1a3b63350bf4f",
        "2f440413db90837c",
        "1da9c8d0f31b122d",
        "27e32c5bb0d04792",
        "c752c15e3b1599d0",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "79ccb921036bf521",
        "f56c4b80f471e965",
        "04263cb14ab3cde8",
        "6620e52cc27071a7",
        "15c0a14bca86aaca",
        "5226df25b25de0f9",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676569368204
    },
    {
      "id": "1da9c8d0f31b122d",
      "type": "move",
      "order": [
        "bc56fe296368e1f7",
        "c197d5bb4e4a1d58",
        "90c685b402a4c6e5",
        "f8d7642eebb1c602",
        "0a7fa8eac820fa8e",
        "d94d5186d695c511",
        "4bb1a3b63350bf4f",
        "1da9c8d0f31b122d",
        "2f440413db90837c",
        "27e32c5bb0d04792",
        "c752c15e3b1599d0",
        "9e4df6c6567b3c2f",
        "802d3758ccef2b55",
        "79ccb921036bf521",
        "f56c4b80f471e965",
        "04263cb14ab3cde8",
        "6620e52cc27071a7",
        "15c0a14bca86aaca",
        "5226df25b25de0f9",
        "a657fb7bd064b775",
        "6d3947ce8516b46c",
        "5c2f794fc29ba96d"
      ],
      "date": 1676569375387
    },
    {
      "type": "edit",
      "id": "2f440413db90837c",
      "item": {
        "type": "paragraph",
        "id": "2f440413db90837c",
        "text": "The Myo wristband from the US company Thalmic Labs came onto the market in 2015 as the result of a crowdfunding project. Touted as revolutionary at the time, the device was intended to enable natural and everyday gesture control for the first time. The core element is the electromyographic sensors, which are supplemented by normal motion sensor technology. At least 50,000 units of the device were sold, but we do not have exact figures. https://kinemic.com/de/band/kinemic-band-im-vergleich/thalmic-labs-myo-kinemic-band/ kinemic] (de), [https://wearabletech.io/myo-bracelet/ wearabletech]"
      },
      "date": 1676569417182
    },
    {
      "type": "add",
      "id": "66d9e959c3738b6a",
      "item": {
        "type": "paragraph",
        "id": "66d9e959c3738b6a",
        "text": "After successful investment rounds, the company focused on the development of a smartglass, which was launched in 2018 under the name Focals by North. [https://www.wired.com/review/focals-by-north-smart-glasses/ wired]"
      },
      "after": "2f440413db90837c",
      "date": 1676569417903
    },
    {
      "type": "edit",
      "id": "2f440413db90837c",
      "item": {
        "type": "paragraph",
        "id": "2f440413db90837c",
        "text": "The Myo wristband from the US company Thalmic Labs came onto the market in 2015 as the result of a crowdfunding project. Touted as revolutionary at the time, the device was intended to enable natural and everyday gesture control for the first time. The core element is the electromyographic sensors, which are supplemented by normal motion sensor technology. At least 50,000 units of the device were sold, but we do not have exact figures. [https://kinemic.com/de/band/kinemic-band-im-vergleich/thalmic-labs-myo-kinemic-band/ kinemic] (de), [https://wearabletech.io/myo-bracelet/ wearabletech]"
      },
      "date": 1676569424329
    },
    {
      "type": "edit",
      "id": "2f440413db90837c",
      "item": {
        "type": "paragraph",
        "id": "2f440413db90837c",
        "text": "The Myo wristband from the US company Thalmic Labs came onto the market in 2015 as the result of a crowdfunding project. Touted as revolutionary at the time, the device was intended to enable natural and everyday gesture control for the first time. The core element is the electromyographic sensors, which are supplemented by normal motion sensor technology. At least 50,000 units of the device were sold, but we do not have exact figures. [https://kinemic.com/de/band/kinemic-band-im-vergleich/thalmic-labs-myo-kinemic-band/ kinemic] (de), [https://wearabletech.io/myo-bracelet/ wearabletech], [https://kinemic.com/en/ kinemic]"
      },
      "date": 1676569656682
    },
    {
      "type": "edit",
      "id": "66d9e959c3738b6a",
      "item": {
        "type": "paragraph",
        "id": "66d9e959c3738b6a",
        "text": "After successful investment rounds, the company focused on the development of a [[Smart Glass]], which was launched in 2018 under the name Focals by North. [https://www.wired.com/review/focals-by-north-smart-glasses/ wired]"
      },
      "date": 1676569876005
    },
    {
      "type": "edit",
      "id": "bc56fe296368e1f7",
      "item": {
        "type": "paragraph",
        "id": "bc56fe296368e1f7",
        "text": "The motivation to develop a tool is given by its interface and the purpose of facilitating the interaction between a user programmer [⇒ [[User-Programmer Unity]]] and a manipulator robot in the task of creating trajectories that will be executed by the robot, one of the aids to implement for the elaboration of this project, 3D models are applied in virtual reality technology that has been welcomed by many sectors, among them the academic and the industrial. "
      },
      "date": 1676569933454
    },
    {
      "type": "edit",
      "id": "d94d5186d695c511",
      "item": {
        "type": "paragraph",
        "id": "d94d5186d695c511",
        "text": "A [[MYO Armband]] sensor is used to operate the position and orientation of the end effector of the robot within the environment and the Oculus Riff DK2 virtual reality glasses [⇒ Focals [[Smart Glass]]es by North [https://www.wired.com/review/focals-by-north-smart-glasses/ wired] ] are used to visualize the entire space, with a first-person perspective. "
      },
      "date": 1676570309190
    },
    {
      "item": {
        "type": "factory",
        "id": "f96444739647dca3"
      },
      "id": "f96444739647dca3",
      "type": "add",
      "after": "5c2f794fc29ba96d",
      "date": 1676571327355
    },
    {
      "id": "f96444739647dca3",
      "type": "remove",
      "removedTo": {
        "page": "2023-02-16"
      },
      "date": 1676571402256
    },
    {
      "type": "edit",
      "id": "bc56fe296368e1f7",
      "item": {
        "type": "paragraph",
        "id": "bc56fe296368e1f7",
        "text": "The motivation to develop a tool is given by its interface and the purpose of facilitating the interaction between a user programmer [⇒ [[User-Programmer Unity]]] and a [[Manipulator]] robot in the task of creating trajectories that will be executed by the robot, one of the aids to implement for the elaboration of this project, 3D models are applied in virtual reality technology that has been welcomed by many sectors, among them the academic and the industrial. "
      },
      "date": 1676611509851
    },
    {
      "type": "edit",
      "id": "f8d7642eebb1c602",
      "item": {
        "type": "paragraph",
        "id": "f8d7642eebb1c602",
        "text": "RODRÍGUEZ HOYOS, Daniel Santiago, TUMIALÁN BORJA, José Antonio and VELASCO PEÑA, Hugo Fernando, 2021. Virtual Reality Interface for Assist in Programming of Tasks of a Robotic Manipulator. In: CORTES TOBAR, Dario Fernando, HOANG DUY, Vo and TRONG DAO, Tran (eds.), AETA 2019 - Recent Advances in Electrical Engineering and Related Sciences: Theory and Application. Cham: Springer International Publishing. 2021. p. 328–335. Lecture Notes in Electrical Engineering. ISBN 978-3-030-53021-1. DOI 10.1007/978-3-030-53021-1_33. "
      },
      "date": 1676613633006
    }
  ]
}