{
  "title": "Modeling Language with Tensor Networks",
  "story": [
    {
      "type": "paragraph",
      "id": "9a9b905d18560cdc",
      "text": "I ([[Tai-Danae Bradley]]) thought it would be helpful before I start writing ideas on the board to give a little bit of background or context for what this talk is about and where I'm coming from, so before I write anything down I just want to spend a few minutes just putting together some ideas that I'm then going to spend many many minutes waiting for, so um I want to share some mathematics that I think is very interesting I like a lot and it's motivated by a very concrete problem in machine learning and the idea is to understand the meaning of words and expressions or longer pieces of text in a way that a computer can understand meaning; is motivated by a very concrete problem and machine learning and the idea is to understand the meaning of words or expressions or longer pieces of text to understand the meaning of these things in a way that a computer can understand so I wouldn't understand the meaning of sentences and a language and then the first question is how is this a mathematical endeavor but it has this mathematics."
    },
    {
      "type": "video",
      "id": "667eb88e2b7e773d",
      "text": "YOUTUBE 12j8OV-ptC4"
    },
    {
      "type": "paragraph",
      "id": "9f5f4eb300be72e9",
      "text": "one way I like to think of it is an idea from linguistics which is this distributional hypothesis so this is the idea that two words or phrases that appear in similar context and language will then have similar meanings so I like to think of this as a German a dilemma for linguistics I sort of know a word by the company it keeps this is a famous quote by John the first so to me this is like the yone dilemma for linguistics so already languages is certainly mathematical from that perspective there&#39;s another way to think of structure mathematically in language and that is this idea that language has both compositional and status constructions"
    },
    {
      "type": "paragraph",
      "id": "9e5e9dc98e4b5cca",
      "text": "so its composition on that words go together to form longer expressions like red firetruck red firetruck okay so you know compositionality but then there&#39;s also statistics behind that for example in English language um we sort of know that the red firetruck appears in language more frequently than the firetruck or you know polka-dotted firetruck and so I&#39;d like to propose then that given that that&#39;s true that somehow red is contributing to the meaning of firetruck so you sort of see red in this context a sort of network of relationships that firetruck shares with other words in the language more frequently so there&#39;s statistics there and so so the question then would be what is this mathematical structure what is this compositionality plus statistics"
    },
    {
      "type": "paragraph",
      "id": "3006b8012230399e",
      "text": "(2:56) so one route you can take to explore this is is a very category category theoretical approach and I&#39;m thinking now of this disco cap model maybe some of you have heard of it some of you haven&#39;t I can briefly describe it so the idea is that well language the meaning of language should be or the meaning of a sentence let&#39;s just take a sentence for example the meaning of a sentence well as determined by the meaning of these individual words that descriptive comprise the symptoms together with this grammatical rules for composing them so you know that you know now I&#39;m followed by a verb that&#39;s a valid thing in English so then what you can do is try to model this grammar together with semantics maybe grammar forms a category like a creeper for example um your semantics these are really like vector embeddings for words"
    },
    {
      "type": "paragraph",
      "id": "73d9fdbc06a4b047",
      "text": "if you want to then feed this into a computer even trying to model this structure as a functor from one to the other a functor from grammar into and nice semantics category like vector spaces so this has a name it&#39;s called the disco cat model distributional categorical compositional approach to language modeling for language but that&#39;s not what I&#39;m going to talk about today"
    },
    {
      "type": "paragraph",
      "id": "48aa6c458bd3c19f",
      "text": "so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language"
    },
    {
      "type": "paragraph",
      "id": "0bf12308ec81c4f3",
      "text": "I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly"
    },
    {
      "type": "paragraph",
      "id": "176a7f6d4ece3024",
      "text": "so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay"
    },
    {
      "type": "paragraph",
      "id": "223e7f6fc18f197a",
      "text": "there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction"
    },
    {
      "type": "video",
      "id": "30d6ac922c0daaa8",
      "text": "START 532\nYOUTUBE 12j8OV-ptC4\n"
    },
    {
      "type": "paragraph",
      "id": "ca91dcabbe29f6e1",
      "text": "(8:53) and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that"
    },
    {
      "type": "paragraph",
      "id": "8b6c60d9316c15fe",
      "text": "let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify"
    },
    {
      "type": "paragraph",
      "id": "580feb04330762e2",
      "text": "this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Modeling Language with Tensor Networks",
        "story": []
      },
      "date": 1714045450524
    },
    {
      "item": {
        "type": "factory",
        "id": "667eb88e2b7e773d"
      },
      "id": "667eb88e2b7e773d",
      "type": "add",
      "date": 1714045452705
    },
    {
      "type": "edit",
      "id": "667eb88e2b7e773d",
      "item": {
        "type": "video",
        "id": "667eb88e2b7e773d",
        "text": "YOUTUBE 12j8OV-ptC4"
      },
      "date": 1714045476296
    },
    {
      "item": {
        "type": "factory",
        "id": "cc78e366c3ed4038"
      },
      "id": "cc78e366c3ed4038",
      "type": "add",
      "after": "667eb88e2b7e773d",
      "date": 1714045562374
    },
    {
      "type": "edit",
      "id": "cc78e366c3ed4038",
      "item": {
        "type": "paragraph",
        "id": "cc78e366c3ed4038",
        "text": "they banned me from community to tell us about what were you language with ten&#39;s networks so modeling language with tensor networks I thought it would be helpful before I start writing ideas on the board to give a little bit of background or context for what this talk is about and where I&#39;m coming from so before I write anything down I just want to spend a few minutes just just putting together some ideas that I&#39;m gonna then then spend many many minutes and waiting so um I&#39;d love to share some mathematics that I think is very interesting I like a lot and it&#39;s motivated by a very concrete problem and machine learning and the idea is to understand meaning of words or expressions or longer pieces of text understand meaning of these things in a way that a computer can understand so I wouldn&#39;t understand meaning of phrases and a language and then the first question is how is this a mathematical endeavor but it has this mathematics one way I like to think of it is an idea from linguistics which is this distributional hypothesis so this is the idea that two words or phrases that appear in similar context and language will then have similar meanings so I like to think of this as a German a dilemma for linguistics I sort of know a word by the company it keeps this is a famous quote by John the first so to me this is like the yone dilemma for linguistics so already languages is certainly mathematical from that perspective there&#39;s another way to think of structure mathematically in language and that is this idea that language has both compositional and status constructions so its composition on that words go together to form longer expressions like red firetruck red firetruck okay so you know compositionality but then there&#39;s also statistics behind that for example in English language um we sort of know that the red firetruck appears in language more frequently than the firetruck or you know polka-dotted firetruck and so I&#39;d like to propose then that given that that&#39;s true that somehow red is contributing to the meaning of firetruck so you sort of see red in this context a sort of network of relationships that firetruck shares with other words in the language more frequently so there&#39;s statistics there and so so the question then would be what is this mathematical structure what is this compositionality plus statistics so one route you can take to explore this is is a very category category theoretical approach and I&#39;m thinking now of this disco cap model maybe some of you have heard of it some of you haven&#39;t I can briefly describe it so the idea is that well language the meaning of language should be or the meaning of a sentence let&#39;s just take a sentence for example the meaning of a sentence well as determined by the meaning of these individual words that descriptive comprise the symptoms together with this grammatical rules for composing them so you know that you know now I&#39;m followed by a verb that&#39;s a valid thing in English so then what you can do is try to model this grammar together with semantics maybe grammar forms a category like a creeper for example um your semantics these are really like vector embeddings for words if you want to then feed this into a computer even trying to model this structure as a functor from one to the other a functor from grammar into and nice semantics category like vector spaces so this has a name it&#39;s called the disco cat model distributional categorical compositional approach to language modeling for language but that&#39;s not what I&#39;m going to talk about today so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "date": 1714045566337
    },
    {
      "type": "fork",
      "site": "wiki.ralfbarkow.ch",
      "date": 1714045866963
    },
    {
      "type": "edit",
      "id": "cc78e366c3ed4038",
      "item": {
        "type": "paragraph",
        "id": "cc78e366c3ed4038",
        "text": "they banned me from community to tell us about what were you language with ten&#39;s networks so modeling language with tensor networks"
      },
      "date": 1714046030153
    },
    {
      "type": "add",
      "id": "9a9b905d18560cdc",
      "item": {
        "type": "paragraph",
        "id": "9a9b905d18560cdc",
        "text": "I thought it would be helpful before I start writing ideas on the board to give a little bit of background or context for what this talk is about and where I&#39;m coming from so before I write anything down I just want to spend a few minutes just just putting together some ideas that I&#39;m gonna then then spend many many minutes and waiting so um I&#39;d love to share some mathematics that I think is very interesting I like a lot and it&#39;s motivated by a very concrete problem and machine learning and the idea is to understand meaning of words or expressions or longer pieces of text understand meaning of these things in a way that a computer can understand so I wouldn&#39;t understand meaning of phrases and a language and then the first question is how is this a mathematical endeavor but it has this mathematics one way I like to think of it is an idea from linguistics which is this distributional hypothesis so this is the idea that two words or phrases that appear in similar context and language will then have similar meanings so I like to think of this as a German a dilemma for linguistics I sort of know a word by the company it keeps this is a famous quote by John the first so to me this is like the yone dilemma for linguistics so already languages is certainly mathematical from that perspective there&#39;s another way to think of structure mathematically in language and that is this idea that language has both compositional and status constructions so its composition on that words go together to form longer expressions like red firetruck red firetruck okay so you know compositionality but then there&#39;s also statistics behind that for example in English language um we sort of know that the red firetruck appears in language more frequently than the firetruck or you know polka-dotted firetruck and so I&#39;d like to propose then that given that that&#39;s true that somehow red is contributing to the meaning of firetruck so you sort of see red in this context a sort of network of relationships that firetruck shares with other words in the language more frequently so there&#39;s statistics there and so so the question then would be what is this mathematical structure what is this compositionality plus statistics so one route you can take to explore this is is a very category category theoretical approach and I&#39;m thinking now of this disco cap model maybe some of you have heard of it some of you haven&#39;t I can briefly describe it so the idea is that well language the meaning of language should be or the meaning of a sentence let&#39;s just take a sentence for example the meaning of a sentence well as determined by the meaning of these individual words that descriptive comprise the symptoms together with this grammatical rules for composing them so you know that you know now I&#39;m followed by a verb that&#39;s a valid thing in English so then what you can do is try to model this grammar together with semantics maybe grammar forms a category like a creeper for example um your semantics these are really like vector embeddings for words if you want to then feed this into a computer even trying to model this structure as a functor from one to the other a functor from grammar into and nice semantics category like vector spaces so this has a name it&#39;s called the disco cat model distributional categorical compositional approach to language modeling for language but that&#39;s not what I&#39;m going to talk about today so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "cc78e366c3ed4038",
      "date": 1714046036464
    },
    {
      "type": "remove",
      "id": "cc78e366c3ed4038",
      "date": 1714046037830
    },
    {
      "type": "edit",
      "id": "9a9b905d18560cdc",
      "item": {
        "type": "paragraph",
        "id": "9a9b905d18560cdc",
        "text": "I thought it would be helpful before I start writing ideas on the board to give a little bit of background or context for what this talk is about and where I&#39;m coming from so before I write anything down I just want to spend a few minutes just just putting together some ideas that I&#39;m gonna then then spend many many minutes and waiting so um I&#39;d love to share some mathematics that I think is very interesting I like a lot and it&#39;s motivated by a very concrete problem and machine learning and the idea is to understand meaning of words or expressions or longer pieces of text understand meaning of these things in a way that a computer can understand so I wouldn&#39;t understand meaning of phrases and a language and then the first question is how is this a mathematical endeavor but it has this mathematics"
      },
      "date": 1714046122341
    },
    {
      "type": "add",
      "id": "9f5f4eb300be72e9",
      "item": {
        "type": "paragraph",
        "id": "9f5f4eb300be72e9",
        "text": "one way I like to think of it is an idea from linguistics which is this distributional hypothesis so this is the idea that two words or phrases that appear in similar context and language will then have similar meanings so I like to think of this as a German a dilemma for linguistics I sort of know a word by the company it keeps this is a famous quote by John the first so to me this is like the yone dilemma for linguistics so already languages is certainly mathematical from that perspective there&#39;s another way to think of structure mathematically in language and that is this idea that language has both compositional and status constructions so its composition on that words go together to form longer expressions like red firetruck red firetruck okay so you know compositionality but then there&#39;s also statistics behind that for example in English language um we sort of know that the red firetruck appears in language more frequently than the firetruck or you know polka-dotted firetruck and so I&#39;d like to propose then that given that that&#39;s true that somehow red is contributing to the meaning of firetruck so you sort of see red in this context a sort of network of relationships that firetruck shares with other words in the language more frequently so there&#39;s statistics there and so so the question then would be what is this mathematical structure what is this compositionality plus statistics so one route you can take to explore this is is a very category category theoretical approach and I&#39;m thinking now of this disco cap model maybe some of you have heard of it some of you haven&#39;t I can briefly describe it so the idea is that well language the meaning of language should be or the meaning of a sentence let&#39;s just take a sentence for example the meaning of a sentence well as determined by the meaning of these individual words that descriptive comprise the symptoms together with this grammatical rules for composing them so you know that you know now I&#39;m followed by a verb that&#39;s a valid thing in English so then what you can do is try to model this grammar together with semantics maybe grammar forms a category like a creeper for example um your semantics these are really like vector embeddings for words if you want to then feed this into a computer even trying to model this structure as a functor from one to the other a functor from grammar into and nice semantics category like vector spaces so this has a name it&#39;s called the disco cat model distributional categorical compositional approach to language modeling for language but that&#39;s not what I&#39;m going to talk about today so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "9a9b905d18560cdc",
      "date": 1714046123292
    },
    {
      "id": "9a9b905d18560cdc",
      "type": "move",
      "order": [
        "9a9b905d18560cdc",
        "667eb88e2b7e773d",
        "9f5f4eb300be72e9"
      ],
      "date": 1714046410073
    },
    {
      "type": "edit",
      "id": "9a9b905d18560cdc",
      "item": {
        "type": "paragraph",
        "id": "9a9b905d18560cdc",
        "text": "I ([[Tai-Danae Bradley]]) thought it would be helpful before I start writing ideas on the board to give a little bit of background or context for what this talk is about and where I'm coming from, so before I write anything down I just want to spend a few minutes just putting together some ideas that I'm then going to spend many many minutes waiting for, so um I want to share some mathematics that I think is very interesting I like a lot and it's motivated by a very concrete problem in machine learning and the idea is to understand the meaning of words and expressions or longer pieces of text in a way that a computer can understand meaning; is motivated by a very concrete problem and machine learning and the idea is to understand the meaning of words or expressions or longer pieces of text to understand the meaning of these things in a way that a computer can understand so I wouldn't understand the meaning of sentences and a language and then the first question is how is this a mathematical endeavor but it has this mathematics."
      },
      "date": 1714046432071
    },
    {
      "type": "edit",
      "id": "9f5f4eb300be72e9",
      "item": {
        "type": "paragraph",
        "id": "9f5f4eb300be72e9",
        "text": "one way I like to think of it is an idea from linguistics which is this distributional hypothesis so this is the idea that two words or phrases that appear in similar context and language will then have similar meanings so I like to think of this as a German a dilemma for linguistics I sort of know a word by the company it keeps this is a famous quote by John the first so to me this is like the yone dilemma for linguistics so already languages is certainly mathematical from that perspective there&#39;s another way to think of structure mathematically in language and that is this idea that language has both compositional and status constructions"
      },
      "date": 1714046642266
    },
    {
      "type": "add",
      "id": "9e5e9dc98e4b5cca",
      "item": {
        "type": "paragraph",
        "id": "9e5e9dc98e4b5cca",
        "text": "so its composition on that words go together to form longer expressions like red firetruck red firetruck okay so you know compositionality but then there&#39;s also statistics behind that for example in English language um we sort of know that the red firetruck appears in language more frequently than the firetruck or you know polka-dotted firetruck and so I&#39;d like to propose then that given that that&#39;s true that somehow red is contributing to the meaning of firetruck so you sort of see red in this context a sort of network of relationships that firetruck shares with other words in the language more frequently so there&#39;s statistics there and so so the question then would be what is this mathematical structure what is this compositionality plus statistics so one route you can take to explore this is is a very category category theoretical approach and I&#39;m thinking now of this disco cap model maybe some of you have heard of it some of you haven&#39;t I can briefly describe it so the idea is that well language the meaning of language should be or the meaning of a sentence let&#39;s just take a sentence for example the meaning of a sentence well as determined by the meaning of these individual words that descriptive comprise the symptoms together with this grammatical rules for composing them so you know that you know now I&#39;m followed by a verb that&#39;s a valid thing in English so then what you can do is try to model this grammar together with semantics maybe grammar forms a category like a creeper for example um your semantics these are really like vector embeddings for words if you want to then feed this into a computer even trying to model this structure as a functor from one to the other a functor from grammar into and nice semantics category like vector spaces so this has a name it&#39;s called the disco cat model distributional categorical compositional approach to language modeling for language but that&#39;s not what I&#39;m going to talk about today so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "9f5f4eb300be72e9",
      "date": 1714046643575
    },
    {
      "type": "edit",
      "id": "9e5e9dc98e4b5cca",
      "item": {
        "type": "paragraph",
        "id": "9e5e9dc98e4b5cca",
        "text": "so its composition on that words go together to form longer expressions like red firetruck red firetruck okay so you know compositionality but then there&#39;s also statistics behind that for example in English language um we sort of know that the red firetruck appears in language more frequently than the firetruck or you know polka-dotted firetruck and so I&#39;d like to propose then that given that that&#39;s true that somehow red is contributing to the meaning of firetruck so you sort of see red in this context a sort of network of relationships that firetruck shares with other words in the language more frequently so there&#39;s statistics there and so so the question then would be what is this mathematical structure what is this compositionality plus statistics"
      },
      "date": 1714046742186
    },
    {
      "type": "add",
      "id": "3006b8012230399e",
      "item": {
        "type": "paragraph",
        "id": "3006b8012230399e",
        "text": "so one route you can take to explore this is is a very category category theoretical approach and I&#39;m thinking now of this disco cap model maybe some of you have heard of it some of you haven&#39;t I can briefly describe it so the idea is that well language the meaning of language should be or the meaning of a sentence let&#39;s just take a sentence for example the meaning of a sentence well as determined by the meaning of these individual words that descriptive comprise the symptoms together with this grammatical rules for composing them so you know that you know now I&#39;m followed by a verb that&#39;s a valid thing in English so then what you can do is try to model this grammar together with semantics maybe grammar forms a category like a creeper for example um your semantics these are really like vector embeddings for words if you want to then feed this into a computer even trying to model this structure as a functor from one to the other a functor from grammar into and nice semantics category like vector spaces so this has a name it&#39;s called the disco cat model distributional categorical compositional approach to language modeling for language but that&#39;s not what I&#39;m going to talk about today so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "9e5e9dc98e4b5cca",
      "date": 1714046743239
    },
    {
      "type": "edit",
      "id": "3006b8012230399e",
      "item": {
        "type": "paragraph",
        "id": "3006b8012230399e",
        "text": "so one route you can take to explore this is is a very category category theoretical approach and I&#39;m thinking now of this disco cap model maybe some of you have heard of it some of you haven&#39;t I can briefly describe it so the idea is that well language the meaning of language should be or the meaning of a sentence let&#39;s just take a sentence for example the meaning of a sentence well as determined by the meaning of these individual words that descriptive comprise the symptoms together with this grammatical rules for composing them so you know that you know now I&#39;m followed by a verb that&#39;s a valid thing in English so then what you can do is try to model this grammar together with semantics maybe grammar forms a category like a creeper for example um your semantics these are really like vector embeddings for words"
      },
      "date": 1714046802648
    },
    {
      "type": "add",
      "id": "73d9fdbc06a4b047",
      "item": {
        "type": "paragraph",
        "id": "73d9fdbc06a4b047",
        "text": "if you want to then feed this into a computer even trying to model this structure as a functor from one to the other a functor from grammar into and nice semantics category like vector spaces so this has a name it&#39;s called the disco cat model distributional categorical compositional approach to language modeling for language but that&#39;s not what I&#39;m going to talk about today so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "3006b8012230399e",
      "date": 1714046803768
    },
    {
      "type": "edit",
      "id": "73d9fdbc06a4b047",
      "item": {
        "type": "paragraph",
        "id": "73d9fdbc06a4b047",
        "text": "if you want to then feed this into a computer even trying to model this structure as a functor from one to the other a functor from grammar into and nice semantics category like vector spaces so this has a name it&#39;s called the disco cat model distributional categorical compositional approach to language modeling for language but that&#39;s not what I&#39;m going to talk about today"
      },
      "date": 1714046840823
    },
    {
      "type": "add",
      "id": "48aa6c458bd3c19f",
      "item": {
        "type": "paragraph",
        "id": "48aa6c458bd3c19f",
        "text": "so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "73d9fdbc06a4b047",
      "date": 1714046841266
    },
    {
      "type": "edit",
      "id": "48aa6c458bd3c19f",
      "item": {
        "type": "paragraph",
        "id": "48aa6c458bd3c19f",
        "text": "so I like to think of that as it&#39;s kind of an inside out approach but I like to describe something a little bit different today which is more of an outside in approach and the key starting idea for this is to let statistics serve as a proxy for grammar it&#39;s actually that&#39;s an important idea so I will write that I mean write that down so what I want to describe today is a model for language and awake my life statistics serve as a proxy in other words what I mean here I mean I would like to infer I&#39;d like to infer what are valid expressions in my language just given some examples of valid expressions in that language"
      },
      "date": 1714046896156
    },
    {
      "type": "add",
      "id": "0bf12308ec81c4f3",
      "item": {
        "type": "paragraph",
        "id": "0bf12308ec81c4f3",
        "text": "I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "48aa6c458bd3c19f",
      "date": 1714046897219
    },
    {
      "type": "edit",
      "id": "0bf12308ec81c4f3",
      "item": {
        "type": "paragraph",
        "id": "0bf12308ec81c4f3",
        "text": "I want to know that some words go together to form good sentences and others don&#39;t just by me saying the statistics in that language just by seeing it so in other words if I write write this down I want to I&#39;m going to sort of refine my goal a little bit I want to infer a probability distribution on some text so I want to know what goes with what in my language you know with high probability I&#39;ll see a red firetruck but with very little probability or lower I&#39;ll see blue fire truck this is what I what goes together with what based on statistics okay so then how do you how do you do this how does one do this so what I&#39;d like to do is sort of give away the punchline I&#39;m going to give you give away the punchline and sort of say how I&#39;m going to describe from your model in which I can infer this probability distribution give away the punchline within the rest of the talk will sort of unwind it very slowly"
      },
      "date": 1714046963761
    },
    {
      "type": "add",
      "id": "176a7f6d4ece3024",
      "item": {
        "type": "paragraph",
        "id": "176a7f6d4ece3024",
        "text": "so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "0bf12308ec81c4f3",
      "date": 1714046964309
    },
    {
      "type": "edit",
      "id": "176a7f6d4ece3024",
      "item": {
        "type": "paragraph",
        "id": "176a7f6d4ece3024",
        "text": "so the idea for this model starts by viewing language as a quantum many-body problem in other words what that means is you have these atomic pieces of your language these are characters or words for example and you think of these atomic pieces as individual quantum particles each of which can occupy many many states then language then is this interacting system of a bunch of these quantum particles and well how do you model this system then what is a model for this language what you can do is then take samples of it examples of expressions in your language you can think of as observations of this quantum system then when you have these observations you can then try to infer the ground state of it using some machine learning algorithm or physics inspired ideas yes attention ultras of associative algebra language is not associative language is not associative well associative how language associates is the essence of the grammar of the language but this is I haven&#39;t mentioned any algebra yet that although that would be interesting yeah that&#39;s a great point yeah okay"
      },
      "date": 1714047037629
    },
    {
      "type": "add",
      "id": "223e7f6fc18f197a",
      "item": {
        "type": "paragraph",
        "id": "223e7f6fc18f197a",
        "text": "there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "176a7f6d4ece3024",
      "date": 1714047037990
    },
    {
      "type": "edit",
      "id": "223e7f6fc18f197a",
      "item": {
        "type": "paragraph",
        "id": "223e7f6fc18f197a",
        "text": "there is maybe a little representation theory so maybe it&#39;ll come up later in the talk yeah but let me not get ahead okay so but I just kind of threw a bunch of words at you so what does that mean so that&#39;s now what I&#39;d like to describe that&#39;s my light district I&#39;d like to unwind everything I just said and it turns out so so everything I just said is a little bit complicated and a little bit heavy-handed and so you might wonder why in the world would you do this and what does it mean but it turns out the math.max is actually quite simple it&#39;s quite it&#39;s a little bit Elementary actually I mean surprisingly and wonderfully it&#39;s it&#39;s it&#39;s very easy to understand and it also it&#39;s a very good thing to do so there are advantages to doing that so there are advantages to taking somebody that&#39;s very classical like frequency counts on text and then passing over into this world what&#39;s really quantum probability theory so that&#39;s now what I&#39;d like to describe and so so here&#39;s my just finished introduction"
      },
      "date": 1714047104003
    },
    {
      "type": "add",
      "id": "ca91dcabbe29f6e1",
      "item": {
        "type": "paragraph",
        "id": "ca91dcabbe29f6e1",
        "text": "and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "223e7f6fc18f197a",
      "date": 1714047104410
    },
    {
      "type": "edit",
      "id": "ca91dcabbe29f6e1",
      "item": {
        "type": "paragraph",
        "id": "ca91dcabbe29f6e1",
        "text": "and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so"
      },
      "date": 1714047160016
    },
    {
      "type": "add",
      "id": "8b6c60d9316c15fe",
      "item": {
        "type": "paragraph",
        "id": "8b6c60d9316c15fe",
        "text": "let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "ca91dcabbe29f6e1",
      "date": 1714047160625
    },
    {
      "type": "edit",
      "id": "8b6c60d9316c15fe",
      "item": {
        "type": "paragraph",
        "id": "8b6c60d9316c15fe",
        "text": "let me now just describe for you a very nice classical a passage from classical probability theory to quantum probability theory and then at the end we&#39;ll tie all of that back into language using what I&#39;m calling attention hour I&#39;m not going to say what that is just yet but whenever you see tend to network you&#39;re more than welcome to think string diagram so so let me describe for you now a passage from classical probability theory to quantify"
      },
      "date": 1714047204582
    },
    {
      "type": "add",
      "id": "580feb04330762e2",
      "item": {
        "type": "paragraph",
        "id": "580feb04330762e2",
        "text": "this is now section two and just let me just start off with some notation on the board so in what follows let&#39;s let high the probability distribution on a finite set s is a finite set sum of all of these numbers so the first thing I&#39;d like to do is to describe the quantum analog of this what this isn&#39;t the Sokolov quantum analog or the quantum version of a probability distribution well this past is from classical to quantum probabilities is essentially passes from sets into vector spaces or into linear algebra so so well to every finite said there&#39;s the free vector space on that set and if that was do complex vector spaces so there&#39;s the houses sort of just no recall there&#39;s a natural map from us into the free vector space on s so every elements in ways that corresponds to an independent basis vector here for notation if little s is an in capital S I what&#39;s the vector representation for that I&#39;ve grown to like this physicist ket&#39;s notation I can&#39;t see in Ted noting so that&#39;s good so what do I mean by this well if if I choose an ordering on this then you know say this is the if-- if-- element in my set then the corresponding factor for that I just really mean to stand the ID standard basis vector so this is 0 with a 1 in the highest place so to fill a your object but this is just notation that I like ok so then this is now nice orthonormal set and it&#39;s over at space ok so I hi starting and starting my passage from sets into vector spaces that just take the free vector space on that set but then what is this quantum version of a probability distribution well it&#39;s going to be a particular linear operator on this space it&#39;s an operator which satisfies some properties and it&#39;s called a density operator so I&#39;ll write it down but a density operator is a map on this space which is self-adjoint positive semi definite and has trace 1 all right this is the quantum version of a probability distribution so I&#39;ll say a density operator row on PS well satisfies the properties it&#39;s a madness positive semi-definite and has trace 1 okay so I you know I am talking about string diagrams or tensor networks so if I were to draw this I just mean you know unknown with two edges so that the edges are my vector space and the notice to operator itself and that is Trace is one I need is some of our common index which is connecting the wires okay so why is this the quartz and analog of a probability distribution well these three properties may may cause you to think of Oh non-negative real non negative eigenvalues that add up to one so that&#39;s like a good sanity check but there&#39;s also a very concrete way to see this mainly any density operator on the space yes defines a classical probability distribution on the underling set best so so this is an important so know every density defines a probability distribution on s by what is it let&#39;s see actually well I&#39;m going to use this notation that the probability distribution induced by Rho all right pycelle growth so what&#39;s the probability of a little elements as in my set what you do is you take the vector associated to the element apply will get a new vector and then take the inner product with Amy starting with so if you know if if I look back at my ordering what&#39;s the probability of my elements well it&#39;s really just the ice diagonal of my density so this is what this is this writing okay great so this is so low you can think of as I&#39;ll say the content along vision so then you might wonder what happens if I already have a distribution on a set s can i define a density operator so that the probability distribution defined by it coincides with the one I I mean that&#39;s it that&#39;s a good question and the answer is yes actually there I&#39;m in fact I can show you two ways that you can do this so suppose I&#39;m going to leave this up so suppose you already have the distribution on pi I mean sorry a distribution PI on this here R I&#39;m going to show you two ways to define a density so that the distribution induced by it is the one that you started with in fact you can already think of the first way or maybe it&#39;s the second way you can think of a way so I have a finite probability distribution and I want to lend your operator that is self-adjoint positive semi-definite and has trace one yeah just hug the probably somebody reckon definitely just the boring the most boring thing you could do is to create a diagonal operator from it so okay so so option number one is a diagonal diagonal operator in other words I have this friend list I just stick over that title like you said so bro maybe I&#39;ll save road I am this looks like well it&#39;s the probability of s1 probability yes - okay in zeros customer then then it&#39;s easy to check those things for satisfied but also what&#39;s the important thing well the important thing is that is this equality satisfied yeah that&#39;s just the diagonal okay so that&#39;s the easy way I don&#39;t really like that way because we haven&#39;t done anything you just like took a list and then made it tilt it a little bit so that&#39;s not going anything so there&#39;s another way what else fun so maybe you can think of the other way yeah like maybe you could have an outer product with the square roots yeah absolutely yeah well Wow how did you get how did you think of that well I&#39;m not sure but this looks sort of like a maximal ring thing you know so maybe like Oh anything of the extreme cases maximal ring version maybe there&#39;s a minimal ring version like projection onto a single single thing then you mentioned is something about square roots I don&#39;t know why square root if that seems like a very clever choice that may or may not have taken some thought or maybe just felt like or I mean like because you want because you want to maintain that you want to do the diagonals to be this yeah yeah I know you said sure you could or product you&#39;re gonna be multiplying things twice yeah so alright option number one is not as exciting so what&#39;s the second thing it&#39;s exactly as he prescribed I can&#39;t let me write this as projection on a unit vector so in other words the loop alright let me say a row is rejection I&#39;m gonna call my unit vector I&#39;m gonna use it over again several times so let me I liked sobbing so I&#39;m going to call it aside so so so amazing this notation ket notation this is just a linear combination of elements in s what what combination is it so sy is a very particular vector it&#39;s a very specific one what is it well I&#39;m going to sum over all elements in my set but I&#39;m going to weight them by not the probability but the square root and why is it this why is this square root here is opposed to not it&#39;s precisely because I want this to be satisfied that&#39;s why all right so this is like a random one thing this is like a next moment thing you see good things in the sort of two extremes this is this is boring a little bit and this I think is much better it turns out we did a lot of mileage this way and so so for the rest of the talk unless I remember to say otherwise when I say density operator you know that&#39;s induced by some dystrophy generally thinking of this one this ring one one okay so why then why is it that I like number two better well to see why we should go back for a few minutes to thinking about classical probability theory again so let&#39;s momentarily go back to probability costantly probabilities so why is to better better you know according to me not in like a universal sense but just why so what we want to do is this is no more - more interesting so let&#39;s let&#39;s think you know back to possible probability so I started with a probability distribution on a finite set it told you the quantum version so I&#39;m actually thinking I&#39;m sort of creating for you a dictionary from the world of classical probability theory to quantum probabilities so let me now add another entry in that dictionary so consider for a moment a Joint Distribution so suppose actually that you know our set that&#39;s maybe is really a Cartesian product of finite sets x and y so if I have a Joint Distribution then of course I can I can get a distribution on either one of the factors by marginalizing so give it a joint I can get a marginal distribution by the probability of a little element I sort of sum over integrate out this complimentary complimentary set okay of course we know this why am I putting on the board because this has a nice quantum analog so you know I&#39;m sort of adding again to my dictionary how do I go from classical to quantum well we already know how to do this joint you know we don&#39;t have it translated from classical to quantum for a joint distribution I get a density operator how do I do it well I really like I like this one so now just imagine each s is a tuple X comma Y so I just have the probability of my pair X comma Y and well what&#39;s the vector corresponding to that parable it&#39;s just their tensor product so I sum all of these pairs weighted by the skirts or their probability project onto that vector that vector spans a line you project onto it and so that&#39;s a perfectly finding density operator but also do the down in the lining but I like this one better so so you know what&#39;s the common analog at this again it&#39;s row which is projection on the side and just to be explicit let me write out okay so this is a vector in the tensor product so actually I know I&#39;m running into my outlines there but I do like I do like remembering my pictures so of course when I write a vector and tensor product I really mean something like a node with two edges so this is like CX and CY okay and then so this is what Silas like so that a projection on his side I can think of that as well it&#39;s really the outer product emphasize with itself so this kind of has business picture okay so I have the quantum analog of it Joint Distribution it&#39;s just this this density operator projection onto the sum of my elements weighted by these probabilities so then what would be the quantum analog of marginalizing well this son has a nice linear algebraic analog it&#39;s called the partial trace okay sorry that I&#39;m sorry crowded here but the mark the act of summing over it has a nice analog a minute algebra the partial trace and the thing that you get as a result or the analog of the marginal distribution is going to be again a density operator but it&#39;s a density operator on a smaller space so so it&#39;ll be call it a reduced density operator okay so let me actually back up and explain each of these things a little bit a little bit slower so this is actually where where things get really interesting so what what really is the partial trace then actually I see so there&#39;s sort of two ways to think about the partial trace there&#39;s just the linear algebra definition and then there&#39;s the intuition so let me give you the intuition first so you imagine you have this large system of a whole bunch of interacting components and suppose you know the state of this system suppose you have a density operator ah why am i referring to a density operator as a state I forgot to mention this so another word another name for density operators is quantum state so if you look at physics literature quantum information theory literature density operators in quantum space these are interchangeable things so suppose I have this big system of a whole bunch of things that are interacting with each other and suppose I know this state of this system ah but also suppose I&#39;d like to sort of hone in or zoom in on just one smaller piece of it or maybe you know a subset of these components I&#39;d like to know what is the state of that smaller subsystem so the partial trace helps you do this but the key is it&#39;s not understanding this smaller subsystem in isolation but it&#39;s learning the state of that thing given the fact that it has all of these interactions with this larger environment so the partial trace of this operation that allows you to go from large to small but given that that that small thing is interacting with this larger in widgets it&#39;s so sort of the intuition but what actually isn&#39;t in math monitor what&#39;s the lady on the bus behind it so so let me tell you now so consider it for a moment and I&#39;m talking about this larger system so this is really like a tensor product of Hilbert spaces so consider for a Mayans just this tensor product that we have on the board already now here&#39;s here&#39;s just an observation there are no natural linear maps down to each vector so you can you do have maps down but they sort of require twice you would have to pick a basis then pick an element then do something like project but there&#39;s nothing really natural that goes down but it turns out there are after you pass to endomorphisms of these spaces so what I read on the board is not true so let me fix it if I then passed you in there morphisms of these spaces then I do have natural maps down and these max are called these are exactly what this is what the pressure traces will be their partial trace partial trace Maps so what what what what are these maps um suppose if I had it when you have an element in this space an operator on the CX tensor an operator on the CY how do I just get an operator on the first back or alone well sort of in the name yeah sure yeah you just I&#39;ll take the trace of G and then the same thing here if I have tensor product f GG how to get a map on y alone will I ready have a map on tomorrow me so then I&#39;ll just multiply it by the trace of F okay so these are our partial trace maps what&#39;s nice now so so so what if the operator that I start in it is one of these densities in particular but if it&#39;s this one density which is projection onto this vector side here well well the answer is nice then the answer is really the reason why is thinking of partial trace as the quantum analog to marginalizing and why the resulting things you get down here are like the quantum analogues of these marginal distributions so let me just write what I just said if I start with it gets to be consistently operating screen so if I start with a density it operates on the tensor product I can apply the partial trace actually I can give this a name let me call this tracing out why and this is tracing out X so I can apply this partial trace map so if i trace out why and i apply that to row i get well let me let me say Rho sub X so this is an operator on C X alone or I can trace out X from this operator roll again another operator I&#39;ll call it row sub y this operates on cya home and these are what I&#39;m referring to as reduced dancing so it turns out it turns out that the partial trace map preserves the properties which I raise self-adjoint positive somebody&#39;s definiteness and trace and so what I&#39;d get down here truly I density operators and I claim that these operators are the quantum analog of marginal probability distributions so it&#39;s not maybe it&#39;s not obvious why but it it is it&#39;s quite clear when you look at the matrix representation of these operators with respect to the base is given by ourselves so actually let&#39;s do that that&#39;s for a half an hour warning but I forgot so you have a twenty two minutes okay so what is so let me just focus on this row sub axis into the operator on the space DX what is this so as a matrix you discovered that when you&#39;re a density the one on top the cream on the top is orthogonal projection onto that vector what you discover is that on the diagonal of this operator you have exactly these marginal probabilities the classical ones do freaky so I&#39;ve just shown you another way to compute marginal probability you can you can map it over into the world of linear algebra projectiles when I respect her and then take the partial trace I get exactly said marginalize and if that was it you should be a little bit dissatisfied because Wow by those are all that work just to compute something I already knew so here&#39;s what some so great is that in addition to this diagonal there are also non-zero off-diagonal entries these non-zero off-diagonal entries tell you something about how elements in your set X relate to each other given sort of shared relationships in this set y then you integrate it out so you actually have information about how things and X relate to each other based you know dependent on or given their interactions into this other system that you sort of turn to blind blind eye to and this is more than just intuition there&#39;s actually a very concrete but sort of combinatorial way to understand this so let me tell you what is this IJ&#39;s entry so what is the IJ entry of Mori reduced density operator for simplicity what is the original density operator the yeah the original one is power projection yeah yeah yeah if it&#39;s all right with your offline exactly exactly that&#39;s it yep so when when that is your original density the IJ off IJ entry of the off diagonal has a nice combinatorial interpretation for simplicity let&#39;s just for the moment imagine that our original distribution pipe is an empirical one so I&#39;m really just things then the IJ entry is basically let me just say this and words then I&#39;ll write it it&#39;s basically the number of shared continuations and why that X I and xj have so what do I mean by continuation if I think of a pair X comma Y let me just you know we talk about language earlier so let me think of this as an expression like red firetruck so I&#39;ll think of when I say continuation or suffix I&#39;m thinking of Oh firetruck is a continuation of the word red so this is you know I have this scenario you know think of X I is is something like you know red firetruck in what else are fire trucks they&#39;re like pink big but what else is bacon red what word Clifford yeah exactly exactly okay so why could be fire truck okay so that contributes to this entry okay Clifford is also oh I really fast started um Clifford is also big in red ok Clifford so then that contributes to this offhand entry what else is big and red like what bonfire okay fine fire okay and so forth so for everything that&#39;s like fast bla red blob that water bowl yeah okay great great great thank you thanks yeah okay so for each such you know word and why that is then is fast and big I mean huh red and Bank then that&#39;s contributing to this red big entry and my participants in matrix okay so okay so that&#39;s nice why is that nice because this is exactly I mean this is information you do not have access to when you compute marginal probability in the usual way so when you compute marginal probability in the classical way you sort of can only see this diagonal but there&#39;s all this other information and an exists on the off diagonal of this reduced density so then there&#39;s sort of two questions that come to mind now one is is there a more principled way of sort of harnessing this information and then to if there is what can I do with it so let me answer those questions one yes there is a much more principled way of harnessing this I think you can see it already it&#39;s in this spectral information this matrix if the density I started with was this boring diagonal one I don&#39;t know they said I guess and if if I start with a diagonal density and then I compute the partial trace I&#39;m gonna get diagonal operators again so you haven&#39;t done anything but the fact that you said we started with this projection now oh let me slow down if this is a diagonal operator I haven&#39;t done a thing and we&#39;re over its eigenvectors correspond exactly to the elements and X so I really haven&#39;t done anything but now that I&#39;ve projected onto this unit vector side and you compute the partial trace and look at these off diagonal entries they&#39;re nonzero so the eigenvectors of this operator are then going to be combinations of elements and X I kind of like to think of those as picking up concepts that exist in the Sun that that&#39;s the X concepts determined by this statistics given to me by PI okay so so if you look at the special decomposition of this this is telling me interesting statistical information about this joint distribution which I don&#39;t really have access to when I marginalize in the usual way that&#39;s interesting that is just that you know this is number two are a list that&#39;s just you know so very interesting about Maddox so now what can I do with that so what can I do with that now I&#39;d like to circle back to my this quest for modeling language so so I didn&#39;t erase and that&#39;s good so what was my goal my vote let&#39;s do infer a probability distribution on text in other words if you show me some examples of valid expressions in your language I can sort of get the statistics from that and then I want to infer then what is that distribution so that I can then a sample from it and then generate new data it&#39;s just sort of a goal okay so how is this connected to that so in the minutes that remain working all right then I&#39;m gonna describe this so this is now part three so to sort of you know what&#39;s my starting point for when this kind of language type model well I like to view language you know expressions are like sequences of things from some alphabet so let me now start to think of languages aesthetic sequences and then I&#39;m going to be interested in a probability distribution on that set so so let&#39;s let&#39;s set it up the follow anyway so I&#39;ll say suppose now for this sort of application part let s the set of sequences okay I&#39;m thinking of sentences are like sequences of words or a word in the sequence of characters and for simplicity let&#39;s think of this set containing sequences all of this sequences of length in I&#39;ll say we couple it in from a sign off of it so an element in s is something like anyone okay so you know if if a for example is the set 0 1 I&#39;m looking at this strings and women for instance or you can take it to be you know the 26 letters and you must alphabet or whatever you like okay then then what the idea now is that on on this set s is some probability distribution now you may not have access to it maybe I don&#39;t know what it is like the probability distribution on language the English language but I&#39;d like to learn I&#39;d like to get really close to it okay so so you can just imagine that hi is distribution on s maybe we don&#39;t know it but we went to invert it in other words I want to know what things go you guys are my regulation let them so how do you do this well you know maybe I don&#39;t know PI because it&#39;s very you know maybe my language is huge and massive and not manageable and there&#39;s no hope so what I should do is look at a small subset just a subset of sequences just take a collection of things and then that will be that&#39;ll determine an empirical distribution and have something more manageable so let&#39;s let see the subset of my sequences so then so if I take a subset that then determines an empirical distribution so let me write it this way so I have an empirical probability distribution let me use PI hat so PI hat is like an approximation of pi so it&#39;s not quite pi but I kind of want to get better so I have pi hat and I want to do something I wouldn&#39;t I want to tweak it somehow so I can then get this sort of true distribution from which I can then sample and generate new new examples so then what is that what is this process what is this cheating process so this is exactly now we&#39;re going to take this interesting mathematics and apply it to this concrete problem and then see see what happens so so I want to I want to sort of model this pi hat and then apply some algorithm to it which gets me closer to PI and the first starting place for that model is to exactly define this this vector sign so so so what&#39;s a model for a PI hat so I&#39;m gonna again define a vector starting the same one I&#39;m just going to take the sum of all sequences and but the sum of all sequence is only in my settee I&#39;m thinking of T is like my painting set and then wait those sequences by the square root of these probabilities these empirical probabilities now here&#39;s what&#39;s sort of the thing to notice remember that s is a sequence of length n so what space is this side and this side is in CS but this decomposes as the tensor product on one factor for each copy of my alphabet so you know if a is bit strings and I have length you know n of them leave in distance this is like c2 tensor see - since they&#39;re a whole bunch of time so it&#39;s a huge space okay and you know just be clear when I say s this is it Spencer product of individual vectors these are actual words so this could be looks like what you&#39;re doing with your sentences yes yeah yeah so this you know a sentence of length I don&#39;t know whatever big mad dog named Clifford lives in that space so that&#39;s fine okay so just to you know I have tensor network stream diagrams on the board so just to be clear what&#39;s the picture for this this is like a blog within many indices coming out of it okay and then why do I show that picture well because what I now want to do I want some how to sort of massage or tweak a little bit this this big vector basically the sum of all of my samples with these nice leaves I want to tweak this vector I want to do something to it so that I get a new vector in the same space so that is the probability distribution given by these weeks is very very close to the one that I started with so in other words in pictures now I&#39;m gonna sort of give up and a picture picture description of what happens next I start with this vector sign and the idea is you want to do something which I&#39;ll explain in a second you sort of want to take the math ideas we&#39;ve been talking about due to something tweak this vector and get another vector which I&#39;m gonna draw like this okay and it turns out that this vector which I&#39;ll describe in a second this Specter defines a probability distribution which is very close to them when you wanted to start with so the question is what&#39;s going on in this squiggly line so there&#39;s two ways to explain it one is I just it hasn&#39;t it has a name it&#39;s an algorithm from the physics literature and maybe that won&#39;t mean much so then I&#39;ll just tell you within let me go back and say how it relates to what we&#39;ve been talking about so so there&#39;s an algorithm that is called the density matrix renormalization group procedure this has a long name it&#39;s it&#39;s an algorithm that&#39;s used in physics to find the ground state of the Hamiltonian of some quantum system so the Hamiltonian is an operator that tells you something about the energy levels of your system but instead of applying it to a ground state of such an operator we&#39;re just applying it to the some of our data all right so that has a name this is an algorithm that was introduced in the early nineties by Stephen White and you know the paper that introduced it has something like five to six thousand citations so it&#39;s used by real physicists do real physics play frequently so um so so it&#39;s an important idea but it turns out what this algorithm is really doing it&#39;s computing these partial traces oh oh yeah is looking at these partial trace operators it&#39;s recognizing that these off diagonal entries convey meaning about sort of local information together with information that&#39;s not in your system that&#39;s right in the environment but then harnesses that information and then it fills this low ranked tensor approximation of the vector that you started with and turns out this low rate tensor approximation sort of in that compression gets you to the meaningful pieces of your statistics so this is I mean rank another way actually let me so let me explain what I mean let me say what I mean with that um so and I can do that in the minutes remaining by describing kind of what it what is going on here okay great so what&#39;s the idea um the idea is to do exactly what we did you start with this vector sign okay then you project onto it as we said then suppose you know you started work from from you build down the line so you sort of saying what&#39;s going on here in my sentences what&#39;s going on here what&#39;s going on here so what you end up doing is you trace out for example all but the first you know let&#39;s just leave the first two open so if I apply this partial trace operation I&#39;m sort of summing away all of the possible suffixes after sites one and two so this is well it&#39;s just an operator from a tensor product to a tensor product so let me just save some talk and just write it like this so what I mean here is my thrill X I reduced they have the matrix representation here but earlier we saw this has interesting spectral information if I do a spectral decomposition I get well a unitary and diagonal and another unitary like same unitary transpose but because we started with this projection operator the eigenvectors are containing this information that she didn&#39;t have access to constantly and as these eigenvectors they&#39;re being contributed that they&#39;re taking contributions from these off diagonal entries which tell you something about how things here relate to each other based on shared continuations in your language so what you can do you can just sort of take off this tensor and then stick it as one of these so you know maybe this might be this one for instance so you know these two legs are right here and then this third leg over here so when I say low rank I mean I mean maybe I&#39;m working in a very very large space and so I kind of want to you know not deal with all of that maybe there&#39;s a lot of noise and I might get rid of that noise so what you can do is just take maybe the first few largest eigen vectors so I&#39;m sort of compressing and so now I&#39;ve got this low rank operator let&#39;s stick in here yes wondering about the picture right there where you&#39;re tracing out a lot of stuff yes with that correspond to finding shared continuations where the continuations themselves are like all the words except for two but if so like for reason for practically size datasets you would almost never have that right never never have like or actually yeah yeah you would almost never have two sentences being exactly identical except for two words or is that nothing ah that&#39;s right okay so what I&#39;m describing for you ah so let me say two things yes yes so actually this algorithm I&#39;m describing this is an experiment so I&#39;m just driving work with John torilla who&#39;s here from CUNY and miles student Meyer who is for the Flatiron Institute so we trained this on bitstrings of a certain length so it turns out you do get interesting information there but if you sort of want to bump up to something that&#39;s like more real-world ish then you can do things that are like other architecture choices and you can try to take this model to handle things like that so I&#39;m just fairly free sort of like a nice theoretical but if you want to handle you know you want to make it a little more of a bus than there definitely because you can do but these are like different architecture choices yeah so okay so I just ride for you how to get this first tensor but you sort of just like a an iterative thing we sort of keep on going down the line I&#39;m doing this but the punchline is is that at the end this model which by the way it has a meaning that&#39;s called a matrix products pain products eight this is a particular type of tensor network or you know we look at it and maybe say stream diagram and in the end you learn it very well the distribution you wanted to start with and so the model sort of knows what words go together to form meaningful expressions just based on the statistics that you started with and my student wire thank you so much for this amazing talk I wanted to pick up on one of your last couple of words meaningful expression yes so could you comment at all about you know how this is speculated by amongst illuminate the notions of semantics and pragmatics in language because you know culture and you know extractions of emails reflect a certain collection of what&#39;s meaningful but what is meaningful is varies from state to state tongue I mean New Jersey you know ambassadors it&#39;s okay so if you want to get grasp on meaning it would seem like this is a promo shoot the whole thing about meaning is what&#39;s so powerful about this is could you comment yes yes so one looks at this and says ah there must be a good definition of meaning underlying all of this yes and I think a good candidate for meaning is closely related to this distributional earlier which I like it a lot because you see it all the time in mathematics you know thanks to category theory and the UN a dilemma you want to understand an object you know it&#39;s enough to look at its network of relationships that it&#39;s tears okay so then you might think okay so then the meaning of a word like firetruck should be maybe if we&#39;re taking that direction something like the totality of all expressions in which it sits but then not quite because you need something about the statistics as well so then maybe a good candidate for the meaning of something should be not just the totality of all expressions in which it&#39;s it&#39;s sort of how it fits into your language but also the statistics that go along with that are the probabilities of that that&#39;s sort of what&#39;s going on here I&#39;m very open to other ideas as well yeah yeah if one has a good definition of meaning you should be able to do a lot of things with it and so I&#39;d be interested in it well I was wondering what you were doing with T here how you actually used that it was T instead of s yeah well if I empirically just mean a sample set that could have met two s just as well as two T right so I don&#39;t even take a subset that like the probability of a sequence will just be the number of times it appeared in my subset divided by the total number or the cardinality but if it a subset can only they could only appear once I guess okay so maybe it&#39;s just some map T is mapping to s yeah okay great yeah great okay thanks yeah this compares to hidden Markov models because I mean structurally it looks very akin and it would seem that like maybe here we just have continuous belief states and some yeah so because I was a very popular model language yes yes yes yes so I think I think there&#39;s a way to see that something like this is a little bit like the quantum version of it hidden Markov model maybe but I think maybe so the thing I like to think about well so one could make the case that language is not Markovian right and that there is sort of long-range correlations that you&#39;d like your model to know about and so tensor networks since they come from the world of physics and quantum physics are well certain ones are very good at capturing these long-range correlations and so that&#39;s something you know you&#39;d like you&#39;d like that to be built into your model and so what I&#39;ve described for you now is a matrix product state which sort of can be thought of as being a good approximation to and network which can you know recognize that that language doesn&#39;t have this idea of meaning sequences to sequence yes in translation of language where you might have two sequences and they&#39;re translating I don&#39;t have well-developed thoughts on that yeah we should stop here and then um break and then people can go but we&#39;ll figure out you&#39;ll stick around"
      },
      "after": "8b6c60d9316c15fe",
      "date": 1714047205238
    },
    {
      "type": "edit",
      "id": "3006b8012230399e",
      "item": {
        "type": "paragraph",
        "id": "3006b8012230399e",
        "text": "(2:56) so one route you can take to explore this is is a very category category theoretical approach and I&#39;m thinking now of this disco cap model maybe some of you have heard of it some of you haven&#39;t I can briefly describe it so the idea is that well language the meaning of language should be or the meaning of a sentence let&#39;s just take a sentence for example the meaning of a sentence well as determined by the meaning of these individual words that descriptive comprise the symptoms together with this grammatical rules for composing them so you know that you know now I&#39;m followed by a verb that&#39;s a valid thing in English so then what you can do is try to model this grammar together with semantics maybe grammar forms a category like a creeper for example um your semantics these are really like vector embeddings for words"
      },
      "date": 1714047309773
    },
    {
      "type": "edit",
      "id": "ca91dcabbe29f6e1",
      "item": {
        "type": "paragraph",
        "id": "ca91dcabbe29f6e1",
        "text": "(8:53) and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that so so"
      },
      "date": 1714047418906
    },
    {
      "id": "30d6ac922c0daaa8",
      "type": "add",
      "item": {
        "type": "video",
        "id": "30d6ac922c0daaa8",
        "text": "START 53\nYOUTUBE 12j8OV-ptC4"
      },
      "after": "223e7f6fc18f197a",
      "attribution": {
        "page": "scratch"
      },
      "date": 1714047489584
    },
    {
      "type": "edit",
      "id": "30d6ac922c0daaa8",
      "item": {
        "type": "video",
        "id": "30d6ac922c0daaa8",
        "text": "START 532\nYOUTUBE 12j8OV-ptC4\n"
      },
      "date": 1714047524747
    },
    {
      "type": "edit",
      "id": "ca91dcabbe29f6e1",
      "item": {
        "type": "paragraph",
        "id": "ca91dcabbe29f6e1",
        "text": "(8:53) and now I&#39;d like to describe how do you start with something very classical like text and frequency counts or just a probability distribution on a finite set then what is this passage over into this world of quantum probability and then what are the advantages of doing that so this is just mathematics that is sort of stands on its own outside in language but then I want to take those ideas those mathematical ideas and then circle back to my quest for understanding something about meaning and language and what words go with what and how can I model that"
      },
      "date": 1714047573099
    },
    {
      "type": "fork",
      "site": "localhost:3000",
      "date": 1714048007601
    }
  ]
}