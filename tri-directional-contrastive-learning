{
  "title": "Tri-Directional Contrastive Learning",
  "story": [
    {
      "type": "paragraph",
      "id": "35d54c65b024c05b",
      "text": "[[I]]'m Me, [[We]]'re Us, and I'm Us: Tri-directional Contrastive Learning on [[Hypergraph]]s [https://github.com/wooner49/TriCL#im-me-were-us-and-im-us-tri-directional-contrastive-learning-on-hypergraphs github]"
    },
    {
      "type": "paragraph",
      "id": "29675cd10110b8c0",
      "text": "This repository contains the source code for the paper I'm Me, We're Us, and I'm Us: Tri-directional Contrastive Learning on Hypergraphs, by Dongjin Lee and Kijung Shin, presented at AAAI 2023.\n\nIn this paper, we propose TriCL (Tri-directional Contrastive Learning), a general framework for contrastive learning on hypergraphs. Its main idea is tri-directional contrast, and specifically, it aims to maximize in two augmented views the agreement (a) between the same node, (b) between the same group of nodes, and (c) between each group and its members. Together with simple but surprisingly effective data augmentation and negative sampling schemes, these three forms of contrast enable TriCL to capture both microscopic and mesoscopic structural information in node embeddings. Our extensive experiments using 14 baseline approaches, 10 datasets, and two tasks demonstrate the effectiveness of TriCL, and most noticeably, TriCL almost consistently outperforms not just unsupervised competitors but also (semi-)supervised competitors mostly by significant margins for node classification."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Tri-Directional Contrastive Learning",
        "story": []
      },
      "date": 1678876244978
    },
    {
      "item": {
        "type": "factory",
        "id": "35d54c65b024c05b"
      },
      "id": "35d54c65b024c05b",
      "type": "add",
      "date": 1678876246261
    },
    {
      "type": "edit",
      "id": "35d54c65b024c05b",
      "item": {
        "type": "paragraph",
        "id": "35d54c65b024c05b",
        "text": "I'm Me, We're Us, and I'm Us: Tri-directional Contrastive Learning on Hypergraphs"
      },
      "date": 1678876248555
    },
    {
      "type": "edit",
      "id": "35d54c65b024c05b",
      "item": {
        "type": "paragraph",
        "id": "35d54c65b024c05b",
        "text": "I'm Me, We're Us, and I'm Us: Tri-directional Contrastive Learning on Hypergraphs [https://github.com/wooner49/TriCL#im-me-were-us-and-im-us-tri-directional-contrastive-learning-on-hypergraphs github]"
      },
      "date": 1678876265764
    },
    {
      "type": "edit",
      "id": "35d54c65b024c05b",
      "item": {
        "type": "paragraph",
        "id": "35d54c65b024c05b",
        "text": "I'm Me, We're Us, and I'm Us: Tri-directional Contrastive Learning on [[Hypergraph]]s [https://github.com/wooner49/TriCL#im-me-were-us-and-im-us-tri-directional-contrastive-learning-on-hypergraphs github]"
      },
      "date": 1678876276704
    },
    {
      "type": "edit",
      "id": "35d54c65b024c05b",
      "item": {
        "type": "paragraph",
        "id": "35d54c65b024c05b",
        "text": "[[I]]'m Me, [[We]]'re Us, and I'm Us: Tri-directional Contrastive Learning on [[Hypergraph]]s [https://github.com/wooner49/TriCL#im-me-were-us-and-im-us-tri-directional-contrastive-learning-on-hypergraphs github]"
      },
      "date": 1678876293380
    },
    {
      "item": {
        "type": "factory",
        "id": "29675cd10110b8c0"
      },
      "id": "29675cd10110b8c0",
      "type": "add",
      "after": "35d54c65b024c05b",
      "date": 1678876316435
    },
    {
      "type": "edit",
      "id": "29675cd10110b8c0",
      "item": {
        "type": "paragraph",
        "id": "29675cd10110b8c0",
        "text": "This repository contains the source code for the paper I'm Me, We're Us, and I'm Us: Tri-directional Contrastive Learning on Hypergraphs, by Dongjin Lee and Kijung Shin, presented at AAAI 2023.\n\nIn this paper, we propose TriCL (Tri-directional Contrastive Learning), a general framework for contrastive learning on hypergraphs. Its main idea is tri-directional contrast, and specifically, it aims to maximize in two augmented views the agreement (a) between the same node, (b) between the same group of nodes, and (c) between each group and its members. Together with simple but surprisingly effective data augmentation and negative sampling schemes, these three forms of contrast enable TriCL to capture both microscopic and mesoscopic structural information in node embeddings. Our extensive experiments using 14 baseline approaches, 10 datasets, and two tasks demonstrate the effectiveness of TriCL, and most noticeably, TriCL almost consistently outperforms not just unsupervised competitors but also (semi-)supervised competitors mostly by significant margins for node classification."
      },
      "date": 1678876317575
    }
  ]
}