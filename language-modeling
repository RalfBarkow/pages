{
  "title": "Language Modeling",
  "story": [
    {
      "type": "paragraph",
      "id": "54bc8a5601734f25",
      "text": "“The simplest kind of language model is: I’m going to read a bunch of words and then try to predict the next word,” explained [[Myle Ott]], a research scientist at Facebook. “If I say, ‘George Bush was born in,’ the model now has to predict the next word in that sentence.”"
    },
    {
      "type": "paragraph",
      "id": "4c5a18f09f66a58b",
      "text": "These deep pretrained language models could be produced relatively efficiently. Researchers simply fed their neural networks massive amounts of written text copied from freely available sources like Wikipedia — billions of words, preformatted into grammatically correct sentences — and let the networks derive next-word predictions on their own. In essence, it was like asking the person inside a Chinese room to write all his own rules, using only the incoming Chinese messages for reference."
    },
    {
      "type": "paragraph",
      "id": "e7155357a6e54873",
      "text": "“The great thing about this approach is it turns out that the model learns a ton of stuff about syntax,” Ott said.\n\nWhat’s more, these pretrained neural networks could then apply their richer representations of language to the job of learning an unrelated, more specific NLP task, a process called [[fine-tuning]]."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Language Modeling",
        "story": []
      },
      "date": 1650972552632
    },
    {
      "item": {
        "type": "factory",
        "id": "54bc8a5601734f25"
      },
      "id": "54bc8a5601734f25",
      "type": "add",
      "date": 1650972610244
    },
    {
      "type": "edit",
      "id": "54bc8a5601734f25",
      "item": {
        "type": "paragraph",
        "id": "54bc8a5601734f25",
        "text": "“The simplest kind of language model is: I’m going to read a bunch of words and then try to predict the next word,” explained Myle Ott, a research scientist at Facebook. “If I say, ‘George Bush was born in,’ the model now has to predict the next word in that sentence.”"
      },
      "date": 1650972612100
    },
    {
      "type": "edit",
      "id": "54bc8a5601734f25",
      "item": {
        "type": "paragraph",
        "id": "54bc8a5601734f25",
        "text": "“The simplest kind of language model is: I’m going to read a bunch of words and then try to predict the next word,” explained [[Myle Ott]], a research scientist at Facebook. “If I say, ‘George Bush was born in,’ the model now has to predict the next word in that sentence.”"
      },
      "date": 1650972656207
    },
    {
      "item": {
        "type": "factory",
        "id": "4c5a18f09f66a58b"
      },
      "id": "4c5a18f09f66a58b",
      "type": "add",
      "after": "54bc8a5601734f25",
      "date": 1650972679973
    },
    {
      "type": "edit",
      "id": "4c5a18f09f66a58b",
      "item": {
        "type": "paragraph",
        "id": "4c5a18f09f66a58b",
        "text": "These deep pretrained language models could be produced relatively efficiently. Researchers simply fed their neural networks massive amounts of written text copied from freely available sources like Wikipedia — billions of words, preformatted into grammatically correct sentences — and let the networks derive next-word predictions on their own. In essence, it was like asking the person inside a Chinese room to write all his own rules, using only the incoming Chinese messages for reference."
      },
      "date": 1650972681264
    },
    {
      "item": {
        "type": "factory",
        "id": "e7155357a6e54873"
      },
      "id": "e7155357a6e54873",
      "type": "add",
      "after": "4c5a18f09f66a58b",
      "date": 1650972704981
    },
    {
      "type": "edit",
      "id": "e7155357a6e54873",
      "item": {
        "type": "paragraph",
        "id": "e7155357a6e54873",
        "text": "“The great thing about this approach is it turns out that the model learns a ton of stuff about syntax,” Ott said.\n\nWhat’s more, these pretrained neural networks could then apply their richer representations of language to the job of learning an unrelated, more specific NLP task, a process called [[fine-tuning]]."
      },
      "date": 1650972714414
    }
  ]
}