{
  "title": "Hallucination",
  "story": [
    {
      "type": "paragraph",
      "id": "eaa1eabf9dbe461f",
      "text": "Can we get strong guarantees from AI tools that are known to hallucinate? We discuss some strategies, and ways that [[Elm]] might be a great target for AI assistance."
    },
    {
      "type": "audio",
      "id": "0c7f5903c3db90c2",
      "text": "https://cdn.simplecast.com/audio/6a206baa-9c8e-4c25-9037-2b674204ba84/episodes/d1c5f97c-9700-48b0-ab35-a039edbfd0d5/audio/16dc506d-5aa1-42c1-8838-9ffaa3e0e1e9/default_tc.mp3\nelm radio – 080: Elm and AI [https://elm-radio.com/episode/elm-and-ai/ page]"
    },
    {
      "type": "paragraph",
      "id": "e76429a5dc2bb4e3",
      "text": "[00:07:55]\nHallucination being when it says something and it thinks [sic!] it's right. […] and hallucination is like sort of the technical term that open AI is using in some of these white papers […].\n"
    },
    {
      "type": "paragraph",
      "id": "041492b1afe8ad45",
      "text": "[00:08:14]\nBut hallucination, it's very prone to hallucination because these are sort of predictive models that kind of synthesize information, but it's not an exact science. And sometimes it mixes things together that don't quite fit. And so I think, I mean, Jeroen, I think it's fair to say that we really like having tools that we can trust."
    },
    {
      "type": "paragraph",
      "id": "78911dacf0741aa7",
      "text": "⇒ [[Guarantee]]"
    },
    {
      "type": "pagefold",
      "id": "0f175d8f3638118f",
      "text": "~"
    },
    {
      "type": "markdown",
      "id": "784a606062812ffe",
      "text": "hallucination | BrE həˌluːsɪˈneɪʃ(ə)n, AmE həˌlusəˈneɪʃ(ə)n |\nnoun\n(act) Halluzinieren (Neutr.) (instance, imagined object) Halluzination (Fem.), Sinnestäuschung (Fem.)"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Hallucination",
        "story": []
      },
      "date": 1681920013039
    },
    {
      "item": {
        "type": "factory",
        "id": "e76429a5dc2bb4e3"
      },
      "id": "e76429a5dc2bb4e3",
      "type": "add",
      "date": 1681920015105
    },
    {
      "type": "edit",
      "id": "e76429a5dc2bb4e3",
      "item": {
        "type": "paragraph",
        "id": "e76429a5dc2bb4e3",
        "text": "[00:07:55]\nHallucination being when it says something and it thinks it's right.\n[00:08:01]\nExactly.\n[00:08:02]\nAnd when it isn't, when it isn't.\n[00:08:03]\nYes, because these... and hallucination is like sort of the technical term that open\n[00:08:09]\nAI is using in some of these white papers talking about this and stuff nowadays.\n[00:08:14]\nBut hallucination, it's very prone to hallucination because these are sort of predictive models\n[00:08:20]\nthat kind of synthesize information, but it's not an exact science.\n[00:08:26]\nAnd sometimes it mixes things together that don't quite fit.\n[00:08:30]\nAnd so I think, I mean, Jeroen, I think it's fair to say that we really like having tools\n[00:08:36]\nthat we can trust."
      },
      "date": 1681920020504
    },
    {
      "item": {
        "type": "factory",
        "id": "0f175d8f3638118f"
      },
      "id": "0f175d8f3638118f",
      "type": "add",
      "after": "e76429a5dc2bb4e3",
      "date": 1681920049150
    },
    {
      "type": "edit",
      "id": "0f175d8f3638118f",
      "item": {
        "type": "pagefold",
        "id": "0f175d8f3638118f",
        "text": "~"
      },
      "date": 1681920052857
    },
    {
      "item": {
        "type": "factory",
        "id": "d99133cc0ceb9f3a"
      },
      "id": "d99133cc0ceb9f3a",
      "type": "add",
      "after": "0f175d8f3638118f",
      "date": 1681920054371
    },
    {
      "type": "edit",
      "id": "d99133cc0ceb9f3a",
      "item": {
        "type": "paragraph",
        "id": "d99133cc0ceb9f3a",
        "text": "hallucination | BrE həˌluːsɪˈneɪʃ(ə)n, AmE həˌlusəˈneɪʃ(ə)n |\nnoun\n(act) Halluzinieren (Neutr.) (instance, imagined object) Halluzination (Fem.), Sinnestäuschung (Fem.)"
      },
      "date": 1681920056593
    },
    {
      "id": "eaa1eabf9dbe461f",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "eaa1eabf9dbe461f",
        "text": "Can we get strong guarantees from AI tools that are known to hallucinate? We discuss some strategies, and ways that [[Elm]] might be a great target for AI assistance."
      },
      "attribution": {
        "page": "Elm and AI"
      },
      "date": 1681920086604
    },
    {
      "id": "0c7f5903c3db90c2",
      "type": "add",
      "item": {
        "type": "audio",
        "id": "0c7f5903c3db90c2",
        "text": "https://cdn.simplecast.com/audio/6a206baa-9c8e-4c25-9037-2b674204ba84/episodes/d1c5f97c-9700-48b0-ab35-a039edbfd0d5/audio/16dc506d-5aa1-42c1-8838-9ffaa3e0e1e9/default_tc.mp3\nelm radio – 080: Elm and AI [https://elm-radio.com/episode/elm-and-ai/ page]"
      },
      "after": "eaa1eabf9dbe461f",
      "attribution": {
        "page": "Elm and AI"
      },
      "date": 1681920089147
    },
    {
      "type": "edit",
      "id": "e76429a5dc2bb4e3",
      "item": {
        "type": "paragraph",
        "id": "e76429a5dc2bb4e3",
        "text": "[00:07:55]\nHallucination being when it says something and it thinks it's right.\n[00:08:01]\nExactly.\n[00:08:02]\nAnd when it isn't, when it isn't.\n[00:08:03]\nYes, because these... and hallucination is like sort of the technical term that open\n[00:08:09]\nAI is using in some of these white papers talking about this and stuff nowadays.\n"
      },
      "date": 1681920245130
    },
    {
      "type": "add",
      "id": "041492b1afe8ad45",
      "item": {
        "type": "paragraph",
        "id": "041492b1afe8ad45",
        "text": "[00:08:14]\nBut hallucination, it's very prone to hallucination because these are sort of predictive models\n[00:08:20]\nthat kind of synthesize information, but it's not an exact science.\n[00:08:26]\nAnd sometimes it mixes things together that don't quite fit.\n[00:08:30]\nAnd so I think, I mean, Jeroen, I think it's fair to say that we really like having tools\n[00:08:36]\nthat we can trust."
      },
      "after": "e76429a5dc2bb4e3",
      "date": 1681920245688
    },
    {
      "item": {
        "type": "factory",
        "id": "78911dacf0741aa7"
      },
      "id": "78911dacf0741aa7",
      "type": "add",
      "after": "d99133cc0ceb9f3a",
      "date": 1681920486952
    },
    {
      "type": "edit",
      "id": "78911dacf0741aa7",
      "item": {
        "type": "paragraph",
        "id": "78911dacf0741aa7",
        "text": "guarantee"
      },
      "date": 1681920493047
    },
    {
      "type": "edit",
      "id": "78911dacf0741aa7",
      "item": {
        "type": "paragraph",
        "id": "78911dacf0741aa7",
        "text": "⇒ [[Guarantee]]"
      },
      "date": 1681920556992
    },
    {
      "id": "78911dacf0741aa7",
      "type": "move",
      "order": [
        "eaa1eabf9dbe461f",
        "0c7f5903c3db90c2",
        "e76429a5dc2bb4e3",
        "041492b1afe8ad45",
        "78911dacf0741aa7",
        "0f175d8f3638118f",
        "d99133cc0ceb9f3a"
      ],
      "date": 1681920559430
    },
    {
      "type": "edit",
      "id": "041492b1afe8ad45",
      "item": {
        "type": "paragraph",
        "id": "041492b1afe8ad45",
        "text": "[00:08:14]\nBut hallucination, it's very prone to hallucination because these are sort of predictive models that kind of synthesize information, but it's not an exact science. And sometimes it mixes things together that don't quite fit. And so I think, I mean, Jeroen, I think it's fair to say that we really like having tools that we can trust."
      },
      "date": 1685050306139
    },
    {
      "type": "edit",
      "id": "e76429a5dc2bb4e3",
      "item": {
        "type": "paragraph",
        "id": "e76429a5dc2bb4e3",
        "text": "[00:07:55]\nHallucination being when it says something and it thinks it's right.\n\nExactly.\n\nAnd when it isn't, when it isn't.\n\nYes, because these... and hallucination is like sort of the technical term that open AI is using in some of these white papers talking about this and stuff nowadays.\n"
      },
      "date": 1685053264813
    },
    {
      "type": "edit",
      "id": "e76429a5dc2bb4e3",
      "item": {
        "type": "paragraph",
        "id": "e76429a5dc2bb4e3",
        "text": "[00:07:55]\nHallucination being when it says something and it thinks it's right. […] and hallucination is like sort of the technical term that open AI is using in some of these white papers talking about this and stuff nowadays.\n"
      },
      "date": 1685053300240
    },
    {
      "type": "edit",
      "id": "e76429a5dc2bb4e3",
      "item": {
        "type": "paragraph",
        "id": "e76429a5dc2bb4e3",
        "text": "[00:07:55]\nHallucination being when it says something and it thinks [sic!] it's right. […] and hallucination is like sort of the technical term that open AI is using in some of these white papers […].\n"
      },
      "date": 1685053381786
    },
    {
      "type": "remove",
      "id": "d99133cc0ceb9f3a",
      "date": 1685053458213
    },
    {
      "item": {
        "type": "factory",
        "id": "784a606062812ffe"
      },
      "id": "784a606062812ffe",
      "type": "add",
      "after": "0f175d8f3638118f",
      "date": 1685053459445
    },
    {
      "type": "edit",
      "id": "784a606062812ffe",
      "item": {
        "type": "markdown",
        "id": "784a606062812ffe",
        "text": "hallucination | BrE həˌluːsɪˈneɪʃ(ə)n, AmE həˌlusəˈneɪʃ(ə)n |\nnoun\n(act) Halluzinieren (Neutr.) (instance, imagined object) Halluzination (Fem.), Sinnestäuschung (Fem.)"
      },
      "date": 1685053461148
    }
  ]
}