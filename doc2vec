{
  "title": "Doc2Vec",
  "story": [
    {
      "type": "paragraph",
      "id": "4756d193952d5af3",
      "text": "In order to directly represent documents, Le and Mikolov [2014] introduced Doc2Vec, where every paragraph has a unique vector in the matrix D and every word has its own vector in matrix W (same local context architecture as Word2Vec). These vectors are averaged and combined to predict the next word in the context in a given paragraph."
    },
    {
      "type": "pagefold",
      "id": "927e5c9be70c9308",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "e87d9c58809ccc6c",
      "text": "ELSAFTY, Ahmed, 2017. Document Similarity using Dense Vector Representation. [https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2017-ma-elsafty.pdf pdf], p. 16.\n"
    },
    {
      "type": "paragraph",
      "id": "1b10e287cc6b7645",
      "text": "The [[Paragraph Vector]] is only shared among words of the same paragraph. It can be represented as another word in the context that is fixed for all sentences and windows in the paragraph. Hence it preserves (or memorize) the topic of the paragraph. That’s where the architecture name got its name \"Distributed Memory\" shown in figure 2.5."
    },
    {
      "type": "image",
      "id": "0bc7dad970a16672",
      "text": "Figure 2.5: Doc2Vec Distributed Memory model uses the paragraph vector along with the local context words to predict the word w(t), it also acts as a memory of the paragraph’s topic [Le and Mikolov 2014]",
      "size": "wide",
      "width": 418,
      "height": 264,
      "url": "/assets/plugins/image/b2252d263f417983d230aa264379d17e.jpg"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Doc2Vec",
        "story": []
      },
      "date": 1710946003905
    },
    {
      "item": {
        "type": "factory",
        "id": "4756d193952d5af3"
      },
      "id": "4756d193952d5af3",
      "type": "add",
      "date": 1710946005439
    },
    {
      "type": "edit",
      "id": "4756d193952d5af3",
      "item": {
        "type": "paragraph",
        "id": "4756d193952d5af3",
        "text": "In order to directly represent documents, Le and Mikolov [2014] introduced Doc2Vec, where every paragraph has a unique vector in the matrix D and every word has its own vector in matrix W (same local context architecture as Word2Vec). These vectors are averaged and combined to predict the next word in the context in a given paragraph."
      },
      "date": 1710946008518
    },
    {
      "id": "927e5c9be70c9308",
      "type": "add",
      "item": {
        "type": "pagefold",
        "id": "927e5c9be70c9308",
        "text": "~"
      },
      "after": "4756d193952d5af3",
      "attribution": {
        "page": "Recommender System"
      },
      "date": 1710946012177
    },
    {
      "id": "e87d9c58809ccc6c",
      "type": "add",
      "item": {
        "type": "paragraph",
        "id": "e87d9c58809ccc6c",
        "text": "ELSAFTY, Ahmed, 2017. Document Similarity using Dense Vector Representation. [https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2017-ma-elsafty.pdf pdf]\n"
      },
      "after": "927e5c9be70c9308",
      "attribution": {
        "page": "Recommender System"
      },
      "date": 1710946017903
    },
    {
      "type": "edit",
      "id": "e87d9c58809ccc6c",
      "item": {
        "type": "paragraph",
        "id": "e87d9c58809ccc6c",
        "text": "ELSAFTY, Ahmed, 2017. Document Similarity using Dense Vector Representation. [https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2017-ma-elsafty.pdf pdf], p. 16.\n"
      },
      "date": 1710946028580
    },
    {
      "item": {
        "type": "factory",
        "id": "1b10e287cc6b7645"
      },
      "id": "1b10e287cc6b7645",
      "type": "add",
      "after": "e87d9c58809ccc6c",
      "date": 1710946055133
    },
    {
      "type": "edit",
      "id": "1b10e287cc6b7645",
      "item": {
        "type": "paragraph",
        "id": "1b10e287cc6b7645",
        "text": "The [[Paragraph Vector]] is only shared among words of the same paragraph. It can be represented as another word in the context that is fixed for all sentences and windows in the paragraph. Hence it preserves (or memorize) the topic of the paragraph. That’s where the architecture name got its name \"Distributed Memory\" shown in figure 2.5."
      },
      "date": 1710946068099
    },
    {
      "id": "0bc7dad970a16672",
      "type": "add",
      "item": {
        "type": "image",
        "id": "0bc7dad970a16672",
        "text": "Figure 2. A framework for learning paragraph vector. ",
        "size": "wide",
        "width": 418,
        "height": 264,
        "url": "/assets/plugins/image/b2252d263f417983d230aa264379d17e.jpg"
      },
      "after": "1b10e287cc6b7645",
      "attribution": {
        "page": "Paragraph Vector"
      },
      "date": 1710946086179
    },
    {
      "type": "edit",
      "id": "0bc7dad970a16672",
      "item": {
        "type": "image",
        "id": "0bc7dad970a16672",
        "text": "Figure 2.5: Doc2Vec Distributed Memory model uses the paragraph vector along with the local context words to predict the word w(t), it also acts as a memory of the paragraph’s topic [Le and Mikolov 2014]",
        "size": "wide",
        "width": 418,
        "height": 264,
        "url": "/assets/plugins/image/b2252d263f417983d230aa264379d17e.jpg"
      },
      "date": 1710946105264
    },
    {
      "type": "fork",
      "site": "localhost:3000",
      "date": 1710946462599
    },
    {
      "type": "fork",
      "site": "wiki.ralfbarkow.ch",
      "date": 1711030723698
    }
  ]
}