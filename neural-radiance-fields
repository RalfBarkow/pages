{
  "title": "Neural Radiance Fields",
  "story": [
    {
      "type": "paragraph",
      "id": "b8a8683907eab617",
      "text": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views.\n[https://www.matthewtancik.com/nerf post] [https://github.com/bmild/nerf github]"
    },
    {
      "type": "video",
      "id": "66aee0d40001ad02",
      "text": "YOUTUBE JuH79E8rdKc\nPublished Mar 19, 2020."
    },
    {
      "type": "paragraph",
      "id": "1e148e9d04a3a97c",
      "text": "We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. "
    },
    {
      "type": "html",
      "id": "d52f3649865e1632",
      "text": "<img width=100% src=https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01.png> "
    },
    {
      "type": "paragraph",
      "id": "a5ae93f0e133c36e",
      "text": "By efficiently rendering anti-aliased conical frustums instead of rays, our followup, mip-NeRF, reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size."
    },
    {
      "type": "html",
      "id": "ed9f8570c8be9289",
      "text": "<img width=100% src=https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/6086e97ec8800aae5cc3c7c0_rays.jpg>"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Neural Radiance Fields",
        "story": []
      },
      "date": 1629578442989
    },
    {
      "item": {
        "type": "factory",
        "id": "b8a8683907eab617"
      },
      "id": "b8a8683907eab617",
      "type": "add",
      "date": 1629578493685
    },
    {
      "type": "edit",
      "id": "b8a8683907eab617",
      "item": {
        "type": "paragraph",
        "id": "b8a8683907eab617",
        "text": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views.\n\n"
      },
      "date": 1629578497726
    },
    {
      "type": "edit",
      "id": "b8a8683907eab617",
      "item": {
        "type": "paragraph",
        "id": "b8a8683907eab617",
        "text": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views.\n[https://www.matthewtancik.com/nerf post]"
      },
      "date": 1629578515497
    },
    {
      "item": {
        "type": "factory",
        "id": "66aee0d40001ad02"
      },
      "id": "66aee0d40001ad02",
      "type": "add",
      "after": "b8a8683907eab617",
      "date": 1629578519114
    },
    {
      "type": "edit",
      "id": "66aee0d40001ad02",
      "item": {
        "type": "video",
        "id": "66aee0d40001ad02",
        "text": "YOUTUBE JuH79E8rdKc\n(double-click to edit caption)\n"
      },
      "date": 1629578527974
    },
    {
      "type": "edit",
      "id": "66aee0d40001ad02",
      "item": {
        "type": "video",
        "id": "66aee0d40001ad02",
        "text": "YOUTUBE JuH79E8rdKc\nPublished Mar 19, 2020."
      },
      "date": 1629578553410
    },
    {
      "item": {
        "type": "factory",
        "id": "d52f3649865e1632"
      },
      "id": "d52f3649865e1632",
      "type": "add",
      "after": "66aee0d40001ad02",
      "date": 1629578605238
    },
    {
      "type": "edit",
      "id": "d52f3649865e1632",
      "item": {
        "type": "factory",
        "id": "d52f3649865e1632",
        "text": "<img width=100% src=https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01.png>"
      },
      "date": 1629578629033
    },
    {
      "type": "edit",
      "id": "d52f3649865e1632",
      "item": {
        "type": "html",
        "id": "d52f3649865e1632",
        "text": "<img width=100% src=https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01.png> "
      },
      "date": 1629578633767
    },
    {
      "type": "add",
      "id": "1e148e9d04a3a97c",
      "item": {
        "type": "paragraph",
        "id": "1e148e9d04a3a97c",
        "text": "We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis."
      },
      "after": "b8a8683907eab617",
      "date": 1629578676619
    },
    {
      "id": "1e148e9d04a3a97c",
      "type": "move",
      "order": [
        "b8a8683907eab617",
        "66aee0d40001ad02",
        "1e148e9d04a3a97c",
        "d52f3649865e1632"
      ],
      "date": 1629578679478
    },
    {
      "type": "edit",
      "id": "1e148e9d04a3a97c",
      "item": {
        "type": "paragraph",
        "id": "1e148e9d04a3a97c",
        "text": "We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. "
      },
      "date": 1629578709011
    },
    {
      "item": {
        "type": "factory",
        "id": "a5ae93f0e133c36e"
      },
      "id": "a5ae93f0e133c36e",
      "type": "add",
      "after": "d52f3649865e1632",
      "date": 1629578796123
    },
    {
      "type": "edit",
      "id": "a5ae93f0e133c36e",
      "item": {
        "type": "paragraph",
        "id": "a5ae93f0e133c36e",
        "text": "By efficiently rendering anti-aliased conical frustums instead of rays, our followup, mip-NeRF, reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size."
      },
      "date": 1629578799452
    },
    {
      "item": {
        "type": "factory",
        "id": "ed9f8570c8be9289"
      },
      "id": "ed9f8570c8be9289",
      "type": "add",
      "after": "a5ae93f0e133c36e",
      "date": 1629578814924
    },
    {
      "type": "edit",
      "id": "ed9f8570c8be9289",
      "item": {
        "type": "html",
        "id": "ed9f8570c8be9289",
        "text": "<img width=100% src=https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/6086e97ec8800aae5cc3c7c0_rays.jpg>"
      },
      "date": 1629578834541
    },
    {
      "type": "edit",
      "id": "b8a8683907eab617",
      "item": {
        "type": "paragraph",
        "id": "b8a8683907eab617",
        "text": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views.\n[https://www.matthewtancik.com/nerf post] [https://github.com/bmild/nerf github]"
      },
      "date": 1629579258279
    },
    {
      "type": "fork",
      "site": "found.ward.bay.wiki.org",
      "date": 1629586476445
    },
    {
      "type": "fork",
      "site": "don.noyes.asia.wiki.org",
      "date": 1630100566829
    }
  ]
}