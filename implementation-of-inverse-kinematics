{
  "title": "Implementation of Inverse Kinematics",
  "story": [
    {
      "type": "paragraph",
      "id": "513b42827b81b157",
      "text": "One of the important and core aspects of the application is the movement animation of the robot, with this the user will observe the behavior of the robot according to the points on which the robot is positioned and trajectories that it is expected to perform. Mentioned previously is the robot’s kinematics which is implemented in a programming script which is executed continuously, and varies the values of each articulation."
    },
    {
      "type": "paragraph",
      "id": "cf315c7e4dec28b4",
      "text": "From the observed coordinate system (See Fig. 4), The orientation of the final effector is determined, reflected in the virtual environment with the orientation [[Gizmo]] located in the end effector of the robot."
    },
    {
      "type": "paragraph",
      "id": "3624cd27aa62d6b8",
      "text": "Its kinematics is constructed as a C# script in Unity 3D that is executed in a cyclical way each visualization frame, obtained its mathematical calculation of [6]. The need to find the values in degrees of each joint according to the objective point of the final effector is supplied from the following algorithm constructed to give a solution to the inverse kinematics of the robot."
    },
    {
      "type": "image",
      "id": "5bea80976c986858",
      "text": "Fig. 4. Robot coordinate system. (a) Coordinates of articulations, (b) Gizmo of orientation in Robot.",
      "size": "wide",
      "width": 416,
      "height": 232,
      "url": "/assets/plugins/image/854fa9847bf1c6a3b50dcae623b89e95.jpg"
    },
    {
      "type": "image",
      "id": "aaf6312f497d84e4",
      "text": "Algorithm 1 Implementation of inverse kinematics",
      "size": "wide",
      "width": 416,
      "height": 277,
      "url": "/assets/plugins/image/b5ffce9a131e17b5bb9ebdbc7ffd6f49.jpg"
    },
    {
      "type": "markdown",
      "id": "264a930642cc2877",
      "text": "# Results"
    },
    {
      "type": "paragraph",
      "id": "1a4e17471550f089",
      "text": "By integrating the virtual environment developed with the peripherals used by the user, the main result is an interface for a software application that implements virtual reality technology, its main function is to be a programming tool for the Yaskawa robot. motoman HP20D, as well as providing ease to people without deep knowledge in programming of manipulators (See Fig. 5)."
    },
    {
      "type": "image",
      "id": "539e61f9bf0887c7",
      "text": "Fig. 5. User in interaction with the programming tool",
      "size": "wide",
      "width": 416,
      "height": 211,
      "url": "/assets/plugins/image/a368ed22d5000d32ad6e4048d40cbd52.jpg"
    },
    {
      "type": "paragraph",
      "id": "b653c92587139f3c",
      "text": "And as previously described, the hand gestures of the user allow to activate the operation on the end effector and the storage of desired points, these are the fist of the hand and the opening of the palm out determined by code. Respectively."
    },
    {
      "type": "paragraph",
      "id": "dad2972e9daa81e0",
      "text": "The procedure to evaluate the programming tool is create a task with a tool on the final effector at the virtual environment. It was observed in the tool that a successful linear trajectory is constructed in suitable conditions to be evaluated in the real robot and to be able to make a comparison (See Fig. 6)."
    },
    {
      "type": "paragraph",
      "id": "4d5a7de6123191cf",
      "text": "[…] Fig. 6. Performing task of robot with a marker tool."
    },
    {
      "type": "paragraph",
      "id": "e09b67696da3aa55",
      "text": "Then it have to play the same trajectory with the real robot. In the case of the trajectory executed in real robot it is concluded that it fulfills what was expected, it must enter to evaluate the details of the position errors (See Fig. 7)."
    },
    {
      "type": "paragraph",
      "id": "0a74c775e3f93702",
      "text": "[…] Fig. 7. Evaluating the created task in the real robot and their work environment."
    },
    {
      "type": "markdown",
      "id": "ba386944151ae948",
      "text": "# Conclusions and Future Work"
    },
    {
      "type": "paragraph",
      "id": "fbe53be8021d129a",
      "text": "The integration of the virtual environment and the positioning tool was successfully carried out. Although the result of characterizing the MYO Armband sensor was not as expected, an alternative interaction to the keyboard and mouse of a computer is achieved, with an unconventional tool that provides an interface with additional features in the construction of a concept of user immersion in the virtual reality environment. The usual techniques to define the tasks of a robot are to write machine code interpreted by the controller of the robot, but with this, we seek to contribute to the theory that the method of teaching the trajectory to robot in a way guided by the gestures of a user programmer could be quickly and flexible."
    },
    {
      "type": "paragraph",
      "id": "5259b70b8c7314f2",
      "text": "The developed interface manages to shorten robot manipulation procedures, in relation to the preparation of trajectories that are programmed point-to-point in the robot’s teach pendant, compared with the naturalness of moving the end effector of the robot with gestures and rotating movements of the robot. Hand and forearm respectively. In projects that want to give continuity to this research, apply in new developments the alternative of creating trajectories with curvilinear interpolation, as well as including the same task in the generation of the machine code for its later execution in the real robot. Also, open the possibility of directly connecting the application to the control of the robot online and approach real-time execution."
    },
    {
      "type": "paragraph",
      "id": "408b56c7067ee764",
      "text": "Associating this development with the educational approach, the results presented with this interface can be related to projects that seek to give importance to the application 11 of virtual reality in the teaching of robotics as shown in the work in [5], which an application concludes a high potential as a learning and practice tool. This scope can be applied in the same way in this project, since the virtual environments give the flexibility of implementation and execution."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Implementation of Inverse Kinematics",
        "story": []
      },
      "date": 1676567638191
    },
    {
      "item": {
        "type": "factory",
        "id": "513b42827b81b157"
      },
      "id": "513b42827b81b157",
      "type": "add",
      "date": 1676567639671
    },
    {
      "type": "edit",
      "id": "513b42827b81b157",
      "item": {
        "type": "paragraph",
        "id": "513b42827b81b157",
        "text": "One of the important and core aspects of the application is the movement animation of the robot, with this the user will observe the behavior of the robot according to the points on which the robot is positioned and trajectories that it is expected to perform. Mentioned previously is the robot’s kinematics which is implemented in a programming script which is executed continuously, and varies the values of each articulation."
      },
      "date": 1676567641196
    },
    {
      "item": {
        "type": "factory",
        "id": "cf315c7e4dec28b4"
      },
      "id": "cf315c7e4dec28b4",
      "type": "add",
      "after": "513b42827b81b157",
      "date": 1676567649534
    },
    {
      "type": "edit",
      "id": "cf315c7e4dec28b4",
      "item": {
        "type": "paragraph",
        "id": "cf315c7e4dec28b4",
        "text": "From the observed coordinate system (See Fig. 4), The orientation of the final effector is determined, reflected in the virtual environment with the orientation gizmo located in the end effector of the robot."
      },
      "date": 1676567650968
    },
    {
      "item": {
        "type": "factory",
        "id": "3624cd27aa62d6b8"
      },
      "id": "3624cd27aa62d6b8",
      "type": "add",
      "after": "cf315c7e4dec28b4",
      "date": 1676567658298
    },
    {
      "type": "edit",
      "id": "3624cd27aa62d6b8",
      "item": {
        "type": "paragraph",
        "id": "3624cd27aa62d6b8",
        "text": "Its kinematics is constructed as a C# script in Unity 3D that is executed in a cyclical way each visualization frame, obtained its mathematical calculation of [6]. The need to find the values in degrees of each joint according to the objective point of the final effector is supplied from the following algorithm constructed to give a solution to the inverse kinematics of the robot."
      },
      "date": 1676567659621
    },
    {
      "item": {
        "type": "factory",
        "id": "5bea80976c986858"
      },
      "id": "5bea80976c986858",
      "type": "add",
      "after": "3624cd27aa62d6b8",
      "date": 1676567687501
    },
    {
      "type": "edit",
      "id": "5bea80976c986858",
      "item": {
        "type": "image",
        "id": "5bea80976c986858",
        "text": "Fig. 4. Robot coordinate system. (a) Coordinates of articulations, (b) Gizmo of orientation in Robot.",
        "size": "wide",
        "width": 416,
        "height": 232,
        "url": "/assets/plugins/image/854fa9847bf1c6a3b50dcae623b89e95.jpg"
      },
      "date": 1676567700949
    },
    {
      "item": {
        "type": "factory",
        "id": "aaf6312f497d84e4"
      },
      "id": "aaf6312f497d84e4",
      "type": "add",
      "after": "5bea80976c986858",
      "date": 1676567733019
    },
    {
      "type": "edit",
      "id": "aaf6312f497d84e4",
      "item": {
        "type": "image",
        "id": "aaf6312f497d84e4",
        "text": "Uploaded image",
        "size": "wide",
        "width": 416,
        "height": 277,
        "url": "/assets/plugins/image/b5ffce9a131e17b5bb9ebdbc7ffd6f49.jpg"
      },
      "date": 1676567767799
    },
    {
      "type": "edit",
      "id": "aaf6312f497d84e4",
      "item": {
        "type": "image",
        "id": "aaf6312f497d84e4",
        "text": "Algorithm 1 Implementation of inverse kinematics",
        "size": "wide",
        "width": 416,
        "height": 277,
        "url": "/assets/plugins/image/b5ffce9a131e17b5bb9ebdbc7ffd6f49.jpg"
      },
      "date": 1676567793565
    },
    {
      "item": {
        "type": "factory",
        "id": "264a930642cc2877"
      },
      "id": "264a930642cc2877",
      "type": "add",
      "after": "aaf6312f497d84e4",
      "date": 1676567809361
    },
    {
      "type": "edit",
      "id": "264a930642cc2877",
      "item": {
        "type": "paragraph",
        "id": "264a930642cc2877",
        "text": "# Results"
      },
      "date": 1676567816896
    },
    {
      "type": "edit",
      "id": "264a930642cc2877",
      "item": {
        "type": "markdown",
        "id": "264a930642cc2877",
        "text": "# Results"
      },
      "date": 1676567818531
    },
    {
      "item": {
        "type": "factory",
        "id": "1a4e17471550f089"
      },
      "id": "1a4e17471550f089",
      "type": "add",
      "after": "264a930642cc2877",
      "date": 1676567825357
    },
    {
      "type": "edit",
      "id": "1a4e17471550f089",
      "item": {
        "type": "paragraph",
        "id": "1a4e17471550f089",
        "text": "By integrating the virtual environment developed with the peripherals used by the user, the main result is an interface for a software application that implements virtual reality technology, its main function is to be a programming tool for the Yaskawa robot. motoman HP20D, as well as providing ease to people without deep knowledge in programming of manipulators (See Fig. 5)."
      },
      "date": 1676567826881
    },
    {
      "item": {
        "type": "factory",
        "id": "539e61f9bf0887c7"
      },
      "id": "539e61f9bf0887c7",
      "type": "add",
      "after": "1a4e17471550f089",
      "date": 1676567884024
    },
    {
      "type": "edit",
      "id": "539e61f9bf0887c7",
      "item": {
        "type": "image",
        "id": "539e61f9bf0887c7",
        "text": "Fig. 5. User in interaction with the programming tool",
        "size": "wide",
        "width": 416,
        "height": 211,
        "url": "/assets/plugins/image/a368ed22d5000d32ad6e4048d40cbd52.jpg"
      },
      "date": 1676567892689
    },
    {
      "item": {
        "type": "factory",
        "id": "b653c92587139f3c"
      },
      "id": "b653c92587139f3c",
      "type": "add",
      "after": "539e61f9bf0887c7",
      "date": 1676567932104
    },
    {
      "type": "edit",
      "id": "b653c92587139f3c",
      "item": {
        "type": "paragraph",
        "id": "b653c92587139f3c",
        "text": "And as previously described, the hand gestures of the user allow to activate the operation on the end effector and the storage of desired points, these are the fist of the hand and the opening of the palm out determined by code. Respectively."
      },
      "date": 1676567934194
    },
    {
      "item": {
        "type": "factory",
        "id": "dad2972e9daa81e0"
      },
      "id": "dad2972e9daa81e0",
      "type": "add",
      "after": "b653c92587139f3c",
      "date": 1676567940700
    },
    {
      "type": "edit",
      "id": "dad2972e9daa81e0",
      "item": {
        "type": "paragraph",
        "id": "dad2972e9daa81e0",
        "text": "The procedure to evaluate the programming tool is create a task with a tool on the final effector at the virtual environment. It was observed in the tool that a successful linear trajectory is constructed in suitable conditions to be evaluated in the real robot and to be able to make a comparison (See Fig. 6)."
      },
      "date": 1676567942123
    },
    {
      "type": "add",
      "id": "4d5a7de6123191cf",
      "item": {
        "type": "paragraph",
        "id": "4d5a7de6123191cf",
        "text": "[…] Fig. 6. Performing task of robot with a marker tool."
      },
      "after": "dad2972e9daa81e0",
      "date": 1676567980026
    },
    {
      "item": {
        "type": "factory",
        "id": "e09b67696da3aa55"
      },
      "id": "e09b67696da3aa55",
      "type": "add",
      "after": "4d5a7de6123191cf",
      "date": 1676567988299
    },
    {
      "type": "edit",
      "id": "e09b67696da3aa55",
      "item": {
        "type": "paragraph",
        "id": "e09b67696da3aa55",
        "text": "Then it have to play the same trajectory with the real robot. In the case of the trajectory executed in real robot it is concluded that it fulfills what was expected, it must enter to evaluate the details of the position errors (See Fig. 7)."
      },
      "date": 1676567989992
    },
    {
      "item": {
        "type": "factory",
        "id": "0a74c775e3f93702"
      },
      "id": "0a74c775e3f93702",
      "type": "add",
      "after": "e09b67696da3aa55",
      "date": 1676568000405
    },
    {
      "type": "edit",
      "id": "0a74c775e3f93702",
      "item": {
        "type": "paragraph",
        "id": "0a74c775e3f93702",
        "text": "[…] Fig. 7. Evaluating the created task in the real robot and their work environment."
      },
      "date": 1676568003862
    },
    {
      "item": {
        "type": "factory",
        "id": "ba386944151ae948"
      },
      "id": "ba386944151ae948",
      "type": "add",
      "after": "0a74c775e3f93702",
      "date": 1676568028326
    },
    {
      "type": "edit",
      "id": "ba386944151ae948",
      "item": {
        "type": "paragraph",
        "id": "ba386944151ae948",
        "text": "# Conclusions and Future Work"
      },
      "date": 1676568031683
    },
    {
      "type": "edit",
      "id": "ba386944151ae948",
      "item": {
        "type": "markdown",
        "id": "ba386944151ae948",
        "text": "# Conclusions and Future Work"
      },
      "date": 1676568032928
    },
    {
      "item": {
        "type": "factory",
        "id": "fbe53be8021d129a"
      },
      "id": "fbe53be8021d129a",
      "type": "add",
      "after": "ba386944151ae948",
      "date": 1676568041704
    },
    {
      "type": "edit",
      "id": "fbe53be8021d129a",
      "item": {
        "type": "paragraph",
        "id": "fbe53be8021d129a",
        "text": "The integration of the virtual environment and the positioning tool was successfully carried out. Although the result of characterizing the MYO Armband sensor was not as expected, an alternative interaction to the keyboard and mouse of a computer is achieved, with an unconventional tool that provides an interface with additional features in the construction of a concept of user immersion in the virtual reality environment. The usual techniques to define the tasks of a robot are to write machine code interpreted by the controller of the robot, but with this, we seek to contribute to the theory that the method of teaching the trajectory to robot in a way guided by the gestures of a user programmer could be quickly and flexible [4]."
      },
      "date": 1676568043009
    },
    {
      "item": {
        "type": "factory",
        "id": "5259b70b8c7314f2"
      },
      "id": "5259b70b8c7314f2",
      "type": "add",
      "after": "fbe53be8021d129a",
      "date": 1676568050488
    },
    {
      "type": "edit",
      "id": "5259b70b8c7314f2",
      "item": {
        "type": "paragraph",
        "id": "5259b70b8c7314f2",
        "text": "The developed interface manages to shorten robot manipulation procedures, in relation to the preparation of trajectories that are programmed point-to-point in the robot’s teach pendant, compared with the naturalness of moving the end effector of the robot with gestures and rotating movements of the robot. Hand and forearm respectively. In projects that want to give continuity to this research, apply in new developments the alternative of creating trajectories with curvilinear interpolation, as well as including the same task in the generation of the machine code for its later execution in the real robot. Also, open the possibility of directly connecting the application to the control of the robot online and approach real-time execution."
      },
      "date": 1676568051843
    },
    {
      "item": {
        "type": "factory",
        "id": "408b56c7067ee764"
      },
      "id": "408b56c7067ee764",
      "type": "add",
      "after": "5259b70b8c7314f2",
      "date": 1676568060848
    },
    {
      "type": "edit",
      "id": "408b56c7067ee764",
      "item": {
        "type": "paragraph",
        "id": "408b56c7067ee764",
        "text": "Associating this development with the educational approach, the results presented with this interface can be related to projects that seek to give importance to the application 11 of virtual reality in the teaching of robotics as shown in the work in [5], which an application concludes a high potential as a learning and practice tool. This scope can be applied in the same way in this project, since the virtual environments give the flexibility of implementation and execution."
      },
      "date": 1676568062376
    },
    {
      "type": "edit",
      "id": "fbe53be8021d129a",
      "item": {
        "type": "paragraph",
        "id": "fbe53be8021d129a",
        "text": "The integration of the virtual environment and the positioning tool was successfully carried out. Although the result of characterizing the MYO Armband sensor was not as expected, an alternative interaction to the keyboard and mouse of a computer is achieved, with an unconventional tool that provides an interface with additional features in the construction of a concept of user immersion in the virtual reality environment. The usual techniques to define the tasks of a robot are to write machine code interpreted by the controller of the robot, but with this, we seek to contribute to the theory that the method of teaching the trajectory to robot in a way guided by the gestures of a user programmer could be quickly and flexible."
      },
      "date": 1676568079401
    },
    {
      "type": "edit",
      "id": "cf315c7e4dec28b4",
      "item": {
        "type": "paragraph",
        "id": "cf315c7e4dec28b4",
        "text": "From the observed coordinate system (See Fig. 4), The orientation of the final effector is determined, reflected in the virtual environment with the orientation [[Gizmo]] located in the end effector of the robot."
      },
      "date": 1676570421412
    }
  ]
}