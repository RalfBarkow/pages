{
  "title": "The Singularity",
  "story": [
    {
      "type": "html",
      "text": "Read this first: it's all here for the 'layman': [http://yudkowsky.net/tmol-faq/meaningoflife.html yudkowsky.net] ",
      "id": "e9b88638771c14d97e60765b0499c20c"
    },
    {
      "type": "html",
      "text": "(and it's pretty darn important to your life).",
      "id": "77c68ec00ab2c324ec2c51e60618ae69"
    },
    {
      "type": "html",
      "text": " [[Edit Hint]]: bad link?",
      "id": "65121ac6a259a2293af24509591e9801"
    },
    {
      "type": "html",
      "text": "\nThe above link was later called by Yudkowsky \"The greatest mistake he ever made\". Unfortunately he doesn't seem to believe in taking them down, but there's a nifty warning at the top. Oh, the singularity is still a Big Deal, but if you think AIs would *automatically* be nice to us you're sadly (and fatally) deluded.",
      "id": "f4ee72da981a943943f7b60ccf196e4a"
    },
    {
      "type": "html",
      "text": "\nAlso, [http://www.aleph.se/Trans/Global/Singularity/ www.aleph.se] , [http://www.singularitywatch.com/ www.singularitywatch.com]",
      "id": "9bfd0378cc1d815ef4c2167fe3e4712e"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nSomeone said: A millennialist religious idea expounded by some followers of [[Trans Humanism]].",
      "id": "96adf18d0c249e0268a14fbcab642c3c"
    },
    {
      "type": "html",
      "text": "\nI disagree: Millennialism is the belief that just because we have reached a round number in an arbitrary base ten number system, something very significant is going to happen. Also [[Trans Humanism]] is pretty rejective of religion. Read the Meaning of Life link above to see what I mean.",
      "id": "92cd1410758c5486acc17614355cd270"
    },
    {
      "type": "html",
      "text": "<i>Millenialism is often applied to cases where the final number isn't round, and I think the gist of the comment was that transhumanism itself is a religion, at least in such extreme forms (in which case it's obviously going to reject other religions).  Compare the guide linked above.</i>",
      "id": "811a616133598110622f7e2e36f07750"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nThe basic gist,",
      "id": "a57f3934ecea639f0835a0b95ab63c24"
    },
    {
      "type": "html",
      "text": "\nAt some time in the not too distant future (<i>A lot of people and cultures believe [[http://www.2013.com www.2013.com] 2012]</i>), technological change will accelerate so much and humanity will be so linked together and so able to communicate with itself that in a timeless moment of transcendence we will pop into a higher plane of existence in a puff of boundless optimism.",
      "id": "60579f4aff2d1d90a8bf530ee0c3522c"
    },
    {
      "type": "html",
      "text": "\nThe idea has antecedents, especially [[Teilhard De Chardin]]'s [[Omega Point]]; but the modern version is generally credited to the [[Science Fiction]] of [[Vernor Vinge]].",
      "id": "4d461e7fe6ceeff6b64b10bd7eb35fa2"
    },
    {
      "type": "html",
      "text": "\nActually the gist in [[Vernor Vinge]]'s essay on [[The Singularity]] [http://www.ugcs.caltech.edu/~phoenix/vinge/vinge-sing.html www.ugcs.caltech.edu], and in recent works of \"speculative fiction\" by [[Ray Kurzweil]] [ISBN: 0140282025], [[Hans Moravec]] [ISBN: 0195136306] and [[Ken Mac Leod]] [ISBN: 0765305038], seems to carry more implications for our simply being surpassed and abandoned? ignored? tolerated? eliminated? by our superintelligent machine progeny. Here's an excerpt from Vinge's essay:",
      "id": "ae6594487355c073507476a794b4f208"
    },
    {
      "type": "html",
      "text": "In the 1960s there was recognition of some of the implications of superhuman intelligence. I. J. Good wrote:",
      "id": "68336d217241f991c8382f1235dee1fd"
    },
    {
      "type": "html",
      "text": "Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \"intelligence explosion,\" and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the _last_ invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make.",
      "id": "29f9672bb07b9d23126f389104c11b96"
    },
    {
      "type": "html",
      "text": "Good has captured the essence of the runaway, but does not pursue its most disturbing consequences. Any intelligent machine of the sort he describes would not be humankind's \"tool\" -- any more than humans are the tools of rabbits or robins or chimpanzees.",
      "id": "387829ef918de1ef467a65d55f0a991e"
    },
    {
      "type": "html",
      "text": "\nOpinions differ:",
      "id": "f7e3bd5e57486ffe16da2bb10ea84367"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nAs a big Vinge fan, I have to say that I believe his inclusion of The Singularity in 'Marooned in Realtime' was primarily so that the storyline was possible and not due to any particular belief in it or any specific concept of what it was. Characters in the book were explicit on the point that it wasn't intended to be anything specific, though many possibilities were raised. Of course, it is a legitimate concept; I just felt that this comment was worth making. -- [[Daniel Knapp]]",
      "id": "bbc62cd6fecb0a8bb06b056d626b2744"
    },
    {
      "type": "html",
      "text": "\nHe's been writing about the Singularity in one way or another for some time. In one of the explanatory essays in <i>True Names and Other Dangers</i> (a collection of his Singularity-related short stories), he talks about a story he was working on for John Campbell about some kind of super-intelligence. Campbell told him he couldn't write that story, because he simply couldn't grok that much intelligence. Since then he has focused on stories in which for some reason or another the characters are near but not over the line of the Singularity.",
      "id": "9f0ef82c3be9a6629fff54f0dc64dfa3"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nThe books of [[Iain Banks]] explore life after The Singularity, and how people interact with such ultraintelligent machines.",
      "id": "c5b1af91736d007664286d8c466fb157"
    },
    {
      "type": "html",
      "text": "\nSingularity, The. The Techno-Rapture. A black hole in the Extropian worldview whose gravity is so intense that no light can be shed on what lies beyond it.\nFrom <i>Godling's Glossary</i> by David Victor de Transend.",
      "id": "b7e0a1e6900d3b6736a11f9202251d7f"
    },
    {
      "type": "html",
      "text": "\nThe Singularity is a common matter of discussion in [[Trans Humanism]] circles. There is no clear definition, but usually the Singularity is meant as a future time when societal, scientific and economic change is so fast we cannot even imagine what will happen from our present perspective, and when humanity will become posthumanity. Another definition is used in the Extropians FAQ, where it denotes the singular time when technological development will be at its fastest. Of course, there are some who think the whole idea is just technocalyptic dreaming. -- sg",
      "id": "ad7f80db60ac901f19216887951d678d"
    },
    {
      "type": "html",
      "text": "\nIndeed, singularity is not just about technology. [[Teilhard De Chardin]]: \"Someday, after we have mastered the winds, the waves, the tides and gravity, we shall harness for God the energies of love. Then, for the second time in the history of the world, [hu]mankind will have discovered fire.\" Further, to speak of \"the\" singularity is pretty us-centric. The transition from non-life to life (in de Chardin's terms, geosphere to biosphere) is a shift as profound as anything we are approaching. (Well it would be, except that as far as we can tell, life ought to be abundant anywhere there are planets of the appropriate size and temperature. Certainly organic molecules are abundant in interstellar space.)",
      "id": "498160695749b85c00a5e8d34fa287e4"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nI think that a more likely outcome than The Singularity is what one might call \"The Dissolution\": we will be more and more dependent upon technology and it will become more and more complex until, finally, it will all collapse. Some essential system will crash, and everything else will crash or malfunction as a consequence. It will be impossible to get the whole thing running again, because it will be so complicated, decentralized, and interdependent that there is no correct order for starting it all up again piece by piece.",
      "id": "901c5252f095dc6cdbb063fa0a240e8f"
    },
    {
      "type": "html",
      "text": "\nSo we'll just have to start over again with fire, the plow, and the wheel.",
      "id": "99fd2d4eef555f646927c76006b6c41b"
    },
    {
      "type": "html",
      "text": "\nThe idea that superhuman intelligence will just inevitably \"happen\" as a natural consequence of technological progress seems ludicrous to me.",
      "id": "4a596023428e77c2c1911723e9de3c96"
    },
    {
      "type": "html",
      "text": "-- [[Kris Johnson]]",
      "id": "dd6f62c94b8fffb5c7a72baae7389261"
    },
    {
      "type": "html",
      "text": "\nIf you replace \"technology\" with \"biology\" does it sound so ludicrous? Is there a difference? -- [[Andy Pierce]]",
      "id": "651b01a66196123c158ef07a95e90058"
    },
    {
      "type": "html",
      "text": "\nI'm not sure what the timeline is like for the development of the notion of [[The Singularity]], but [[Terence Mc Kenna]] wrote quite a bit about it in his books. The view he held was neither technologically nor biologically based, but stemmed from the fact that the increase in human knowledge has been growing at an exponential rate, and positing that based on the rate of increase there will be some point (I think forecasted for 2026 or so) <i>[[http://www.2013.com www.2013.com] 2012]</i> where the rate of knowledge increase will exceed the passage of time. This is the moment that humankind could possibly enter a new phase of existence. It's a neat idea, and one that you can have lots of fun speculating about. It could be a point at which our notion of time is redefined to more minute scale, it could be the point at which we begin to perceive more than 4-dimensions, it could be [[The Rapture]], etc.... Fun! Some information regarding this idea from [[Terence Mc Kenna]] at [http://www.sculptors.com/~salsbury/Articles/singularity.html www.sculptors.com] -- [[Sean Mc Namara]]",
      "id": "6a24e93386a6b07724099a91e04d92f2"
    },
    {
      "type": "html",
      "text": "<i>Clearly, knowledge will <b>not</b> grow exponentially for very long.</i>",
      "id": "eb3cea813c2691217e9fe418fe5345fb"
    },
    {
      "type": "html",
      "text": "<i>Clearly, population will <b>not</b> grow exponentially for very long either. knowledge and people are related. Knowledge is assimilated by people who by necessity must specialize given the rapidity of development. Thus knowledge can not really grow any faster than an assimilating population (unless you are talking about knowledge as [[Automated Intelligence]]).</i> ",
      "id": "f3384e68f5b76f85cd9ddfb079ac8582"
    },
    {
      "type": "html",
      "text": "\nCertainly, a possible outcome is that we won't be able to assimilate knowledge, so the rate of new knowledge will slow. This seems quite plausible from a behaviorist/synthesists point of view. If, however, you consider the possibility of a collective mind from which knowledge is revealed, then the possibility of [[Automated Intelligence]] exists. This is an interesting possibility since if we accept time to be a dimension that we merely perceive as serial in nature, then everything that ever will be invented, has already been invented. If this is the case then wouldn't this be grounds for the collective consciousness from whence all thought springs? I'm not saying any of this is the case, only that it is a fruitful ground for speculation.",
      "id": "60ccea611bbf2caa94b4228faade9cef"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nAs may have been gathered by now there are many varieties of \"the Singularity\".  The simplest and most brutally literal, harking back to Vinge's stories, is \"the point at which science fiction authors stop being able to tell stories\"; this is correlated with the rise of greater-than-human intelligence.  This idea goes back to [[Larry Niven]], actually, who had superhuman Pak protectors (but with extremely constrained values) and maybe some stuff in <i>A World Out of Time</i>.",
      "id": "c7dcdcdf5ba0aeade4aad9b4d3cc1761"
    },
    {
      "type": "html",
      "text": "\nAll the stuff about rates of progress going to infinity or analogies to mathematical functions are, in my opinion, from people taking \"Singularity\" too literally and trying to do something mathematical with it.  This is just wrong.",
      "id": "d68072fc52fbb53e85771b6ce06c5b40"
    },
    {
      "type": "html",
      "text": "\n[[Ken Mac Leod]]'s <i>The Sky Road</i> has a brief bit about being post-Singularity; this plus the Culture show a usage referring as much to the presence of nanotech and biological immortality as to superhuman intelligence.  Basically the \"whoah, weird\" point.",
      "id": "38dd2e371f2419fe7e2c798ef27f7a46"
    },
    {
      "type": "html",
      "text": "\nThere's questions about whether we'd notice the Singularity happening if we live through it -- some people prefer the term \"Horizon\", something which recedes as you approach it, and echoes event horizon, and echoes [[Robert Heinlein]]'s <i>[[Beyond This Horizon]]</i> too.  Others wonder if a Singularity hasn't already happened, with the acceleration of scientific progress since 1800.",
      "id": "c514bbcc727352ee211a15d35f5260bc"
    },
    {
      "type": "html",
      "text": "-- [[Damien Sullivan]] (author of the [[Vernor Vinge]] page)",
      "id": "ec4267de6c2d0b9cf812218f4a782342"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nSL4 Wiki is specifically intended for Singularity-related discussion: [http://sl4.org/wiki sl4.org]",
      "id": "4adb8a4aef47fb4b1909ef7d1a748d27"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nCould somebody tell me what makes singularitarians think that increasing <i>intelligence</i> will make it easier to invent a \"correct\" meaning of life?",
      "id": "214f553e9b17cf27982ae44e72590e0a"
    },
    {
      "type": "html",
      "text": "<i>The meaning of life is something people think about. Increasing \"intelligence\" helps people think better about everything. Therefore, increasing \"intelligence\" helps people think better about the meaning of life. Yes ?</i>",
      "id": "2e3d211451320c4511d94668f418c89e"
    },
    {
      "type": "html",
      "text": "\nFrom [http://yudkowsky.net/tmol-faq/meaningoflife.html yudkowsky.net] :",
      "id": "5fe9e54096dae7c2c63794f42d25a464"
    },
    {
      "type": "html",
      "text": "\nThe more intelligent you are, the better your chance of discovering the true meaning of life, the more power you have to achieve it, and the greater your probability of acting on it (16).",
      "id": "453b828f1cbf63e8d1f7623f3f8f0214"
    },
    {
      "type": "html",
      "text": "<i>This assumes that life has meaning.  My vast and superior intelligence has deduced that life is meaningless, aside from the meaning we assign to life in order to amuse ourselves.</i>",
      "id": "ea472a172fdc41eb5b79604bedb8ef55"
    },
    {
      "type": "html",
      "text": " Your intelligence would indeed have to be vast to make such an observation.  Those of us in the subset of Life, don't have the authority to determine the [[Meaning Of Life]].  Only the Being responsible for creating life has that authority.  Of course, we are [[Chasing Rabbits]] here, as the topic is [[The Singularity]] rather than [[The Meaning Of Life]]. -- [[Bruce Pennington]]",
      "id": "e904512aa73fd42d9d52dcc3832e47c0"
    },
    {
      "type": "html",
      "text": "\nYou may quite well just have reached a local maximum and can't get to the real answer until your search strikes far enough in an unlikely direction to find a steeper climb.",
      "id": "4d4c9d3a3cf7a427af1cfc53002497b4"
    },
    {
      "type": "html",
      "text": " Indeed so; anyone who stops at this point is merely demonstrating that, despite their self-assessed intelligence, they fall well short of the thinkers preceding them. The well-known answer to go beyond the above stage is to say \"fine, say there is no externally-imposed meaning of life; well and good, then we have individual freedom to invent <b>our own</b> meanings for our own lives\", and the debate continutes unabated from there. (All of which is simply adding details to the already-stated point that such a view is a local maximum, not a global maximum.)",
      "id": "362fe0b1133cc421805dfb38507ca380"
    },
    {
      "type": "html",
      "text": "...",
      "id": "9181033ba286889257941ff84e6eae48"
    },
    {
      "type": "html",
      "text": "# 16:  Some people disagree with that last part.  They are, in fact, wrong.  (17).  But even so, very few people think that being more intelligent makes you intrinsically less moral.  So, when you run the model through the algebraic goal system, it's enough to create the differential of desirability that lets you make choices (see below).",
      "id": "9fcdcadc02fee5f6edd58a98f2747921"
    },
    {
      "type": "html",
      "text": "# 17:  Intelligence isn't just high-speed arithmetic, or a better memory, or winning at chess, or other stereotypical party tricks.  Intelligence is self-awareness, and wisdom, and the ability to not be stupid, and other things that alter every aspect of the personality.",
      "id": "c1cc80e4d1b934302dee3eb95df79c8a"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "\nCould somebody tell me what makes singularitarians think that utilitarianism is the ultimate value philosophy?",
      "id": "cee243251d4547a08e2b2fd4d08736da"
    },
    {
      "type": "html",
      "text": "<i>Why do you think we think that ?</i>",
      "id": "0892aa3bd41f59d1a901ef5498c9f493"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "The webcomic A Miracle of Science ([http://www.project-apollo.net/mos/ www.project-apollo.net]) depicts two possible aspects of [[The Singularity]]: the \nMartian 'collective intelligence' on the one hand, and the rise of [[Mad Scientists]] (see [[Science Related Memetic Disorder]]) on the other. - [[Jay Osako]]",
      "id": "7b646e142905baeac216c1696d4fb666"
    },
    {
      "type": "html",
      "text": "\nSomeone here, who has read [[The Singularity Is Near]] and wants to elaborate on this nearly empty page? -- fp",
      "id": "5872e533c0b59c9320713031c9e27083"
    },
    {
      "type": "html",
      "text": "\nSure: I don't think that there is/will be a singularity of that kind. My argument runes the same way as on [[Simulation Argument]]: \nI think that there are (and we we will discover) inherent complexity limits due to energy required to keep the complexity that make the hypothetical \"planet computers\" etc. infeasable. Just have a look at our [[Most Complex System That Ever Worked]]. I think though that AI is possible - after all BI is possible so there are not theoretical complexity limits for that,",
      "id": "85d6093b341c5930c2705ce442831e01"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "Robots are probably gonna kick our ass because they have one advantage over biological systems: they copy the best minds instead of have to start over again for each new generation.",
      "id": "d5315cb7ea461dcd23e3ace1b5b756f9"
    },
    {
      "type": "html",
      "text": "\nHow could we fix this? Thoughts?",
      "id": "ed8961566b22bc8a10dc1cdbc02311f4"
    },
    {
      "type": "html",
      "text": "<hr>",
      "id": "d2f253de00fbae5952291aa0e158b76c"
    },
    {
      "type": "html",
      "text": "[[Category Future]]",
      "id": "c6313ec1287797348aa8250d8cc7f450"
    },
    {
      "type": "html",
      "text": "See original on  [http://c2.com/cgi/wiki?TheSingularity c2.com]",
      "id": "baa698c00fbf6048c2912de1f8b33fc8"
    }
  ],
  "journal": [
    {
      "date": 1415682520000,
      "id": "31808504d3629b453c3fe922f029f7f0",
      "type": "create",
      "item": {
        "title": "The Singularity",
        "story": [
          {
            "type": "html",
            "text": "Read this first: it's all here for the 'layman': [http://yudkowsky.net/tmol-faq/meaningoflife.html yudkowsky.net] ",
            "id": "e9b88638771c14d97e60765b0499c20c"
          },
          {
            "type": "html",
            "text": "(and it's pretty darn important to your life).",
            "id": "77c68ec00ab2c324ec2c51e60618ae69"
          },
          {
            "type": "html",
            "text": " [[Edit Hint]]: bad link?",
            "id": "65121ac6a259a2293af24509591e9801"
          },
          {
            "type": "html",
            "text": "\nThe above link was later called by Yudkowsky \"The greatest mistake he ever made\". Unfortunately he doesn't seem to believe in taking them down, but there's a nifty warning at the top. Oh, the singularity is still a Big Deal, but if you think AIs would *automatically* be nice to us you're sadly (and fatally) deluded.",
            "id": "f4ee72da981a943943f7b60ccf196e4a"
          },
          {
            "type": "html",
            "text": "\nAlso, [http://www.aleph.se/Trans/Global/Singularity/ www.aleph.se] , [http://www.singularitywatch.com/ www.singularitywatch.com]",
            "id": "9bfd0378cc1d815ef4c2167fe3e4712e"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nSomeone said: A millennialist religious idea expounded by some followers of [[Trans Humanism]].",
            "id": "96adf18d0c249e0268a14fbcab642c3c"
          },
          {
            "type": "html",
            "text": "\nI disagree: Millennialism is the belief that just because we have reached a round number in an arbitrary base ten number system, something very significant is going to happen. Also [[Trans Humanism]] is pretty rejective of religion. Read the Meaning of Life link above to see what I mean.",
            "id": "92cd1410758c5486acc17614355cd270"
          },
          {
            "type": "html",
            "text": "<i>Millenialism is often applied to cases where the final number isn't round, and I think the gist of the comment was that transhumanism itself is a religion, at least in such extreme forms (in which case it's obviously going to reject other religions).  Compare the guide linked above.</i>",
            "id": "811a616133598110622f7e2e36f07750"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nThe basic gist,",
            "id": "a57f3934ecea639f0835a0b95ab63c24"
          },
          {
            "type": "html",
            "text": "\nAt some time in the not too distant future (<i>A lot of people and cultures believe [[http://www.2013.com www.2013.com] 2012]</i>), technological change will accelerate so much and humanity will be so linked together and so able to communicate with itself that in a timeless moment of transcendence we will pop into a higher plane of existence in a puff of boundless optimism.",
            "id": "60579f4aff2d1d90a8bf530ee0c3522c"
          },
          {
            "type": "html",
            "text": "\nThe idea has antecedents, especially [[Teilhard De Chardin]]'s [[Omega Point]]; but the modern version is generally credited to the [[Science Fiction]] of [[Vernor Vinge]].",
            "id": "4d461e7fe6ceeff6b64b10bd7eb35fa2"
          },
          {
            "type": "html",
            "text": "\nActually the gist in [[Vernor Vinge]]'s essay on [[The Singularity]] [http://www.ugcs.caltech.edu/~phoenix/vinge/vinge-sing.html www.ugcs.caltech.edu], and in recent works of \"speculative fiction\" by [[Ray Kurzweil]] [ISBN: 0140282025], [[Hans Moravec]] [ISBN: 0195136306] and [[Ken Mac Leod]] [ISBN: 0765305038], seems to carry more implications for our simply being surpassed and abandoned? ignored? tolerated? eliminated? by our superintelligent machine progeny. Here's an excerpt from Vinge's essay:",
            "id": "ae6594487355c073507476a794b4f208"
          },
          {
            "type": "html",
            "text": "In the 1960s there was recognition of some of the implications of superhuman intelligence. I. J. Good wrote:",
            "id": "68336d217241f991c8382f1235dee1fd"
          },
          {
            "type": "html",
            "text": "Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \"intelligence explosion,\" and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the _last_ invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make.",
            "id": "29f9672bb07b9d23126f389104c11b96"
          },
          {
            "type": "html",
            "text": "Good has captured the essence of the runaway, but does not pursue its most disturbing consequences. Any intelligent machine of the sort he describes would not be humankind's \"tool\" -- any more than humans are the tools of rabbits or robins or chimpanzees.",
            "id": "387829ef918de1ef467a65d55f0a991e"
          },
          {
            "type": "html",
            "text": "\nOpinions differ:",
            "id": "f7e3bd5e57486ffe16da2bb10ea84367"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nAs a big Vinge fan, I have to say that I believe his inclusion of The Singularity in 'Marooned in Realtime' was primarily so that the storyline was possible and not due to any particular belief in it or any specific concept of what it was. Characters in the book were explicit on the point that it wasn't intended to be anything specific, though many possibilities were raised. Of course, it is a legitimate concept; I just felt that this comment was worth making. -- [[Daniel Knapp]]",
            "id": "bbc62cd6fecb0a8bb06b056d626b2744"
          },
          {
            "type": "html",
            "text": "\nHe's been writing about the Singularity in one way or another for some time. In one of the explanatory essays in <i>True Names and Other Dangers</i> (a collection of his Singularity-related short stories), he talks about a story he was working on for John Campbell about some kind of super-intelligence. Campbell told him he couldn't write that story, because he simply couldn't grok that much intelligence. Since then he has focused on stories in which for some reason or another the characters are near but not over the line of the Singularity.",
            "id": "9f0ef82c3be9a6629fff54f0dc64dfa3"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nThe books of [[Iain Banks]] explore life after The Singularity, and how people interact with such ultraintelligent machines.",
            "id": "c5b1af91736d007664286d8c466fb157"
          },
          {
            "type": "html",
            "text": "\nSingularity, The. The Techno-Rapture. A black hole in the Extropian worldview whose gravity is so intense that no light can be shed on what lies beyond it.\nFrom <i>Godling's Glossary</i> by David Victor de Transend.",
            "id": "b7e0a1e6900d3b6736a11f9202251d7f"
          },
          {
            "type": "html",
            "text": "\nThe Singularity is a common matter of discussion in [[Trans Humanism]] circles. There is no clear definition, but usually the Singularity is meant as a future time when societal, scientific and economic change is so fast we cannot even imagine what will happen from our present perspective, and when humanity will become posthumanity. Another definition is used in the Extropians FAQ, where it denotes the singular time when technological development will be at its fastest. Of course, there are some who think the whole idea is just technocalyptic dreaming. -- sg",
            "id": "ad7f80db60ac901f19216887951d678d"
          },
          {
            "type": "html",
            "text": "\nIndeed, singularity is not just about technology. [[Teilhard De Chardin]]: \"Someday, after we have mastered the winds, the waves, the tides and gravity, we shall harness for God the energies of love. Then, for the second time in the history of the world, [hu]mankind will have discovered fire.\" Further, to speak of \"the\" singularity is pretty us-centric. The transition from non-life to life (in de Chardin's terms, geosphere to biosphere) is a shift as profound as anything we are approaching. (Well it would be, except that as far as we can tell, life ought to be abundant anywhere there are planets of the appropriate size and temperature. Certainly organic molecules are abundant in interstellar space.)",
            "id": "498160695749b85c00a5e8d34fa287e4"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nI think that a more likely outcome than The Singularity is what one might call \"The Dissolution\": we will be more and more dependent upon technology and it will become more and more complex until, finally, it will all collapse. Some essential system will crash, and everything else will crash or malfunction as a consequence. It will be impossible to get the whole thing running again, because it will be so complicated, decentralized, and interdependent that there is no correct order for starting it all up again piece by piece.",
            "id": "901c5252f095dc6cdbb063fa0a240e8f"
          },
          {
            "type": "html",
            "text": "\nSo we'll just have to start over again with fire, the plow, and the wheel.",
            "id": "99fd2d4eef555f646927c76006b6c41b"
          },
          {
            "type": "html",
            "text": "\nThe idea that superhuman intelligence will just inevitably \"happen\" as a natural consequence of technological progress seems ludicrous to me.",
            "id": "4a596023428e77c2c1911723e9de3c96"
          },
          {
            "type": "html",
            "text": "-- [[Kris Johnson]]",
            "id": "dd6f62c94b8fffb5c7a72baae7389261"
          },
          {
            "type": "html",
            "text": "\nIf you replace \"technology\" with \"biology\" does it sound so ludicrous? Is there a difference? -- [[Andy Pierce]]",
            "id": "651b01a66196123c158ef07a95e90058"
          },
          {
            "type": "html",
            "text": "\nI'm not sure what the timeline is like for the development of the notion of [[The Singularity]], but [[Terence Mc Kenna]] wrote quite a bit about it in his books. The view he held was neither technologically nor biologically based, but stemmed from the fact that the increase in human knowledge has been growing at an exponential rate, and positing that based on the rate of increase there will be some point (I think forecasted for 2026 or so) <i>[[http://www.2013.com www.2013.com] 2012]</i> where the rate of knowledge increase will exceed the passage of time. This is the moment that humankind could possibly enter a new phase of existence. It's a neat idea, and one that you can have lots of fun speculating about. It could be a point at which our notion of time is redefined to more minute scale, it could be the point at which we begin to perceive more than 4-dimensions, it could be [[The Rapture]], etc.... Fun! Some information regarding this idea from [[Terence Mc Kenna]] at [http://www.sculptors.com/~salsbury/Articles/singularity.html www.sculptors.com] -- [[Sean Mc Namara]]",
            "id": "6a24e93386a6b07724099a91e04d92f2"
          },
          {
            "type": "html",
            "text": "<i>Clearly, knowledge will <b>not</b> grow exponentially for very long.</i>",
            "id": "eb3cea813c2691217e9fe418fe5345fb"
          },
          {
            "type": "html",
            "text": "<i>Clearly, population will <b>not</b> grow exponentially for very long either. knowledge and people are related. Knowledge is assimilated by people who by necessity must specialize given the rapidity of development. Thus knowledge can not really grow any faster than an assimilating population (unless you are talking about knowledge as [[Automated Intelligence]]).</i> ",
            "id": "f3384e68f5b76f85cd9ddfb079ac8582"
          },
          {
            "type": "html",
            "text": "\nCertainly, a possible outcome is that we won't be able to assimilate knowledge, so the rate of new knowledge will slow. This seems quite plausible from a behaviorist/synthesists point of view. If, however, you consider the possibility of a collective mind from which knowledge is revealed, then the possibility of [[Automated Intelligence]] exists. This is an interesting possibility since if we accept time to be a dimension that we merely perceive as serial in nature, then everything that ever will be invented, has already been invented. If this is the case then wouldn't this be grounds for the collective consciousness from whence all thought springs? I'm not saying any of this is the case, only that it is a fruitful ground for speculation.",
            "id": "60ccea611bbf2caa94b4228faade9cef"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nAs may have been gathered by now there are many varieties of \"the Singularity\".  The simplest and most brutally literal, harking back to Vinge's stories, is \"the point at which science fiction authors stop being able to tell stories\"; this is correlated with the rise of greater-than-human intelligence.  This idea goes back to [[Larry Niven]], actually, who had superhuman Pak protectors (but with extremely constrained values) and maybe some stuff in <i>A World Out of Time</i>.",
            "id": "c7dcdcdf5ba0aeade4aad9b4d3cc1761"
          },
          {
            "type": "html",
            "text": "\nAll the stuff about rates of progress going to infinity or analogies to mathematical functions are, in my opinion, from people taking \"Singularity\" too literally and trying to do something mathematical with it.  This is just wrong.",
            "id": "d68072fc52fbb53e85771b6ce06c5b40"
          },
          {
            "type": "html",
            "text": "\n[[Ken Mac Leod]]'s <i>The Sky Road</i> has a brief bit about being post-Singularity; this plus the Culture show a usage referring as much to the presence of nanotech and biological immortality as to superhuman intelligence.  Basically the \"whoah, weird\" point.",
            "id": "38dd2e371f2419fe7e2c798ef27f7a46"
          },
          {
            "type": "html",
            "text": "\nThere's questions about whether we'd notice the Singularity happening if we live through it -- some people prefer the term \"Horizon\", something which recedes as you approach it, and echoes event horizon, and echoes [[Robert Heinlein]]'s <i>[[Beyond This Horizon]]</i> too.  Others wonder if a Singularity hasn't already happened, with the acceleration of scientific progress since 1800.",
            "id": "c514bbcc727352ee211a15d35f5260bc"
          },
          {
            "type": "html",
            "text": "-- [[Damien Sullivan]] (author of the [[Vernor Vinge]] page)",
            "id": "ec4267de6c2d0b9cf812218f4a782342"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nSL4 Wiki is specifically intended for Singularity-related discussion: [http://sl4.org/wiki sl4.org]",
            "id": "4adb8a4aef47fb4b1909ef7d1a748d27"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nCould somebody tell me what makes singularitarians think that increasing <i>intelligence</i> will make it easier to invent a \"correct\" meaning of life?",
            "id": "214f553e9b17cf27982ae44e72590e0a"
          },
          {
            "type": "html",
            "text": "<i>The meaning of life is something people think about. Increasing \"intelligence\" helps people think better about everything. Therefore, increasing \"intelligence\" helps people think better about the meaning of life. Yes ?</i>",
            "id": "2e3d211451320c4511d94668f418c89e"
          },
          {
            "type": "html",
            "text": "\nFrom [http://yudkowsky.net/tmol-faq/meaningoflife.html yudkowsky.net] :",
            "id": "5fe9e54096dae7c2c63794f42d25a464"
          },
          {
            "type": "html",
            "text": "\nThe more intelligent you are, the better your chance of discovering the true meaning of life, the more power you have to achieve it, and the greater your probability of acting on it (16).",
            "id": "453b828f1cbf63e8d1f7623f3f8f0214"
          },
          {
            "type": "html",
            "text": "<i>This assumes that life has meaning.  My vast and superior intelligence has deduced that life is meaningless, aside from the meaning we assign to life in order to amuse ourselves.</i>",
            "id": "ea472a172fdc41eb5b79604bedb8ef55"
          },
          {
            "type": "html",
            "text": " Your intelligence would indeed have to be vast to make such an observation.  Those of us in the subset of Life, don't have the authority to determine the [[Meaning Of Life]].  Only the Being responsible for creating life has that authority.  Of course, we are [[Chasing Rabbits]] here, as the topic is [[The Singularity]] rather than [[The Meaning Of Life]]. -- [[Bruce Pennington]]",
            "id": "e904512aa73fd42d9d52dcc3832e47c0"
          },
          {
            "type": "html",
            "text": "\nYou may quite well just have reached a local maximum and can't get to the real answer until your search strikes far enough in an unlikely direction to find a steeper climb.",
            "id": "4d4c9d3a3cf7a427af1cfc53002497b4"
          },
          {
            "type": "html",
            "text": " Indeed so; anyone who stops at this point is merely demonstrating that, despite their self-assessed intelligence, they fall well short of the thinkers preceding them. The well-known answer to go beyond the above stage is to say \"fine, say there is no externally-imposed meaning of life; well and good, then we have individual freedom to invent <b>our own</b> meanings for our own lives\", and the debate continutes unabated from there. (All of which is simply adding details to the already-stated point that such a view is a local maximum, not a global maximum.)",
            "id": "362fe0b1133cc421805dfb38507ca380"
          },
          {
            "type": "html",
            "text": "...",
            "id": "9181033ba286889257941ff84e6eae48"
          },
          {
            "type": "html",
            "text": "# 16:  Some people disagree with that last part.  They are, in fact, wrong.  (17).  But even so, very few people think that being more intelligent makes you intrinsically less moral.  So, when you run the model through the algebraic goal system, it's enough to create the differential of desirability that lets you make choices (see below).",
            "id": "9fcdcadc02fee5f6edd58a98f2747921"
          },
          {
            "type": "html",
            "text": "# 17:  Intelligence isn't just high-speed arithmetic, or a better memory, or winning at chess, or other stereotypical party tricks.  Intelligence is self-awareness, and wisdom, and the ability to not be stupid, and other things that alter every aspect of the personality.",
            "id": "c1cc80e4d1b934302dee3eb95df79c8a"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "\nCould somebody tell me what makes singularitarians think that utilitarianism is the ultimate value philosophy?",
            "id": "cee243251d4547a08e2b2fd4d08736da"
          },
          {
            "type": "html",
            "text": "<i>Why do you think we think that ?</i>",
            "id": "0892aa3bd41f59d1a901ef5498c9f493"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "The webcomic A Miracle of Science ([http://www.project-apollo.net/mos/ www.project-apollo.net]) depicts two possible aspects of [[The Singularity]]: the \nMartian 'collective intelligence' on the one hand, and the rise of [[Mad Scientists]] (see [[Science Related Memetic Disorder]]) on the other. - [[Jay Osako]]",
            "id": "7b646e142905baeac216c1696d4fb666"
          },
          {
            "type": "html",
            "text": "\nSomeone here, who has read [[The Singularity Is Near]] and wants to elaborate on this nearly empty page? -- fp",
            "id": "5872e533c0b59c9320713031c9e27083"
          },
          {
            "type": "html",
            "text": "\nSure: I don't think that there is/will be a singularity of that kind. My argument runes the same way as on [[Simulation Argument]]: \nI think that there are (and we we will discover) inherent complexity limits due to energy required to keep the complexity that make the hypothetical \"planet computers\" etc. infeasable. Just have a look at our [[Most Complex System That Ever Worked]]. I think though that AI is possible - after all BI is possible so there are not theoretical complexity limits for that,",
            "id": "85d6093b341c5930c2705ce442831e01"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "Robots are probably gonna kick our ass because they have one advantage over biological systems: they copy the best minds instead of have to start over again for each new generation.",
            "id": "d5315cb7ea461dcd23e3ace1b5b756f9"
          },
          {
            "type": "html",
            "text": "\nHow could we fix this? Thoughts?",
            "id": "ed8961566b22bc8a10dc1cdbc02311f4"
          },
          {
            "type": "html",
            "text": "<hr>",
            "id": "d2f253de00fbae5952291aa0e158b76c"
          },
          {
            "type": "html",
            "text": "[[Category Future]]",
            "id": "c6313ec1287797348aa8250d8cc7f450"
          },
          {
            "type": "html",
            "text": "See original on  [http://c2.com/cgi/wiki?TheSingularity c2.com]",
            "id": "baa698c00fbf6048c2912de1f8b33fc8"
          }
        ]
      }
    },
    {
      "type": "fork",
      "site": "sfw.c2.com",
      "date": 1674546957424
    }
  ]
}