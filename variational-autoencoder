{
  "title": "Variational Autoencoder",
  "story": [
    {
      "type": "paragraph",
      "id": "c3d8c9e67e616aee",
      "text": "In current cognitive science the proposal of [[Karl Friston]] about the fundamental predictive activity of the brain and the related free-energy principle is well known and discussed. "
    },
    {
      "type": "paragraph",
      "id": "b469cadf18fd316b",
      "text": "At the heart of his proposal there is a formal expression of [[Free Energy]], derived from Bayesian variational inference (Friston, 2010; Friston & Kiebel, 2009; Friston & Stephan, 2007). "
    },
    {
      "type": "paragraph",
      "id": "7adbac648c72fbc0",
      "text": "On the deep learning side, an important advancement was achieved a few years ago with an architecture known as the “variational autoencoder”, introduced independently by Kingma and Welling (2014) and by Rezende, Mohamed, and Wierstra (2014). Variational autoencoders in deep learning are a precise correlate of Friston's free-energy principle in the brain, and the mathematical formulations are almost the same. "
    },
    {
      "type": "paragraph",
      "id": "944e23c84b41b655",
      "text": "Curiously, Kingma & Welling glaringly neglect the connection between their new architecture and its cognitive counterpart, as do Rezende and co-authors. This striking connection is ignored in all the further refinement on the variational autoencoder in the deep learning community, and it is first acknowledged only by Ofner and Stober (2018). "
    },
    {
      "type": "paragraph",
      "id": "862de59b868e16be",
      "text": "We are rather inclined to push the difference in scope between the earlier artificial neural network community engaged in cognitive explorations and deep learning even further. To the extent that modelers withdrew from pursuing cognitive investigations, the design of neural models was allowed much more freedom in adopting mathematical solutions alien to mental processes."
    },
    {
      "type": "paragraph",
      "id": "fe5a7cb7f1b91ab0",
      "text": "However, we argue that now deep learning, despite this recent tradition, can and should have its say in cognitive science. There is at least one simple reason: the engineering objectives of deep learning have been met with such success that, for the first time, we have artificial models performing complex cognitive tasks at human performance level. The era of toy worlds in which models are restricted to highly simplified versions of cognitive capabilities is over. We now have empirical examples of algorithms solving cognitive tasks at the full scale of complexity."
    },
    {
      "type": "paragraph",
      "id": "93d988c3677f8cbf",
      "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017; Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018; Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “[[General Artificial Intelligence]]”. >> general artificial intelligence"
    },
    {
      "type": "paragraph",
      "id": "d83cfebe16883836",
      "text": "These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019). >> subsymbolic"
    },
    {
      "type": "pagefold",
      "id": "2486022f411e17f4",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "f1d408ea1c1380ae",
      "text": "PERCONTI, Pietro and PLEBE, Alessio, 2020. Deep learning and cognitive science. Cognition. 1 October 2020. Vol. 203, p. 104365. DOI 10.1016/j.cognition.2020.104365. "
    },
    {
      "type": "paragraph",
      "id": "a1c809cf18ad4f70",
      "text": "In recent years, the family of algorithms collected under the term “deep learning” has revolutionized artificial intelligence, enabling machines to reach human-like performances in many complex cognitive tasks. Although deep learning models are grounded in the connectionist paradigm, their recent advances were basically developed with engineering goals in mind. Despite of their applied focus, deep learning models eventually seem fruitful for cognitive purposes. "
    },
    {
      "type": "paragraph",
      "id": "404b7380f5202e7d",
      "text": "This can be thought as a kind of biological exaptation, where a physiological structure becomes applicable for a function different from that for which it was selected. "
    },
    {
      "type": "paragraph",
      "id": "7f2412ef2524efb1",
      "text": "In this paper, it will be argued that it is time for cognitive science to seriously come to terms with deep learning, and we try to spell out the reasons why this is the case. "
    },
    {
      "type": "paragraph",
      "id": "33cdaa92f734ee1a",
      "text": "First, the path of the evolution of deep learning from the connectionist project is traced, demonstrating the remarkable continuity, and the differences as well. "
    },
    {
      "type": "paragraph",
      "id": "3c3188cc1786bc9b",
      "text": "Then, it will be considered how deep learning models can be useful for many cognitive topics, especially those where it has achieved performance comparable to humans, from perception to language. It will be maintained that deep learning poses questions that cognitive sciences should try to answer. One of such questions is the reasons why deep convolutional models that are disembodied, inactive, unaware of context, and static, are by far the closest to the patterns of activation in the brain visual system.\n"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Variational Autoencoder",
        "story": []
      },
      "date": 1679483955992
    },
    {
      "item": {
        "type": "factory",
        "id": "f1d408ea1c1380ae"
      },
      "id": "f1d408ea1c1380ae",
      "type": "add",
      "date": 1679483968782
    },
    {
      "type": "edit",
      "id": "f1d408ea1c1380ae",
      "item": {
        "type": "paragraph",
        "id": "f1d408ea1c1380ae",
        "text": "\nPERCONTI, Pietro and PLEBE, Alessio, 2020. Deep learning and cognitive science. Cognition. 1 October 2020. Vol. 203, p. 104365. DOI 10.1016/j.cognition.2020.104365. In recent years, the family of algorithms collected under the term “deep learning” has revolutionized artificial intelligence, enabling machines to reach human-like performances in many complex cognitive tasks. Although deep learning models are grounded in the connectionist paradigm, their recent advances were basically developed with engineering goals in mind. Despite of their applied focus, deep learning models eventually seem fruitful for cognitive purposes. This can be thought as a kind of biological exaptation, where a physiological structure becomes applicable for a function different from that for which it was selected. In this paper, it will be argued that it is time for cognitive science to seriously come to terms with deep learning, and we try to spell out the reasons why this is the case. First, the path of the evolution of deep learning from the connectionist project is traced, demonstrating the remarkable continuity, and the differences as well. Then, it will be considered how deep learning models can be useful for many cognitive topics, especially those where it has achieved performance comparable to humans, from perception to language. It will be maintained that deep learning poses questions that cognitive sciences should try to answer. One of such questions is the reasons why deep convolutional models that are disembodied, inactive, unaware of context, and static, are by far the closest to the patterns of activation in the brain visual system.\n"
      },
      "date": 1679483970986
    },
    {
      "item": {
        "type": "factory",
        "id": "2486022f411e17f4"
      },
      "id": "2486022f411e17f4",
      "type": "add",
      "after": "f1d408ea1c1380ae",
      "date": 1679483976083
    },
    {
      "type": "edit",
      "id": "2486022f411e17f4",
      "item": {
        "type": "pagefold",
        "id": "2486022f411e17f4",
        "text": "~"
      },
      "date": 1679483981302
    },
    {
      "item": {
        "type": "factory",
        "id": "c3d8c9e67e616aee"
      },
      "id": "c3d8c9e67e616aee",
      "type": "add",
      "after": "f1d408ea1c1380ae",
      "date": 1679483999809
    },
    {
      "id": "c3d8c9e67e616aee",
      "type": "move",
      "order": [
        "2486022f411e17f4",
        "c3d8c9e67e616aee",
        "f1d408ea1c1380ae"
      ],
      "date": 1679484005964
    },
    {
      "id": "2486022f411e17f4",
      "type": "move",
      "order": [
        "c3d8c9e67e616aee",
        "2486022f411e17f4",
        "f1d408ea1c1380ae"
      ],
      "date": 1679484007505
    },
    {
      "type": "edit",
      "id": "c3d8c9e67e616aee",
      "item": {
        "type": "paragraph",
        "id": "c3d8c9e67e616aee",
        "text": "In current cognitive science the proposal of Karl Friston about the fundamental predictive activity of the brain and the related free-energy principle is well known and discussed. At the heart of his proposal there is a formal expression of free energy, derived from Bayesian variational inference (Friston, 2010;Friston & Kiebel, 2009; Friston & Stephan, 2007). On the deep learning side, an important advancement was achieved a few years ago with an architecture known as the “variational autoencoder”, introduced"
      },
      "date": 1679484009079
    },
    {
      "type": "edit",
      "id": "c3d8c9e67e616aee",
      "item": {
        "type": "paragraph",
        "id": "c3d8c9e67e616aee",
        "text": "In current cognitive science the proposal of [[Karl Friston]] about the fundamental predictive activity of the brain and the related free-energy principle is well known and discussed. At the heart of his proposal there is a formal expression of free energy, derived from Bayesian variational inference (Friston, 2010;Friston & Kiebel, 2009; Friston & Stephan, 2007). On the deep learning side, an important advancement was achieved a few years ago with an architecture known as the “variational autoencoder”, introduced"
      },
      "date": 1679484022401
    },
    {
      "type": "edit",
      "id": "c3d8c9e67e616aee",
      "item": {
        "type": "paragraph",
        "id": "c3d8c9e67e616aee",
        "text": "In current cognitive science the proposal of [[Karl Friston]] about the fundamental predictive activity of the brain and the related free-energy principle is well known and discussed. "
      },
      "date": 1679484040408
    },
    {
      "type": "add",
      "id": "b469cadf18fd316b",
      "item": {
        "type": "paragraph",
        "id": "b469cadf18fd316b",
        "text": "At the heart of his proposal there is a formal expression of [[Free Energy]], derived from Bayesian variational inference (Friston, 2010;Friston & Kiebel, 2009; Friston & Stephan, 2007). On the deep learning side, an important advancement was achieved a few years ago with an architecture known as the “variational autoencoder”, introduced"
      },
      "after": "c3d8c9e67e616aee",
      "date": 1679484052476
    },
    {
      "type": "edit",
      "id": "b469cadf18fd316b",
      "item": {
        "type": "paragraph",
        "id": "b469cadf18fd316b",
        "text": "At the heart of his proposal there is a formal expression of [[Free Energy]], derived from Bayesian variational inference (Friston, 2010; Friston & Kiebel, 2009; Friston & Stephan, 2007). On the deep learning side, an important advancement was achieved a few years ago with an architecture known as the “variational autoencoder”, introduced"
      },
      "date": 1679484069918
    },
    {
      "type": "edit",
      "id": "b469cadf18fd316b",
      "item": {
        "type": "paragraph",
        "id": "b469cadf18fd316b",
        "text": "At the heart of his proposal there is a formal expression of [[Free Energy]], derived from Bayesian variational inference (Friston, 2010; Friston & Kiebel, 2009; Friston & Stephan, 2007). "
      },
      "date": 1679484080033
    },
    {
      "type": "add",
      "id": "7adbac648c72fbc0",
      "item": {
        "type": "paragraph",
        "id": "7adbac648c72fbc0",
        "text": "On the deep learning side, an important advancement was achieved a few years ago with an architecture known as the “variational autoencoder”, introduced"
      },
      "after": "b469cadf18fd316b",
      "date": 1679484081694
    },
    {
      "type": "edit",
      "id": "7adbac648c72fbc0",
      "item": {
        "type": "paragraph",
        "id": "7adbac648c72fbc0",
        "text": "On the deep learning side, an important advancement was achieved a few years ago with an architecture known as the “variational autoencoder”, introduced independently by Kingma and Welling (2014) and by Rezende, Mohamed, and Wierstra (2014). Variational autoencoders in deep learning are a precise correlate of Friston's free-energy principle in the brain, and the mathematical formulations are almost the same. Curiously, Kingma & Welling glaringly neglect the connection between their new architecture and its cognitive counterpart, as do Rezende and co-authors. This striking connection is ignored in all the further refinement on the variational autoencoder in the deep learning community, and it is first acknowledged only by Ofner and Stober (2018).We are rather inclined to push the difference in scope between the earlier artificial neural network community engaged in cognitive explorations and deep learning even further. To the extent that modelers withdrew from pursuing cognitive investigations, the design of neural models was allowed much more freedom in adopting mathematical solutions alien to mental processes."
      },
      "date": 1679484097176
    },
    {
      "type": "edit",
      "id": "7adbac648c72fbc0",
      "item": {
        "type": "paragraph",
        "id": "7adbac648c72fbc0",
        "text": "On the deep learning side, an important advancement was achieved a few years ago with an architecture known as the “variational autoencoder”, introduced independently by Kingma and Welling (2014) and by Rezende, Mohamed, and Wierstra (2014). Variational autoencoders in deep learning are a precise correlate of Friston's free-energy principle in the brain, and the mathematical formulations are almost the same. "
      },
      "date": 1679484141282
    },
    {
      "type": "add",
      "id": "944e23c84b41b655",
      "item": {
        "type": "paragraph",
        "id": "944e23c84b41b655",
        "text": "Curiously, Kingma & Welling glaringly neglect the connection between their new architecture and its cognitive counterpart, as do Rezende and co-authors. This striking connection is ignored in all the further refinement on the variational autoencoder in the deep learning community, and it is first acknowledged only by Ofner and Stober (2018). We are rather inclined to push the difference in scope between the earlier artificial neural network community engaged in cognitive explorations and deep learning even further. To the extent that modelers withdrew from pursuing cognitive investigations, the design of neural models was allowed much more freedom in adopting mathematical solutions alien to mental processes."
      },
      "after": "7adbac648c72fbc0",
      "date": 1679484147316
    },
    {
      "type": "add",
      "id": "fe5a7cb7f1b91ab0",
      "item": {
        "type": "paragraph",
        "id": "fe5a7cb7f1b91ab0",
        "text": "However, we argue that now deep learning, despite this recent tradition, can and should have its say in cognitive science. There is at least one simple reason: the engineering objectives of deep learning have been met with such success that, for the first time, we have artificial models performing complex cognitive tasks at human performance level. The era of toy worlds in which models are restricted to highly simplified versions of cognitive capabilities is over. We now have empirical examples of algorithms solving cognitive tasks at the full scale of complexity."
      },
      "after": "944e23c84b41b655",
      "date": 1679484202240
    },
    {
      "type": "add",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017;Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018;Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “general artificial intelligence”. These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) andLandgrebe and Smith (2019) ."
      },
      "after": "fe5a7cb7f1b91ab0",
      "date": 1679484257239
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017;Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018;Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “general artificial intelligence”. These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019) ."
      },
      "date": 1679484281826
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017;Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018;Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “general artificial intelligence”. These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019). >> subsymbolic"
      },
      "date": 1679484308852
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017; Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018;Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “general artificial intelligence”. These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019). >> subsymbolic"
      },
      "date": 1679484324710
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017; Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018; Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “general artificial intelligence”. These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019). >> subsymbolic"
      },
      "date": 1679484335756
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017; Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018; Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “[[General Artificial Intelligence]]general artificial intelligence”. These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019). >> subsymbolic general artificial intelligence"
      },
      "date": 1679484402760
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017; Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018; Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “[[General Artificial Intelligence]]”. These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019). >> subsymbolic general artificial intelligence"
      },
      "date": 1679484416891
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017; Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018; Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “[[General Artificial Intelligence]]”. These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019). >> general artificial intelligence subsymbolic"
      },
      "date": 1679484486325
    },
    {
      "type": "edit",
      "id": "944e23c84b41b655",
      "item": {
        "type": "paragraph",
        "id": "944e23c84b41b655",
        "text": "Curiously, Kingma & Welling glaringly neglect the connection between their new architecture and its cognitive counterpart, as do Rezende and co-authors. This striking connection is ignored in all the further refinement on the variational autoencoder in the deep learning community, and it is first acknowledged only by Ofner and Stober (2018). "
      },
      "date": 1679487557287
    },
    {
      "type": "add",
      "id": "862de59b868e16be",
      "item": {
        "type": "paragraph",
        "id": "862de59b868e16be",
        "text": "We are rather inclined to push the difference in scope between the earlier artificial neural network community engaged in cognitive explorations and deep learning even further. To the extent that modelers withdrew from pursuing cognitive investigations, the design of neural models was allowed much more freedom in adopting mathematical solutions alien to mental processes."
      },
      "after": "944e23c84b41b655",
      "date": 1679487557861
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017; Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018; Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “[[General Artificial Intelligence]]”. "
      },
      "date": 1679487648331
    },
    {
      "type": "add",
      "id": "d83cfebe16883836",
      "item": {
        "type": "paragraph",
        "id": "d83cfebe16883836",
        "text": "These are important themes, but the focus of this paper is different. Our reflections are on how certain empirical achievements of deep learning may illuminate crucial debates in cognitive science. An easy and immediate consideration is that deep learning may lead to a revision of old debates in cognitive science, in which the first generation of neural networks was engaged, such as symbolic/subsymbolic, or innate/acquired knowledge. Some of these issues are already discussed in a few of the works just cited, like in Marcus (2018) and Landgrebe and Smith (2019). >> subsymbolic"
      },
      "after": "93d988c3677f8cbf",
      "date": 1679487665164
    },
    {
      "type": "edit",
      "id": "93d988c3677f8cbf",
      "item": {
        "type": "paragraph",
        "id": "93d988c3677f8cbf",
        "text": "The resonance of the successes of deep learning has already stirred up reflections and discussions within cognitive science and philosophy (Cichy & Kaiser, 2019; Edelman, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2017; Landgrebe & Smith, 2019; López-Rubio, 2018; Marcus, 2018; Ras, van Gerven, & Haselager, 2018; Schubbach, 2019 ). However, most of the focus of these reflections is about the chances and limits of deep learning in fulfilling the promises of artificial intelligence, in particular its possibility to reach the most coveted goal, the so-called “[[General Artificial Intelligence]]”. >> general artificial intelligence"
      },
      "date": 1679487671798
    },
    {
      "type": "edit",
      "id": "f1d408ea1c1380ae",
      "item": {
        "type": "paragraph",
        "id": "f1d408ea1c1380ae",
        "text": "PERCONTI, Pietro and PLEBE, Alessio, 2020. Deep learning and cognitive science. Cognition. 1 October 2020. Vol. 203, p. 104365. DOI 10.1016/j.cognition.2020.104365. "
      },
      "date": 1679487737280
    },
    {
      "type": "add",
      "id": "a1c809cf18ad4f70",
      "item": {
        "type": "paragraph",
        "id": "a1c809cf18ad4f70",
        "text": "In recent years, the family of algorithms collected under the term “deep learning” has revolutionized artificial intelligence, enabling machines to reach human-like performances in many complex cognitive tasks. Although deep learning models are grounded in the connectionist paradigm, their recent advances were basically developed with engineering goals in mind. Despite of their applied focus, deep learning models eventually seem fruitful for cognitive purposes. "
      },
      "after": "f1d408ea1c1380ae",
      "date": 1679487744814
    },
    {
      "type": "add",
      "id": "404b7380f5202e7d",
      "item": {
        "type": "paragraph",
        "id": "404b7380f5202e7d",
        "text": "This can be thought as a kind of biological exaptation, where a physiological structure becomes applicable for a function different from that for which it was selected. In this paper, it will be argued that it is time for cognitive science to seriously come to terms with deep learning, and we try to spell out the reasons why this is the case. First, the path of the evolution of deep learning from the connectionist project is traced, demonstrating the remarkable continuity, and the differences as well. Then, it will be considered how deep learning models can be useful for many cognitive topics, especially those where it has achieved performance comparable to humans, from perception to language. It will be maintained that deep learning poses questions that cognitive sciences should try to answer. One of such questions is the reasons why deep convolutional models that are disembodied, inactive, unaware of context, and static, are by far the closest to the patterns of activation in the brain visual system.\n"
      },
      "after": "a1c809cf18ad4f70",
      "date": 1679487745621
    },
    {
      "type": "edit",
      "id": "404b7380f5202e7d",
      "item": {
        "type": "paragraph",
        "id": "404b7380f5202e7d",
        "text": "This can be thought as a kind of biological exaptation, where a physiological structure becomes applicable for a function different from that for which it was selected. "
      },
      "date": 1679487775321
    },
    {
      "type": "add",
      "id": "7f2412ef2524efb1",
      "item": {
        "type": "paragraph",
        "id": "7f2412ef2524efb1",
        "text": "In this paper, it will be argued that it is time for cognitive science to seriously come to terms with deep learning, and we try to spell out the reasons why this is the case. First, the path of the evolution of deep learning from the connectionist project is traced, demonstrating the remarkable continuity, and the differences as well. Then, it will be considered how deep learning models can be useful for many cognitive topics, especially those where it has achieved performance comparable to humans, from perception to language. It will be maintained that deep learning poses questions that cognitive sciences should try to answer. One of such questions is the reasons why deep convolutional models that are disembodied, inactive, unaware of context, and static, are by far the closest to the patterns of activation in the brain visual system.\n"
      },
      "after": "404b7380f5202e7d",
      "date": 1679487776295
    },
    {
      "type": "edit",
      "id": "7f2412ef2524efb1",
      "item": {
        "type": "paragraph",
        "id": "7f2412ef2524efb1",
        "text": "In this paper, it will be argued that it is time for cognitive science to seriously come to terms with deep learning, and we try to spell out the reasons why this is the case. "
      },
      "date": 1679487790255
    },
    {
      "type": "add",
      "id": "33cdaa92f734ee1a",
      "item": {
        "type": "paragraph",
        "id": "33cdaa92f734ee1a",
        "text": "First, the path of the evolution of deep learning from the connectionist project is traced, demonstrating the remarkable continuity, and the differences as well. Then, it will be considered how deep learning models can be useful for many cognitive topics, especially those where it has achieved performance comparable to humans, from perception to language. It will be maintained that deep learning poses questions that cognitive sciences should try to answer. One of such questions is the reasons why deep convolutional models that are disembodied, inactive, unaware of context, and static, are by far the closest to the patterns of activation in the brain visual system.\n"
      },
      "after": "7f2412ef2524efb1",
      "date": 1679487791323
    },
    {
      "type": "edit",
      "id": "33cdaa92f734ee1a",
      "item": {
        "type": "paragraph",
        "id": "33cdaa92f734ee1a",
        "text": "First, the path of the evolution of deep learning from the connectionist project is traced, demonstrating the remarkable continuity, and the differences as well. "
      },
      "date": 1679487800303
    },
    {
      "type": "add",
      "id": "3c3188cc1786bc9b",
      "item": {
        "type": "paragraph",
        "id": "3c3188cc1786bc9b",
        "text": "Then, it will be considered how deep learning models can be useful for many cognitive topics, especially those where it has achieved performance comparable to humans, from perception to language. It will be maintained that deep learning poses questions that cognitive sciences should try to answer. One of such questions is the reasons why deep convolutional models that are disembodied, inactive, unaware of context, and static, are by far the closest to the patterns of activation in the brain visual system.\n"
      },
      "after": "33cdaa92f734ee1a",
      "date": 1679487801320
    }
  ]
}