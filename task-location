{
  "title": "Task Location",
  "story": [
    {
      "type": "paragraph",
      "id": "138790c38aba36f2",
      "text": "Contrary to the interpretation of implied by the title of the original GPT-3 paper by Brown et al. [3], Language models are few-shot learners, we argue that GPT-3 is often not actually learning the task during run time from few-shot examples. "
    },
    {
      "type": "paragraph",
      "id": "80c3af94317bed46",
      "text": "Rather than [[Instruction]], the method’s primary function is task location in the model’s existing space of learned tasks. This is evidenced by the effectiveness of alternative prompts which, with no examples, can elicit comparable or superior performance to the few-shot format."
    },
    {
      "type": "paragraph",
      "id": "83e9d866da15d1fa",
      "text": "This motivates new approaches which explicitly pursue the goal of task location. We propose exploring more general methods of [[Prompt Programming]] and specifically techniques for communicating task intention and structure to a self-supervised model in the modality it was trained: [[Natural Language]]. With a few caveats, we want to find prompts which we would expect a human to complete in a way that accomplishes the desired task."
    },
    {
      "type": "paragraph",
      "id": "17770d206f29f370",
      "text": "In this work, we investigate the few-shot paradigm and find that its performance can be matched or exceeded by simple 0-shot prompts. We explore the nature of successful 0-shot prompts and propose general methods of prompt programming through the lens of natural language [[Semiotics]]. We demonstrate novel prompts which force a language model to break a problem into components before producing a verdict, and we introduce the concept of metaprompt programming, an approach which offloads the job of writing a task-specific prompt to the language model itself. Finally, we discuss how these ideas can be incorporated into existing and future benchmarks to allow us to better probe the capabilities of large language models."
    },
    {
      "type": "pagefold",
      "id": "6e47d9b56b60e738",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "7f97a6bb8fcf767c",
      "text": "REYNOLDS, Laria and MCDONELL, Kyle, 2021. [[Prompt Programming]] for [[Large Language Model]]s: Beyond the Few-Shot Paradigm. In: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Online. New York, NY, USA: Association for Computing Machinery. 2021. p. 1–7. [Accessed 29 January 2023]. CHI EA ’21. ISBN 978-1-4503-8095-9. DOI 10.1145/3411763.3451760. "
    },
    {
      "type": "paragraph",
      "id": "2bfe61fbf03b1188",
      "text": "⇒ [[Direct Task Specification]]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Task Location",
        "story": []
      },
      "date": 1675074522838
    },
    {
      "item": {
        "type": "factory",
        "id": "138790c38aba36f2"
      },
      "id": "138790c38aba36f2",
      "type": "add",
      "date": 1675074534101
    },
    {
      "type": "edit",
      "id": "138790c38aba36f2",
      "item": {
        "type": "paragraph",
        "id": "138790c38aba36f2",
        "text": "Contrary to the interpretation of implied by the title of the original GPT-3 paper by Brown et al. [3], Language models are few-shot learners, we argue that GPT-3 is often not actually learning the task during run time from few-shot examples. Rather than instruction, the method’s primary function is task location in the model’s existing space of learned tasks. This is evidenced by the effectiveness of alternative prompts which, with no examples, can elicit comparable or superior performance to the few-shot format."
      },
      "date": 1675074535756
    },
    {
      "item": {
        "type": "factory",
        "id": "6e47d9b56b60e738"
      },
      "id": "6e47d9b56b60e738",
      "type": "add",
      "after": "138790c38aba36f2",
      "date": 1675074546620
    },
    {
      "type": "edit",
      "id": "6e47d9b56b60e738",
      "item": {
        "type": "pagefold",
        "id": "6e47d9b56b60e738",
        "text": "~"
      },
      "date": 1675074549631
    },
    {
      "item": {
        "type": "factory",
        "id": "7f97a6bb8fcf767c"
      },
      "id": "7f97a6bb8fcf767c",
      "type": "add",
      "after": "6e47d9b56b60e738",
      "date": 1675074550858
    },
    {
      "type": "edit",
      "id": "7f97a6bb8fcf767c",
      "item": {
        "type": "paragraph",
        "id": "7f97a6bb8fcf767c",
        "text": "REYNOLDS, Laria and MCDONELL, Kyle, 2021. Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm. In: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Online. New York, NY, USA: Association for Computing Machinery. 2021. p. 1–7. [Accessed 29 January 2023]. CHI EA ’21. ISBN 978-1-4503-8095-9. DOI 10.1145/3411763.3451760. "
      },
      "date": 1675074562827
    },
    {
      "type": "edit",
      "id": "7f97a6bb8fcf767c",
      "item": {
        "type": "paragraph",
        "id": "7f97a6bb8fcf767c",
        "text": "REYNOLDS, Laria and MCDONELL, Kyle, 2021. [[Prompt Programming]] for [[Large Language Model]]s: Beyond the Few-Shot Paradigm. In: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Online. New York, NY, USA: Association for Computing Machinery. 2021. p. 1–7. [Accessed 29 January 2023]. CHI EA ’21. ISBN 978-1-4503-8095-9. DOI 10.1145/3411763.3451760. "
      },
      "date": 1675074579761
    },
    {
      "type": "edit",
      "id": "138790c38aba36f2",
      "item": {
        "type": "paragraph",
        "id": "138790c38aba36f2",
        "text": "Contrary to the interpretation of implied by the title of the original GPT-3 paper by Brown et al. [3], Language models are few-shot learners, we argue that GPT-3 is often not actually learning the task during run time from few-shot examples. "
      },
      "date": 1675074633378
    },
    {
      "type": "add",
      "id": "80c3af94317bed46",
      "item": {
        "type": "paragraph",
        "id": "80c3af94317bed46",
        "text": "Rather than [[Instruction]], the method’s primary function is task location in the model’s existing space of learned tasks. This is evidenced by the effectiveness of alternative prompts which, with no examples, can elicit comparable or superior performance to the few-shot format."
      },
      "after": "138790c38aba36f2",
      "date": 1675074645370
    },
    {
      "type": "add",
      "id": "83e9d866da15d1fa",
      "item": {
        "type": "paragraph",
        "id": "83e9d866da15d1fa",
        "text": "This motivates new approaches which explicitly pursue the goal of task location. We propose exploring more general methods of prompt programming and specifically techniques for communicating task intention and structure to a self-supervised model in the modality it was trained: natural language. With a few caveats, we want to find prompts which we would expect a human to complete in a way that accomplishes the desired task."
      },
      "after": "80c3af94317bed46",
      "date": 1675074729751
    },
    {
      "type": "edit",
      "id": "83e9d866da15d1fa",
      "item": {
        "type": "paragraph",
        "id": "83e9d866da15d1fa",
        "text": "This motivates new approaches which explicitly pursue the goal of task location. We propose exploring more general methods of prompt programming and specifically techniques for communicating task intention and structure to a self-supervised model in the modality it was trained: [[Natural Language]]. With a few caveats, we want to find prompts which we would expect a human to complete in a way that accomplishes the desired task."
      },
      "date": 1675074760299
    },
    {
      "type": "edit",
      "id": "83e9d866da15d1fa",
      "item": {
        "type": "paragraph",
        "id": "83e9d866da15d1fa",
        "text": "This motivates new approaches which explicitly pursue the goal of task location. We propose exploring more general methods of [[Prompt Programming]] and specifically techniques for communicating task intention and structure to a self-supervised model in the modality it was trained: [[Natural Language]]. With a few caveats, we want to find prompts which we would expect a human to complete in a way that accomplishes the desired task."
      },
      "date": 1675075028822
    },
    {
      "type": "add",
      "id": "17770d206f29f370",
      "item": {
        "type": "paragraph",
        "id": "17770d206f29f370",
        "text": "In this work, we investigate the few-shot paradigm and find that its performance can be matched or exceeded by simple 0-shot prompts. We explore the nature of successful 0-shot prompts and propose general methods of prompt programming through the lens of natural language semiotics. We demonstrate novel prompts which force a language model to break a problem into components before producing a verdict, and we introduce the concept of metaprompt programming, an approach which offloads the job of writing a task-specific prompt to the language model itself. Finally, we discuss how these ideas can be incorporated into existing and future benchmarks to allow us to better probe the capabilities of large language models."
      },
      "after": "83e9d866da15d1fa",
      "date": 1675075064442
    },
    {
      "type": "edit",
      "id": "17770d206f29f370",
      "item": {
        "type": "paragraph",
        "id": "17770d206f29f370",
        "text": "In this work, we investigate the few-shot paradigm and find that its performance can be matched or exceeded by simple 0-shot prompts. We explore the nature of successful 0-shot prompts and propose general methods of prompt programming through the lens of natural language [[Semiotics]]. We demonstrate novel prompts which force a language model to break a problem into components before producing a verdict, and we introduce the concept of metaprompt programming, an approach which offloads the job of writing a task-specific prompt to the language model itself. Finally, we discuss how these ideas can be incorporated into existing and future benchmarks to allow us to better probe the capabilities of large language models."
      },
      "date": 1675075169152
    },
    {
      "item": {
        "type": "factory",
        "id": "2bfe61fbf03b1188"
      },
      "id": "2bfe61fbf03b1188",
      "type": "add",
      "after": "7f97a6bb8fcf767c",
      "date": 1675075894262
    },
    {
      "type": "edit",
      "id": "2bfe61fbf03b1188",
      "item": {
        "type": "paragraph",
        "id": "2bfe61fbf03b1188",
        "text": "⇒ [[Direct Task Specification]]"
      },
      "date": 1675075903575
    }
  ]
}