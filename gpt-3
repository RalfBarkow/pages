{
  "title": "GPT-3",
  "story": [
    {
      "type": "markdown",
      "id": "2918732cd9ecff1c",
      "text": "Articles headlines that are too long brought us here\n\n> But most articles have headlines that make natural post titles except that they are toooo long. [https://matrix.to/#/!ORfrUEFeWFcHAMLFLr:matrix.org/$1627961574241812nCoru:matrix.org?via=matrix.org&via=dreyeck.freedombox.rocks&via=matrix.goatpen.co matrix]\n"
    },
    {
      "type": "markdown",
      "id": "b299ba333f364b9c",
      "text": "This leads [[Ward]] to think that \n> this would be a very practical application of GPT-3: rewriting headlines to be short and to the point. [https://matrix.to/#/!ORfrUEFeWFcHAMLFLr:matrix.org/$1627961649241839NBlqG:matrix.org?via=matrix.org&via=dreyeck.freedombox.rocks&via=matrix.goatpen.co matrix]"
    },
    {
      "type": "markdown",
      "id": "d744a581eff959a5",
      "text": "See also\n- [https://en.wikipedia.org/wiki/GPT-3 wikipedia]\n- [https://github.com/openai/gpt-3 GitHub]"
    },
    {
      "type": "markdown",
      "id": "64c14ab5f39c521c",
      "text": "- Language Models are Few-Shot  Learners [https://arxiv.org/abs/2005.14165 arxiv]\n> Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. "
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "GPT-3",
        "story": []
      },
      "date": 1627973502321
    },
    {
      "item": {
        "type": "factory",
        "id": "2918732cd9ecff1c"
      },
      "id": "2918732cd9ecff1c",
      "type": "add",
      "date": 1627973512978
    },
    {
      "type": "edit",
      "id": "2918732cd9ecff1c",
      "item": {
        "type": "markdown",
        "id": "2918732cd9ecff1c",
        "text": "But most articles have headlines that make natural post titles except that they are toooo long.\nThis leads me to think that this would be a very practical application of GPT-3: rewriting headlines to be short and to the point."
      },
      "date": 1627973525524
    },
    {
      "type": "edit",
      "id": "2918732cd9ecff1c",
      "item": {
        "type": "markdown",
        "id": "2918732cd9ecff1c",
        "text": "> But most articles have headlines that make natural post titles except that they are toooo long.\nThis leads [[Ward]] to think that this would be a very practical application of GPT-3: rewriting headlines to be short and to the point."
      },
      "date": 1627973536857
    },
    {
      "item": {
        "type": "factory",
        "id": "d744a581eff959a5"
      },
      "id": "d744a581eff959a5",
      "type": "add",
      "after": "2918732cd9ecff1c",
      "date": 1627973566697
    },
    {
      "type": "edit",
      "id": "d744a581eff959a5",
      "item": {
        "type": "markdown",
        "id": "d744a581eff959a5",
        "text": "See also\n[https://en.wikipedia.org/wiki/GPT-3]"
      },
      "date": 1627973577168
    },
    {
      "type": "edit",
      "id": "d744a581eff959a5",
      "item": {
        "type": "markdown",
        "id": "d744a581eff959a5",
        "text": "See also\n- [https://en.wikipedia.org/wiki/GPT-3 wikipedia]\n- "
      },
      "date": 1627973662636
    },
    {
      "type": "edit",
      "id": "d744a581eff959a5",
      "item": {
        "type": "markdown",
        "id": "d744a581eff959a5",
        "text": "See also\n- [https://en.wikipedia.org/wiki/GPT-3 wikipedia]\n- [https://github.com/openai/gpt-3 GitHub]"
      },
      "date": 1627973681152
    },
    {
      "item": {
        "type": "factory",
        "id": "64c14ab5f39c521c"
      },
      "id": "64c14ab5f39c521c",
      "type": "add",
      "after": "d744a581eff959a5",
      "date": 1627973732104
    },
    {
      "type": "edit",
      "id": "64c14ab5f39c521c",
      "item": {
        "type": "markdown",
        "id": "64c14ab5f39c521c",
        "text": "- Language Models are Few-Shot Learners"
      },
      "date": 1627973738044
    },
    {
      "type": "edit",
      "id": "64c14ab5f39c521c",
      "item": {
        "type": "markdown",
        "id": "64c14ab5f39c521c",
        "text": "- Language Models are Few-Shot  [https://arxiv.org/abs/2005.14165 arxiv]"
      },
      "date": 1627973867153
    },
    {
      "type": "edit",
      "id": "64c14ab5f39c521c",
      "item": {
        "type": "markdown",
        "id": "64c14ab5f39c521c",
        "text": "- Language Models are Few-Shot  Learners [https://arxiv.org/abs/2005.14165 arxiv]"
      },
      "date": 1627973880650
    },
    {
      "type": "edit",
      "id": "64c14ab5f39c521c",
      "item": {
        "type": "markdown",
        "id": "64c14ab5f39c521c",
        "text": "- Language Models are Few-Shot  Learners [https://arxiv.org/abs/2005.14165 arxiv]\n> Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. "
      },
      "date": 1627973904456
    },
    {
      "type": "edit",
      "id": "2918732cd9ecff1c",
      "item": {
        "type": "markdown",
        "id": "2918732cd9ecff1c",
        "text": "> But most articles have headlines that make natural post titles except that they are toooo long.\nThis leads [[Ward]] to think that this would be a very practical application of GPT-3: rewriting headlines to be short and to the point. [https://matrix.to/#/!ORfrUEFeWFcHAMLFLr:matrix.org/$1627961574241812nCoru:matrix.org?via=matrix.org&via=dreyeck.freedombox.rocks&via=matrix.goatpen.co matrix]"
      },
      "date": 1627975558292
    },
    {
      "type": "edit",
      "id": "2918732cd9ecff1c",
      "item": {
        "type": "markdown",
        "id": "2918732cd9ecff1c",
        "text": "Articles headlines that are too long brought us here\n\n> But most articles have headlines that make natural post titles except that they are toooo long.\nThis leads [[Ward]] to think that this would be a very practical application of GPT-3: rewriting headlines to be short and to the point."
      },
      "date": 1627975691714
    },
    {
      "type": "edit",
      "id": "2918732cd9ecff1c",
      "item": {
        "type": "markdown",
        "id": "2918732cd9ecff1c",
        "text": "Articles headlines that are too long brought us here\n\n> But most articles have headlines that make natural post titles except that they are toooo long.\n"
      },
      "date": 1627975709472
    },
    {
      "type": "add",
      "id": "b299ba333f364b9c",
      "item": {
        "type": "markdown",
        "id": "b299ba333f364b9c",
        "text": "This leads [[Ward]] to think that this would be a very practical application of GPT-3: rewriting headlines to be short and to the point."
      },
      "after": "2918732cd9ecff1c",
      "date": 1627975710474
    },
    {
      "type": "edit",
      "id": "b299ba333f364b9c",
      "item": {
        "type": "markdown",
        "id": "b299ba333f364b9c",
        "text": "This leads [[Ward]] to think that \n> this would be a very practical application of GPT-3: rewriting headlines to be short and to the point."
      },
      "date": 1627975722538
    },
    {
      "type": "edit",
      "id": "2918732cd9ecff1c",
      "item": {
        "type": "markdown",
        "id": "2918732cd9ecff1c",
        "text": "Articles headlines that are too long brought us here\n\n> But most articles have headlines that make natural post titles except that they are toooo long. [https://matrix.to/#/!ORfrUEFeWFcHAMLFLr:matrix.org/$1627961574241812nCoru:matrix.org?via=matrix.org&via=dreyeck.freedombox.rocks&via=matrix.goatpen.co matrix]\n"
      },
      "date": 1627975856926
    },
    {
      "type": "edit",
      "id": "b299ba333f364b9c",
      "item": {
        "type": "markdown",
        "id": "b299ba333f364b9c",
        "text": "This leads [[Ward]] to think that \n> this would be a very practical application of GPT-3: rewriting headlines to be short and to the point. [https://matrix.to/#/!ORfrUEFeWFcHAMLFLr:matrix.org/$1627961649241839NBlqG:matrix.org?via=matrix.org&via=dreyeck.freedombox.rocks&via=matrix.goatpen.co matrix]"
      },
      "date": 1627975905126
    }
  ]
}