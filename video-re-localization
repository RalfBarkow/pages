{
  "title": "Video Re-Localization",
  "story": [
    {
      "type": "paragraph",
      "id": "d6ee0a6045f28537",
      "text": "FENG, Yang, MA, Lin, LIU, Wei, ZHANG, Tong and LUO, Jiebo, 2018. Video re-localization. In: Proceedings of the European Conference on Computer Vision (ECCV). 2018. p. 51–66. [http://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.html page] [https://arxiv.org/abs/1808.01575 arxiv] [Accessed 13 February 2024]. \n"
    },
    {
      "type": "paragraph",
      "id": "62e3f2a4c301ac5b",
      "text": "Many methods have been developed to help people find the video content they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely video re-localization, to address this need. Video re-localization is an important enabling technology with many applications, such as fast seeking in videos, video copy detection, as well as video surveillance. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence, and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with the localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the baseline methods. Our code is available at: [https://github.com/fengyang0317/video_reloc github]."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Video Re-Localization",
        "story": []
      },
      "date": 1707859965328
    },
    {
      "type": "edit",
      "id": "d6ee0a6045f28537",
      "item": {
        "type": "paragraph",
        "id": "d6ee0a6045f28537",
        "text": "\nFENG, Yang, MA, Lin, LIU, Wei, ZHANG, Tong and LUO, Jiebo, 2018. Video re-localization. In: Proceedings of the European Conference on Computer Vision (ECCV). Online. 2018. p. 51–66. Available from: http://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.html [Accessed 13 February 2024]. \n"
      },
      "date": 1707859967750
    },
    {
      "type": "edit",
      "id": "d6ee0a6045f28537",
      "item": {
        "type": "paragraph",
        "id": "d6ee0a6045f28537",
        "text": "FENG, Yang, MA, Lin, LIU, Wei, ZHANG, Tong and LUO, Jiebo, 2018. Video re-localization. In: Proceedings of the European Conference on Computer Vision (ECCV) 2018. p. 51–66. Available from: http://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.html [Accessed 13 February 2024]. \n"
      },
      "date": 1707859978927
    },
    {
      "type": "edit",
      "id": "d6ee0a6045f28537",
      "item": {
        "type": "paragraph",
        "id": "d6ee0a6045f28537",
        "text": "FENG, Yang, MA, Lin, LIU, Wei, ZHANG, Tong and LUO, Jiebo, 2018. Video re-localization. In: Proceedings of the European Conference on Computer Vision (ECCV). 2018. p. 51–66. Available from: http://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.html [Accessed 13 February 2024]. \n"
      },
      "date": 1707860004950
    },
    {
      "type": "edit",
      "id": "d6ee0a6045f28537",
      "item": {
        "type": "paragraph",
        "id": "d6ee0a6045f28537",
        "text": "FENG, Yang, MA, Lin, LIU, Wei, ZHANG, Tong and LUO, Jiebo, 2018. Video re-localization. In: Proceedings of the European Conference on Computer Vision (ECCV). 2018. p. 51–66. [http://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.html page] [Accessed 13 February 2024]. \n"
      },
      "date": 1707860025742
    },
    {
      "item": {
        "type": "factory",
        "id": "62e3f2a4c301ac5b"
      },
      "id": "62e3f2a4c301ac5b",
      "type": "add",
      "after": "d6ee0a6045f28537",
      "date": 1707860049363
    },
    {
      "type": "edit",
      "id": "62e3f2a4c301ac5b",
      "item": {
        "type": "paragraph",
        "id": "62e3f2a4c301ac5b",
        "text": "Many methods have been developed to help people find the video content they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely video re-localization, to address this need. Video re-localization is an important enabling technology with many applications, such as fast seeking in videos, video copy detection, as well as video surveillance. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence, and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with the localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the baseline methods. Our code is available at: [https://github.com/fengyang0317/video reloc."
      },
      "date": 1707860057901
    },
    {
      "type": "edit",
      "id": "62e3f2a4c301ac5b",
      "item": {
        "type": "paragraph",
        "id": "62e3f2a4c301ac5b",
        "text": "Many methods have been developed to help people find the video content they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely video re-localization, to address this need. Video re-localization is an important enabling technology with many applications, such as fast seeking in videos, video copy detection, as well as video surveillance. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence, and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with the localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the baseline methods. Our code is available at: [https://github.com/fengyang0317/video–reloc github]."
      },
      "date": 1707860071966
    },
    {
      "type": "fork",
      "date": 1707860194614
    },
    {
      "type": "edit",
      "id": "62e3f2a4c301ac5b",
      "item": {
        "type": "paragraph",
        "id": "62e3f2a4c301ac5b",
        "text": "Many methods have been developed to help people find the video content they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely video re-localization, to address this need. Video re-localization is an important enabling technology with many applications, such as fast seeking in videos, video copy detection, as well as video surveillance. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence, and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with the localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the baseline methods. Our code is available at: [https://github.com/fengyang0317/video_reloc github]."
      },
      "date": 1707860200109
    },
    {
      "type": "edit",
      "id": "d6ee0a6045f28537",
      "item": {
        "type": "paragraph",
        "id": "d6ee0a6045f28537",
        "text": "FENG, Yang, MA, Lin, LIU, Wei, ZHANG, Tong and LUO, Jiebo, 2018. Video re-localization. In: Proceedings of the European Conference on Computer Vision (ECCV). 2018. p. 51–66. [http://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Feng_Video_Re-localization_via_ECCV_2018_paper.html page] [https://arxiv.org/abs/1808.01575 arxiv] [Accessed 13 February 2024]. \n"
      },
      "date": 1707860334926
    }
  ]
}