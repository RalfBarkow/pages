{
  "title": "Paragraph Vector",
  "story": [
    {
      "type": "paragraph",
      "id": "bbbf9a267cb868d1",
      "text": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, “powerful,” “strong” and “Paris” are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
    },
    {
      "type": "pagefold",
      "id": "bcd3cf8ffb732c25",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "c0f429e761e5a9de",
      "text": "LE, Quoc and MIKOLOV, Tomas, 2014. Distributed representations of sentences and documents. In: International conference on machine learning. PMLR. 2014. p. 1188–1196. [http://proceedings.mlr.press/v32/le14.html?ref=https://githubhelp.com page] [Accessed 20 March 2024]. [http://proceedings.mlr.press/v32/le14.pdf pdf]\n"
    },
    {
      "type": "paragraph",
      "id": "9adcb500c3519c40",
      "text": "This framework is similar to the framework presented in Figure 1; the only change is the additional paragraph token that is mapped to a vector via matrix D. In this model, the concatenation or average of this vector with a context of three words is used to predict the fourth word. The paragraph vector represents the missing information from the current context and can act as a memory of the topic of the paragraph."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Paragraph Vector",
        "story": []
      },
      "date": 1710938447608
    },
    {
      "item": {
        "type": "paragraph",
        "id": "c0f429e761e5a9de",
        "text": "LE, Quoc and MIKOLOV, Tomas, 2014. Distributed representations of sentences and documents. In: International conference on machine learning. PMLR. 2014. p. 1188–1196. [http://proceedings.mlr.press/v32/le14.html?ref=https://githubhelp.com page] [Accessed 20 March 2024]. [http://proceedings.mlr.press/v32/le14.pdf pdf]\n"
      },
      "id": "c0f429e761e5a9de",
      "type": "add",
      "date": 1710938448675
    },
    {
      "type": "edit",
      "id": "c0f429e761e5a9de",
      "item": {
        "type": "paragraph",
        "id": "c0f429e761e5a9de",
        "text": "LE, Quoc and MIKOLOV, Tomas, 2014. Distributed representations of sentences and documents. In: International conference on machine learning. PMLR. 2014. p. 1188–1196. [http://proceedings.mlr.press/v32/le14.html?ref=https://githubhelp.com page] [Accessed 20 March 2024]. [http://proceedings.mlr.press/v32/le14.pdf pdf]\n"
      },
      "date": 1710938450147
    },
    {
      "type": "edit",
      "id": "c0f429e761e5a9de",
      "item": {
        "type": "paragraph",
        "id": "c0f429e761e5a9de",
        "text": "LE, Quoc and MIKOLOV, Tomas, 2014. Distributed representations of sentences and documents. In: International conference on machine learning. PMLR. 2014. p. 1188–1196. [http://proceedings.mlr.press/v32/le14.html?ref=https://githubhelp.com page] [Accessed 20 March 2024]. [http://proceedings.mlr.press/v32/le14.pdf pdf]\n"
      },
      "date": 1710938469748
    },
    {
      "type": "edit",
      "id": "c0f429e761e5a9de",
      "item": {
        "type": "paragraph",
        "id": "c0f429e761e5a9de",
        "text": "LE, Quoc and MIKOLOV, Tomas, 2014. Distributed representations of sentences and documents. In: International conference on machine learning. PMLR. 2014. p. 1188–1196. [http://proceedings.mlr.press/v32/le14.html?ref=https://githubhelp.com page] [Accessed 20 March 2024]. [http://proceedings.mlr.press/v32/le14.pdf pdf]\n"
      },
      "date": 1710938488325
    },
    {
      "item": {
        "type": "paragraph",
        "id": "bbbf9a267cb868d1",
        "text": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, “powerful,” “strong” and “Paris” are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
      },
      "id": "bbbf9a267cb868d1",
      "type": "add",
      "after": "c0f429e761e5a9de",
      "date": 1710938523178
    },
    {
      "type": "edit",
      "id": "bbbf9a267cb868d1",
      "item": {
        "type": "paragraph",
        "id": "bbbf9a267cb868d1",
        "text": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, “powerful,” “strong” and “Paris” are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
      },
      "date": 1710938525284
    },
    {
      "id": "bbbf9a267cb868d1",
      "type": "move",
      "order": [
        "bbbf9a267cb868d1",
        "c0f429e761e5a9de"
      ],
      "date": 1710938531478,
      "error": {
        "type": "error",
        "msg": "Internal Server Error",
        "response": "Server Ignoring move. Try reload."
      }
    },
    {
      "type": "fork",
      "date": 1710938533818
    },
    {
      "id": "bcd3cf8ffb732c25",
      "type": "move",
      "order": [
        "bbbf9a267cb868d1",
        "bcd3cf8ffb732c25",
        "c0f429e761e5a9de"
      ],
      "date": 1710938536677,
      "error": {
        "type": "error",
        "msg": "Internal Server Error",
        "response": "Server Ignoring move. Try reload."
      }
    },
    {
      "item": {
        "type": "pagefold",
        "id": "bcd3cf8ffb732c25",
        "text": "~"
      },
      "id": "bcd3cf8ffb732c25",
      "type": "add",
      "after": "c0f429e761e5a9de",
      "date": 1710938535043
    },
    {
      "type": "fork",
      "date": 1710938540028
    },
    {
      "type": "fork",
      "date": 1710938549548
    },
    {
      "type": "fork",
      "site": "localhost:3000",
      "date": 1710938655879
    },
    {
      "type": "remove",
      "date": 1710938658236
    },
    {
      "id": "bcd3cf8ffb732c25",
      "type": "move",
      "order": [
        "bbbf9a267cb868d1",
        "bcd3cf8ffb732c25",
        "c0f429e761e5a9de"
      ],
      "date": 1710938663527
    },
    {
      "type": "edit",
      "id": "bbbf9a267cb868d1",
      "item": {
        "type": "paragraph",
        "id": "bbbf9a267cb868d1",
        "text": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, “powerful,” “strong” and “Paris” are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
      },
      "date": 1710938724611
    },
    {
      "item": {
        "type": "factory",
        "id": "0bc7dad970a16672"
      },
      "id": "0bc7dad970a16672",
      "type": "add",
      "after": "c0f429e761e5a9de",
      "date": 1710938958315
    },
    {
      "type": "edit",
      "id": "0bc7dad970a16672",
      "item": {
        "type": "image",
        "id": "0bc7dad970a16672",
        "text": "Figure2. A framework for learning paragraph vector. This framework is similar to the framework presented in Figure 1; the only change is the additional paragraph token that is mapped to a vector via matrix D. In this model, the concatenation or average of this vector with a context of three words is used to predict the fourth word. The paragraph vector represents the missing information from the current context and can act as a memory of the topic of the paragraph.",
        "size": "wide",
        "width": 418,
        "height": 264,
        "url": "/assets/plugins/image/b2252d263f417983d230aa264379d17e.jpg"
      },
      "date": 1710938974284
    },
    {
      "type": "edit",
      "id": "0bc7dad970a16672",
      "item": {
        "type": "image",
        "id": "0bc7dad970a16672",
        "text": "Figure 2. A framework for learning paragraph vector. ",
        "size": "wide",
        "width": 418,
        "height": 264,
        "url": "/assets/plugins/image/b2252d263f417983d230aa264379d17e.jpg"
      },
      "date": 1710938996594
    },
    {
      "type": "edit",
      "id": "9adcb500c3519c40",
      "item": {
        "type": "paragraph",
        "id": "9adcb500c3519c40",
        "text": "This framework is similar to the framework presented in Figure 1; the only change is the additional paragraph token that is mapped to a vector via matrix D. In this model, the concatenation or average of this vector with a context of three words is used to predict the fourth word. The paragraph vector represents the missing information from the current context and can act as a memory of the topic of the paragraph."
      },
      "date": 1710938999511
    },
    {
      "type": "fork",
      "site": "wiki.ralfbarkow.ch",
      "date": 1710944944054
    },
    {
      "id": "0bc7dad970a16672",
      "type": "remove",
      "removedTo": {
        "page": "Doc2Vec"
      },
      "date": 1710946086178
    }
  ]
}