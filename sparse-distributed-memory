{
  "title": "Sparse Distributed Memory",
  "story": [
    {
      "type": "paragraph",
      "id": "ed575020faf0b8a2",
      "text": "While [[Attention]] has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. "
    },
    {
      "type": "paragraph",
      "id": "165f1feb3f2eecf5",
      "text": "Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva’s Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention."
    },
    {
      "type": "pagefold",
      "id": "90fb2c30fbb870be",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "4b812c1544cad7e8",
      "text": "BRICKEN, Trenton and PEHLEVAN, Cengiz, 2021. Attention Approximates Sparse Distributed Memory. In: Advances in Neural Information Processing Systems. Online. Curran Associates, Inc. 2021. p. 15301–15315. [Accessed 16 January 2023]. Available from: https://proceedings.neurips.cc/paper/2021/hash/8171ac2c5544a5cb54ac0f38bf477af4-Abstract.html [https://proceedings.neurips.cc/paper/2021/file/8171ac2c5544a5cb54ac0f38bf477af4-Paper.pdf pdf]\n"
    },
    {
      "type": "paragraph",
      "id": "9034b4255cfde4d8",
      "text": "SDM is an associative memory model developed in 1988 to solve the “Best Match Problem”, where we have a set of memories and want to quickly find the “best” match to any given query [13, 14]. In the development of its solution, SDM respected fundamental biological constraints, such as Dale’s law, that synapses are fixed to be either excitatory or inhibitory and cannot dynamically switch (see Section 1 for an SDM overview and [13] or [15] for a deeper review). Despite being developed independently of neuroanatomy, SDM’s biologically plausible solution maps strikingly well onto the cerebellum [13, 16]."
    },
    {
      "type": "paragraph",
      "id": "60e66322dc3c8b2d",
      "text": "This cerebellar relationship is additionally compelling by the fact that cerebellum-like neuroanatomy exists in many other organisms including numerous insects (eg. the Drosophila Mushroom Body) and potentially cephalopods [17, 18, 19, 20, 21]."
    },
    {
      "type": "paragraph",
      "id": "df4b8b63619c98e1",
      "text": "Abstractly, the relationship between SDM and Attention exists because SDM’s read operation uses intersections between high dimensional hyperspheres that approximate the exponential over sum of exponentials that is Attention’s softmax function (Section 2). Establishing that Attention approximates SDM mathematically, we then test it in pre-trained GPT2 Transformer models [3] (Section 3) and simulations (Appendix B.7). We use the Query-Key Normalized Transformer variant [22] to directly show that the relationship to SDM holds well. We then use original GPT2 models to help confirm this result and make it more general."
    },
    {
      "type": "paragraph",
      "id": "543b7b150a2a3f1f",
      "text": "Using the SDM framework, we are able to go beyond Attention and interpret the Transformer architecture as a whole, providing deeper intuition (Section 4). Motivated by this mapping between Attention and SDM, we discuss how Attention can be implemented in the brain by summarizing SDM’s relationship to the cerebellum (Section 5). In related work (Section 6), we link SDM to other memory models [23, 24], including how SDM is a generalization of Hopfield Networks and, in turn, how our results extend work relating Hopfield Networks to Attention [25, 26]. Finally, we discuss limitations, and future research directions that could leverage our work (Section 7)."
    },
    {
      "type": "paragraph",
      "id": "f9ae9a873335a936",
      "text": "⇒ [[Review of Kanerva’s SDM]]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Sparse Distributed Memory",
        "story": []
      },
      "date": 1673853972797
    },
    {
      "item": {
        "type": "factory",
        "id": "4b812c1544cad7e8"
      },
      "id": "4b812c1544cad7e8",
      "type": "add",
      "date": 1673853974411
    },
    {
      "item": {
        "type": "factory",
        "id": "ed575020faf0b8a2"
      },
      "id": "ed575020faf0b8a2",
      "type": "add",
      "after": "4b812c1544cad7e8",
      "date": 1673853984627
    },
    {
      "type": "edit",
      "id": "4b812c1544cad7e8",
      "item": {
        "type": "paragraph",
        "id": "4b812c1544cad7e8",
        "text": "\nBRICKEN, Trenton and PEHLEVAN, Cengiz, 2021. Attention Approximates Sparse Distributed Memory. In: Advances in Neural Information Processing Systems. Online. Curran Associates, Inc. 2021. p. 15301–15315. [Accessed 16 January 2023]. Available from: https://proceedings.neurips.cc/paper/2021/hash/8171ac2c5544a5cb54ac0f38bf477af4-Abstract.html\n"
      },
      "date": 1673853985744
    },
    {
      "type": "edit",
      "id": "ed575020faf0b8a2",
      "item": {
        "type": "paragraph",
        "id": "ed575020faf0b8a2",
        "text": "While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva’s Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention."
      },
      "date": 1673853986328
    },
    {
      "id": "ed575020faf0b8a2",
      "type": "move",
      "order": [
        "ed575020faf0b8a2",
        "4b812c1544cad7e8"
      ],
      "date": 1673853987844
    },
    {
      "item": {
        "type": "factory",
        "id": "90fb2c30fbb870be"
      },
      "id": "90fb2c30fbb870be",
      "type": "add",
      "after": "4b812c1544cad7e8",
      "date": 1673853990126
    },
    {
      "id": "90fb2c30fbb870be",
      "type": "move",
      "order": [
        "ed575020faf0b8a2",
        "90fb2c30fbb870be",
        "4b812c1544cad7e8"
      ],
      "date": 1673853992227
    },
    {
      "type": "edit",
      "id": "90fb2c30fbb870be",
      "item": {
        "type": "pagefold",
        "id": "90fb2c30fbb870be",
        "text": "~"
      },
      "date": 1673853995042
    },
    {
      "type": "edit",
      "id": "ed575020faf0b8a2",
      "item": {
        "type": "paragraph",
        "id": "ed575020faf0b8a2",
        "text": "While [[Attention]] has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva’s Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention."
      },
      "date": 1673854010596
    },
    {
      "type": "edit",
      "id": "ed575020faf0b8a2",
      "item": {
        "type": "paragraph",
        "id": "ed575020faf0b8a2",
        "text": "While [[Attention]] has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. "
      },
      "date": 1673854022784
    },
    {
      "type": "add",
      "id": "165f1feb3f2eecf5",
      "item": {
        "type": "paragraph",
        "id": "165f1feb3f2eecf5",
        "text": "Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva’s Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention."
      },
      "after": "ed575020faf0b8a2",
      "date": 1673854024086
    },
    {
      "item": {
        "type": "factory",
        "id": "9034b4255cfde4d8"
      },
      "id": "9034b4255cfde4d8",
      "type": "add",
      "after": "4b812c1544cad7e8",
      "date": 1673854662558
    },
    {
      "type": "edit",
      "id": "9034b4255cfde4d8",
      "item": {
        "type": "paragraph",
        "id": "9034b4255cfde4d8",
        "text": "SDM is an associative memory model developed in 1988 to solve the “Best Match Problem”, where we have a set of memories and want to quickly find the “best” match to any given query [13, 14]. In the development of its solution, SDM respected fundamental biological constraints, such as Dale’s law, that synapses are fixed to be either excitatory or inhibitory and cannot dynamically switch (see Section 1 for an SDM overview and [13] or [15] for a deeper review). Despite being developed independently of neuroanatomy, SDM’s biologically plausible solution maps strikingly well onto the cerebellum [13, 16].1"
      },
      "date": 1673854664219
    },
    {
      "type": "edit",
      "id": "9034b4255cfde4d8",
      "item": {
        "type": "paragraph",
        "id": "9034b4255cfde4d8",
        "text": "SDM is an associative memory model developed in 1988 to solve the “Best Match Problem”, where we have a set of memories and want to quickly find the “best” match to any given query [13, 14]. In the development of its solution, SDM respected fundamental biological constraints, such as Dale’s law, that synapses are fixed to be either excitatory or inhibitory and cannot dynamically switch (see Section 1 for an SDM overview and [13] or [15] for a deeper review). Despite being developed independently of neuroanatomy, SDM’s biologically plausible solution maps strikingly well onto the cerebellum [13, 16]."
      },
      "date": 1673854700446
    },
    {
      "type": "add",
      "id": "60e66322dc3c8b2d",
      "item": {
        "type": "paragraph",
        "id": "60e66322dc3c8b2d",
        "text": "This cerebellar relationship is additionally compelling by the fact that cerebellum-like neuroanatomy exists in many other organisms including numerous insects (eg. the Drosophila Mushroom Body) and potentially cephalopods [17, 18, 19, 20, 21]."
      },
      "after": "9034b4255cfde4d8",
      "date": 1673854701635
    },
    {
      "item": {
        "type": "factory",
        "id": "df4b8b63619c98e1"
      },
      "id": "df4b8b63619c98e1",
      "type": "add",
      "after": "60e66322dc3c8b2d",
      "date": 1673854724587
    },
    {
      "type": "edit",
      "id": "df4b8b63619c98e1",
      "item": {
        "type": "paragraph",
        "id": "df4b8b63619c98e1",
        "text": "Abstractly, the relationship between SDM and Attention exists because SDM’s read operation uses intersections between high dimensional hyperspheres that approximate the exponential over sum of exponentials that is Attention’s softmax function (Section 2). Establishing that Attention approximates SDM mathematically, we then test it in pre-trained GPT2 Transformer models [3] (Section 3) and simulations (Appendix B.7). We use the Query-Key Normalized Transformer variant [22] to directly show that the relationship to SDM holds well. We then use original GPT2 models to help confirm this result and make it more general."
      },
      "date": 1673854726138
    },
    {
      "item": {
        "type": "factory",
        "id": "543b7b150a2a3f1f"
      },
      "id": "543b7b150a2a3f1f",
      "type": "add",
      "after": "df4b8b63619c98e1",
      "date": 1673854761809
    },
    {
      "type": "edit",
      "id": "543b7b150a2a3f1f",
      "item": {
        "type": "paragraph",
        "id": "543b7b150a2a3f1f",
        "text": "Using the SDM framework, we are able to go beyond Attention and interpret the Transformer architecture as a whole, providing deeper intuition (Section 4). Motivated by this mapping between Attention and SDM, we discuss how Attention can be implemented in the brain by summarizing SDM’s relationship to the cerebellum (Section 5). In related work (Section 6), we link SDM to other memory models [23, 24], including how SDM is a generalization of Hopfield Networks and, in turn, how our results extend work relating Hopfield Networks to Attention [25, 26]. Finally, we discuss limitations, and future research directions that could leverage our work (Section 7)."
      },
      "date": 1673854763202
    },
    {
      "item": {
        "type": "factory",
        "id": "f9ae9a873335a936"
      },
      "id": "f9ae9a873335a936",
      "type": "add",
      "after": "543b7b150a2a3f1f",
      "date": 1673854813100
    },
    {
      "type": "edit",
      "id": "f9ae9a873335a936",
      "item": {
        "type": "paragraph",
        "id": "f9ae9a873335a936",
        "text": "⇒ [[Review of Kanerva’s SDM]]"
      },
      "date": 1673854824628
    },
    {
      "type": "add",
      "id": "a55ff5e998dbd5ad",
      "item": {
        "type": "paragraph",
        "id": "a55ff5e998dbd5ad",
        "text": "These d⇤s are only approximate reference points for later comparisons to Transformer Attention, first and foremost because they assume random patterns to make their derivations tractable. In addition, Transformer Attention will not be optimizing for just one of these objectives, and likely interpolates between these optimal d⇤s as it wants to have both a good critical distance to handle noisy queries and a reasonable memory capacity. These optimal d⇤ are a function of n, r and m. For the Transformer […]"
      },
      "after": "165f1feb3f2eecf5",
      "date": 1673855424690
    },
    {
      "id": "a55ff5e998dbd5ad",
      "type": "remove",
      "date": 1673855485709
    },
    {
      "type": "edit",
      "id": "4b812c1544cad7e8",
      "item": {
        "type": "paragraph",
        "id": "4b812c1544cad7e8",
        "text": "BRICKEN, Trenton and PEHLEVAN, Cengiz, 2021. Attention Approximates Sparse Distributed Memory. In: Advances in Neural Information Processing Systems. Online. Curran Associates, Inc. 2021. p. 15301–15315. [Accessed 16 January 2023]. Available from: https://proceedings.neurips.cc/paper/2021/hash/8171ac2c5544a5cb54ac0f38bf477af4-Abstract.html\n"
      },
      "date": 1673944246410
    },
    {
      "type": "edit",
      "id": "4b812c1544cad7e8",
      "item": {
        "type": "paragraph",
        "id": "4b812c1544cad7e8",
        "text": "BRICKEN, Trenton and PEHLEVAN, Cengiz, 2021. Attention Approximates Sparse Distributed Memory. In: Advances in Neural Information Processing Systems. Online. Curran Associates, Inc. 2021. p. 15301–15315. [Accessed 16 January 2023]. Available from: https://proceedings.neurips.cc/paper/2021/hash/8171ac2c5544a5cb54ac0f38bf477af4-Abstract.html [https://proceedings.neurips.cc/paper/2021/file/8171ac2c5544a5cb54ac0f38bf477af4-Paper.pdf pdf]\n"
      },
      "date": 1673944272014
    }
  ]
}