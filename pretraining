{
  "title": "Pretraining",
  "story": [
    {
      "type": "paragraph",
      "id": "dd0a79655b26f0d9",
      "text": "The phrase “colorless green ideas sleep furiously” has perfect syntax, but any natural speaker knows it’s nonsense. What prewritten rulebook could capture this “unwritten” fact about natural language — or innumerable others?\n\nNLP researchers have tried to square this circle by having neural networks write their own makeshift rulebooks, in a process called pretraining. [https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017/ post]"
    },
    {
      "type": "paragraph",
      "id": "657190e4d356669b",
      "text": "Before 2018, one of NLP’s main pretraining tools was something like a dictionary. Known as word embeddings, this dictionary encoded associations between words as numbers in a way that deep neural networks could accept as input — akin to giving the person inside a Chinese room a crude vocabulary book to work with. But a neural network pretrained with word embeddings is still blind to the meaning of words at the sentence level. “It would think that ‘a man bit the dog’ and ‘a dog bit the man’ are exactly the same thing,” said Tal Linzen, a computational linguist at Johns Hopkins University."
    },
    {
      "type": "paragraph",
      "id": "a44a547673be6d55",
      "text": "A better method would use pretraining to equip the network with richer rulebooks — not just for vocabulary, but for syntax and context as well — before training it to perform a specific NLP task. In early 2018, researchers at OpenAI, the University of San Francisco, the Allen Institute for Artificial Intelligence and the University of Washington simultaneously discovered a clever way to approximate this feat. Instead of pretraining just the first layer of a network with word embeddings, the researchers began training entire neural networks on a broader basic task called [[language modeling]]."
    },
    {
      "type": "paragraph",
      "id": "952a77d9c03b0629",
      "text": "⇒ [[Language Modeling]]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Pretraining",
        "story": []
      },
      "date": 1650972381555
    },
    {
      "item": {
        "type": "factory",
        "id": "dd0a79655b26f0d9"
      },
      "id": "dd0a79655b26f0d9",
      "type": "add",
      "date": 1650972413708
    },
    {
      "type": "edit",
      "id": "dd0a79655b26f0d9",
      "item": {
        "type": "paragraph",
        "id": "dd0a79655b26f0d9",
        "text": "The phrase “colorless green ideas sleep furiously” has perfect syntax, but any natural speaker knows it’s nonsense. What prewritten rulebook could capture this “unwritten” fact about natural language — or innumerable others?\n\nNLP researchers have tried to square this circle by having neural networks write their own makeshift rulebooks, in a process called pretraining."
      },
      "date": 1650972416939
    },
    {
      "type": "edit",
      "id": "dd0a79655b26f0d9",
      "item": {
        "type": "paragraph",
        "id": "dd0a79655b26f0d9",
        "text": "The phrase “colorless green ideas sleep furiously” has perfect syntax, but any natural speaker knows it’s nonsense. What prewritten rulebook could capture this “unwritten” fact about natural language — or innumerable others?\n\nNLP researchers have tried to square this circle by having neural networks write their own makeshift rulebooks, in a process called pretraining. [https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017/ post]"
      },
      "date": 1650972444125
    },
    {
      "item": {
        "type": "factory",
        "id": "657190e4d356669b"
      },
      "id": "657190e4d356669b",
      "type": "add",
      "after": "dd0a79655b26f0d9",
      "date": 1650972486559
    },
    {
      "type": "edit",
      "id": "657190e4d356669b",
      "item": {
        "type": "paragraph",
        "id": "657190e4d356669b",
        "text": "Before 2018, one of NLP’s main pretraining tools was something like a dictionary. Known as word embeddings, this dictionary encoded associations between words as numbers in a way that deep neural networks could accept as input — akin to giving the person inside a Chinese room a crude vocabulary book to work with. But a neural network pretrained with word embeddings is still blind to the meaning of words at the sentence level. “It would think that ‘a man bit the dog’ and ‘a dog bit the man’ are exactly the same thing,” said Tal Linzen, a computational linguist at Johns Hopkins University."
      },
      "date": 1650972487897
    },
    {
      "item": {
        "type": "factory",
        "id": "a44a547673be6d55"
      },
      "id": "a44a547673be6d55",
      "type": "add",
      "after": "657190e4d356669b",
      "date": 1650972516792
    },
    {
      "type": "edit",
      "id": "a44a547673be6d55",
      "item": {
        "type": "paragraph",
        "id": "a44a547673be6d55",
        "text": "A better method would use pretraining to equip the network with richer rulebooks — not just for vocabulary, but for syntax and context as well — before training it to perform a specific NLP task. In early 2018, researchers at OpenAI, the University of San Francisco, the Allen Institute for Artificial Intelligence and the University of Washington simultaneously discovered a clever way to approximate this feat. Instead of pretraining just the first layer of a network with word embeddings, the researchers began training entire neural networks on a broader basic task called [[language modeling]]."
      },
      "date": 1650972529003
    },
    {
      "item": {
        "type": "factory",
        "id": "952a77d9c03b0629"
      },
      "id": "952a77d9c03b0629",
      "type": "add",
      "after": "a44a547673be6d55",
      "date": 1650972620101
    },
    {
      "type": "edit",
      "id": "952a77d9c03b0629",
      "item": {
        "type": "paragraph",
        "id": "952a77d9c03b0629",
        "text": "Language Modeling"
      },
      "date": 1650972623052
    },
    {
      "type": "edit",
      "id": "952a77d9c03b0629",
      "item": {
        "type": "paragraph",
        "id": "952a77d9c03b0629",
        "text": "⇒ [[Language Modeling]]"
      },
      "date": 1650972633411
    }
  ]
}