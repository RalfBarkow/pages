{
  "title": "Properties of Parallel Distributed Models",
  "story": [
    {
      "type": "paragraph",
      "id": "6c17e7f6eca01d2f",
      "text": "[[Parallel Distributed Models]] have several appealing qualities. They are based on four basic properties of distributed [[Representation]]s: "
    },
    {
      "type": "markdown",
      "id": "adf9a979b8c5640c",
      "text": "1. The representations are continuously valued; \n2. Similar concepts have similar representations; \n3. Several different pieces of knowledge are [[superimpose]]d on the same finite hardware; and \n4. The representations are [[holographic]], that is, any part of the representation can be used to reconstruct the whole. "
    },
    {
      "type": "paragraph",
      "id": "a18f6e3016326fae",
      "text": "From the first two properties it follows that the [[Representation]]s can [[reflect]] the [[Meaning]]s of the [[Concept]]s for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­gory memberships become a matter of [[Degree]]. "
    },
    {
      "type": "paragraph",
      "id": "06e3f0d0ed455d54",
      "text": "The third property results in spontaneous [[Generalization]]. Knowl­edge about an item is automatically generalized to all other items, to the degree that they are similar to that item. As long as the input situ­ations consist of familiar elements, the system can perform reasonably well in new situations. "
    },
    {
      "type": "paragraph",
      "id": "928afe11a1d52520",
      "text": "For example, when a new fact is learned about John, it is coded into the network by changing the connection weights. When the pattern for John is input, the output of the network now shows the new fact. If the weight changes are small and distributed over the entire set of weights, the output for lobster remains largely unchanged. This is because the input pattern for lobster is very different from the pattern for John. The individual changes made in encoding the new fact about John have mostly a random effect on lobster, canceling out on the average. "
    },
    {
      "type": "paragraph",
      "id": "9127f7de28752ec2",
      "text": "However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ized to Mary, to the degree that the representation for Mary is similar to that for John. "
    },
    {
      "type": "paragraph",
      "id": "1951b6576285a7fd",
      "text": "The third property also results in graceful degradation when the sys­tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to [[can­cel]] out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. "
    },
    {
      "type": "paragraph",
      "id": "0d670a660d5a8238",
      "text": "The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Properties of Parallel Distributed Models",
        "story": []
      },
      "date": 1673877608460
    },
    {
      "item": {
        "type": "factory",
        "id": "6c17e7f6eca01d2f"
      },
      "id": "6c17e7f6eca01d2f",
      "type": "add",
      "date": 1673877620939
    },
    {
      "type": "edit",
      "id": "6c17e7f6eca01d2f",
      "item": {
        "type": "paragraph",
        "id": "6c17e7f6eca01d2f",
        "text": "Parallel distributed models have several appealing qualities. They are based on four basic properties of distributed representations: 1. The representations are continuously valued; 2. Similar concepts have similar representations; 3. Several different pieces of knowledge are superimposed on the same finite hardware; and 4. The representations are holographic, that is, any part of the representation can be used to reconstruct the whole. From the first two properties it follows that the representations can reflect the meanings of the concepts for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­ gory memberships become a matter of degree. The third property results in spontaneous generalization. Knowl­ edge about an item is automatically generalized to all other items, to the degree that they are similar to that item. As long as the input situ­ ations consist of familiar elements, the system can perform reasonably well in new situations. For example, when a new fact is learned about John, it is coded into the network by changing the connection weights. When the pattern for John is input, the output of the network now shows the new fact. If the weight changes are small and distributed over the entire set of weights, the output for lobster remains largely unchanged. This is because the input pattern for lobster is very different from the pattern for John. The individual changes made in encoding the new fact about John have mostly a random effect on lobster, canceling out on the average. However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­ put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ ized to Mary, to the degree that the representation for Mary is similar to that for John. The third property also results in graceful degradation when the sys­ tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "date": 1673877628071
    },
    {
      "type": "edit",
      "id": "6c17e7f6eca01d2f",
      "item": {
        "type": "paragraph",
        "id": "6c17e7f6eca01d2f",
        "text": "Parallel distributed models have several appealing qualities. They are based on four basic properties of distributed representations: "
      },
      "date": 1673877639733
    },
    {
      "type": "add",
      "id": "adf9a979b8c5640c",
      "item": {
        "type": "paragraph",
        "id": "adf9a979b8c5640c",
        "text": "1. The representations are continuously valued; 2. Similar concepts have similar representations; 3. Several different pieces of knowledge are superimposed on the same finite hardware; and 4. The representations are holographic, that is, any part of the representation can be used to reconstruct the whole. "
      },
      "after": "6c17e7f6eca01d2f",
      "date": 1673877649771
    },
    {
      "type": "edit",
      "id": "adf9a979b8c5640c",
      "item": {
        "type": "markdown",
        "id": "adf9a979b8c5640c",
        "text": "1. The representations are continuously valued; 2. Similar concepts have similar representations; 3. Several different pieces of knowledge are superimposed on the same finite hardware; and 4. The representations are holographic, that is, any part of the representation can be used to reconstruct the whole. "
      },
      "date": 1673877651594
    },
    {
      "type": "add",
      "id": "a18f6e3016326fae",
      "item": {
        "type": "paragraph",
        "id": "a18f6e3016326fae",
        "text": "From the first two properties it follows that the representations can reflect the meanings of the concepts for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­ gory memberships become a matter of degree. The third property results in spontaneous generalization. Knowl­ edge about an item is automatically generalized to all other items, to the degree that they are similar to that item. As long as the input situ­ ations consist of familiar elements, the system can perform reasonably well in new situations. For example, when a new fact is learned about John, it is coded into the network by changing the connection weights. When the pattern for John is input, the output of the network now shows the new fact. If the weight changes are small and distributed over the entire set of weights, the output for lobster remains largely unchanged. This is because the input pattern for lobster is very different from the pattern for John. The individual changes made in encoding the new fact about John have mostly a random effect on lobster, canceling out on the average. However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­ put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ ized to Mary, to the degree that the representation for Mary is similar to that for John. The third property also results in graceful degradation when the sys­ tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "after": "adf9a979b8c5640c",
      "date": 1673877652674
    },
    {
      "type": "edit",
      "id": "adf9a979b8c5640c",
      "item": {
        "type": "markdown",
        "id": "adf9a979b8c5640c",
        "text": "1. The representations are continuously valued; \n2. Similar concepts have similar representations; \n3. Several different pieces of knowledge are superimposed on the same finite hardware; and \n4. The representations are holographic, that is, any part of the representation can be used to reconstruct the whole. "
      },
      "date": 1673877660012
    },
    {
      "type": "edit",
      "id": "adf9a979b8c5640c",
      "item": {
        "type": "markdown",
        "id": "adf9a979b8c5640c",
        "text": "1. The representations are continuously valued; \n2. Similar concepts have similar representations; \n3. Several different pieces of knowledge are [[superimpose]]d on the same finite hardware; and \n4. The representations are holographic, that is, any part of the representation can be used to reconstruct the whole. "
      },
      "date": 1673877699650
    },
    {
      "type": "edit",
      "id": "a18f6e3016326fae",
      "item": {
        "type": "paragraph",
        "id": "a18f6e3016326fae",
        "text": "From the first two properties it follows that the representations can reflect the meanings of the concepts for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­ gory memberships become a matter of degree. "
      },
      "date": 1673877733571
    },
    {
      "type": "add",
      "id": "06e3f0d0ed455d54",
      "item": {
        "type": "paragraph",
        "id": "06e3f0d0ed455d54",
        "text": "The third property results in spontaneous generalization. Knowl­ edge about an item is automatically generalized to all other items, to the degree that they are similar to that item. As long as the input situ­ ations consist of familiar elements, the system can perform reasonably well in new situations. For example, when a new fact is learned about John, it is coded into the network by changing the connection weights. When the pattern for John is input, the output of the network now shows the new fact. If the weight changes are small and distributed over the entire set of weights, the output for lobster remains largely unchanged. This is because the input pattern for lobster is very different from the pattern for John. The individual changes made in encoding the new fact about John have mostly a random effect on lobster, canceling out on the average. However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­ put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ ized to Mary, to the degree that the representation for Mary is similar to that for John. The third property also results in graceful degradation when the sys­ tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "after": "a18f6e3016326fae",
      "date": 1673877741050
    },
    {
      "type": "edit",
      "id": "a18f6e3016326fae",
      "item": {
        "type": "paragraph",
        "id": "a18f6e3016326fae",
        "text": "From the first two properties it follows that the representations can reflect the meanings of the concepts for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­gory memberships become a matter of degree. "
      },
      "date": 1673877747089
    },
    {
      "type": "edit",
      "id": "06e3f0d0ed455d54",
      "item": {
        "type": "paragraph",
        "id": "06e3f0d0ed455d54",
        "text": "The third property results in spontaneous [[Generalization]]. Knowl­edge about an item is automatically generalized to all other items, to the degree that they are similar to that item. As long as the input situ­ ations consist of familiar elements, the system can perform reasonably well in new situations. For example, when a new fact is learned about John, it is coded into the network by changing the connection weights. When the pattern for John is input, the output of the network now shows the new fact. If the weight changes are small and distributed over the entire set of weights, the output for lobster remains largely unchanged. This is because the input pattern for lobster is very different from the pattern for John. The individual changes made in encoding the new fact about John have mostly a random effect on lobster, canceling out on the average. However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­ put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ ized to Mary, to the degree that the representation for Mary is similar to that for John. The third property also results in graceful degradation when the sys­ tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "date": 1673877777383
    },
    {
      "type": "edit",
      "id": "06e3f0d0ed455d54",
      "item": {
        "type": "paragraph",
        "id": "06e3f0d0ed455d54",
        "text": "The third property results in spontaneous [[Generalization]]. Knowl­edge about an item is automatically generalized to all other items, to the degree that they are similar to that item. As long as the input situ­ations consist of familiar elements, the system can perform reasonably well in new situations. "
      },
      "date": 1673877811111
    },
    {
      "type": "add",
      "id": "928afe11a1d52520",
      "item": {
        "type": "paragraph",
        "id": "928afe11a1d52520",
        "text": "For example, when a new fact is learned about John, it is coded into the network by changing the connection weights. When the pattern for John is input, the output of the network now shows the new fact. If the weight changes are small and distributed over the entire set of weights, the output for lobster remains largely unchanged. This is because the input pattern for lobster is very different from the pattern for John. The individual changes made in encoding the new fact about John have mostly a random effect on lobster, canceling out on the average. However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­ put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ ized to Mary, to the degree that the representation for Mary is similar to that for John. The third property also results in graceful degradation when the sys­ tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "after": "06e3f0d0ed455d54",
      "date": 1673877811350
    },
    {
      "type": "edit",
      "id": "928afe11a1d52520",
      "item": {
        "type": "paragraph",
        "id": "928afe11a1d52520",
        "text": "For example, when a new fact is learned about John, it is coded into the network by changing the connection weights. When the pattern for John is input, the output of the network now shows the new fact. If the weight changes are small and distributed over the entire set of weights, the output for lobster remains largely unchanged. This is because the input pattern for lobster is very different from the pattern for John. The individual changes made in encoding the new fact about John have mostly a random effect on lobster, canceling out on the average. "
      },
      "date": 1673877847603
    },
    {
      "type": "add",
      "id": "9127f7de28752ec2",
      "item": {
        "type": "paragraph",
        "id": "9127f7de28752ec2",
        "text": "However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­ put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ ized to Mary, to the degree that the representation for Mary is similar to that for John. The third property also results in graceful degradation when the sys­ tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "after": "928afe11a1d52520",
      "date": 1673877848785
    },
    {
      "type": "edit",
      "id": "9127f7de28752ec2",
      "item": {
        "type": "paragraph",
        "id": "9127f7de28752ec2",
        "text": "However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ ized to Mary, to the degree that the representation for Mary is similar to that for John. The third property also results in graceful degradation when the sys­ tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "date": 1673877861412
    },
    {
      "type": "edit",
      "id": "9127f7de28752ec2",
      "item": {
        "type": "paragraph",
        "id": "9127f7de28752ec2",
        "text": "However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ized to Mary, to the degree that the representation for Mary is similar to that for John. The third property also results in graceful degradation when the sys­ tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "date": 1673877876780
    },
    {
      "type": "edit",
      "id": "9127f7de28752ec2",
      "item": {
        "type": "paragraph",
        "id": "9127f7de28752ec2",
        "text": "However, the pattern for Mary is very similar to John. When Mary is input to the network, the weight changes correlate with the in­put pattern very well, and the output for Mary now also shows the new fact. In other words, the new fact about John is automatically general­ized to Mary, to the degree that the representation for Mary is similar to that for John. "
      },
      "date": 1673877900892
    },
    {
      "type": "add",
      "id": "1951b6576285a7fd",
      "item": {
        "type": "paragraph",
        "id": "1951b6576285a7fd",
        "text": "The third property also results in graceful degradation when the sys­tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to can­ cel out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "after": "9127f7de28752ec2",
      "date": 1673877907355
    },
    {
      "type": "edit",
      "id": "1951b6576285a7fd",
      "item": {
        "type": "paragraph",
        "id": "1951b6576285a7fd",
        "text": "The third property also results in graceful degradation when the sys­tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to [[can­cel]] out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "date": 1673877939784
    },
    {
      "type": "edit",
      "id": "1951b6576285a7fd",
      "item": {
        "type": "paragraph",
        "id": "1951b6576285a7fd",
        "text": "The third property also results in graceful degradation when the sys­tem is overloaded with information. When several representations are superimposed in the same hardware, individual variations tend to [[can­cel]] out, and central tendencies are enhanced. There is no fixed limit on how much can be stored, but the representations become less and less accurate. "
      },
      "date": 1673877956726
    },
    {
      "type": "add",
      "id": "0d670a660d5a8238",
      "item": {
        "type": "paragraph",
        "id": "0d670a660d5a8238",
        "text": "The holographic property (4) makes the system robust against noise, damage, and incomplete information. Because the same information is represented in several places, the processing is effectively based on […]"
      },
      "after": "1951b6576285a7fd",
      "date": 1673877957807
    },
    {
      "type": "edit",
      "id": "6c17e7f6eca01d2f",
      "item": {
        "type": "paragraph",
        "id": "6c17e7f6eca01d2f",
        "text": "[[Parallel Distributed Models]] have several appealing qualities. They are based on four basic properties of distributed representations: "
      },
      "date": 1673878127045
    },
    {
      "type": "edit",
      "id": "adf9a979b8c5640c",
      "item": {
        "type": "markdown",
        "id": "adf9a979b8c5640c",
        "text": "1. The representations are continuously valued; \n2. Similar concepts have similar representations; \n3. Several different pieces of knowledge are [[superimpose]]d on the same finite hardware; and \n4. The representations are [[holographic]], that is, any part of the representation can be used to reconstruct the whole. "
      },
      "date": 1673878201204
    },
    {
      "type": "edit",
      "id": "a18f6e3016326fae",
      "item": {
        "type": "paragraph",
        "id": "a18f6e3016326fae",
        "text": "From the first two properties it follows that the representations can [[reflect]] the meanings of the concepts for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­gory memberships become a matter of degree. "
      },
      "date": 1673878259740
    },
    {
      "type": "edit",
      "id": "a18f6e3016326fae",
      "item": {
        "type": "paragraph",
        "id": "a18f6e3016326fae",
        "text": "From the first two properties it follows that the representations can [[reflect]] the [[Meaning]]s of the concepts for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­gory memberships become a matter of degree. "
      },
      "date": 1673878394468
    },
    {
      "type": "edit",
      "id": "a18f6e3016326fae",
      "item": {
        "type": "paragraph",
        "id": "a18f6e3016326fae",
        "text": "From the first two properties it follows that the representations can [[reflect]] the [[Meaning]]s of the [[Concept]]s for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­gory memberships become a matter of degree. "
      },
      "date": 1673878434016
    },
    {
      "type": "edit",
      "id": "a18f6e3016326fae",
      "item": {
        "type": "paragraph",
        "id": "a18f6e3016326fae",
        "text": "From the first two properties it follows that the [[Representation]]s can [[reflect]] the [[Meaning]]s of the [[Concept]]s for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­gory memberships become a matter of degree. "
      },
      "date": 1673878774652
    },
    {
      "type": "edit",
      "id": "6c17e7f6eca01d2f",
      "item": {
        "type": "paragraph",
        "id": "6c17e7f6eca01d2f",
        "text": "[[Parallel Distributed Models]] have several appealing qualities. They are based on four basic properties of distributed [[Representations]]: "
      },
      "date": 1673879715398
    },
    {
      "type": "edit",
      "id": "6c17e7f6eca01d2f",
      "item": {
        "type": "paragraph",
        "id": "6c17e7f6eca01d2f",
        "text": "[[Parallel Distributed Models]] have several appealing qualities. They are based on four basic properties of distributed [[Representation]]s: "
      },
      "date": 1673879725401
    },
    {
      "type": "edit",
      "id": "a18f6e3016326fae",
      "item": {
        "type": "paragraph",
        "id": "a18f6e3016326fae",
        "text": "From the first two properties it follows that the [[Representation]]s can [[reflect]] the [[Meaning]]s of the [[Concept]]s for which they stand. Because they are continuous, it is possible to represent shades of meaning, and cate­gory memberships become a matter of [[Degree]]. "
      },
      "date": 1673879807930
    }
  ]
}