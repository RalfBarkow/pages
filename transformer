{
  "title": "Transformer",
  "story": [
    {
      "type": "paragraph",
      "id": "f7122878eb49effb",
      "text": "Used most notably in the Transformer, [[Attention]] has helped [[Deep Learning]] to arguably approach human level performance in various tasks with larger models continuing to boost performance [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, the heuristic motivations that produced Attention leave open the question of why it performs so well [1, 10]. Insights into why Attention is so effective would not only make it more interpretable but also guide future improvements."
    },
    {
      "type": "paragraph",
      "id": "be1dc1ca3576704c",
      "text": "Much has been done to try and explain Attention’s success, including work showing that Transformer representations map more closely to human brain recordings and inductive biases than other models [11, 12]. Our work takes another step in this direction by showing the potential relationship between Attention and biologically plausible neural processing at the level of neuronal wiring, providing a novel mechanistic perspective behind the Attention operation. This potential relationship is created by showing mathematically that Attention closely approximates [[Sparse Distributed Memory]] (SDM)."
    },
    {
      "type": "pagefold",
      "id": "e3b8c5721d355464",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "b2753b9b544b26e4",
      "text": "\nBRICKEN, Trenton and PEHLEVAN, Cengiz, 2021. Attention Approximates Sparse Distributed Memory. In: Advances in Neural Information Processing Systems. Online. Curran Associates, Inc. 2021. p. 15301–15315. [Accessed 16 January 2023]. Available from: https://proceedings.neurips.cc/paper/2021/hash/8171ac2c5544a5cb54ac0f38bf477af4-Abstract.html\n"
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Transformer",
        "story": []
      },
      "date": 1673854414963
    },
    {
      "item": {
        "type": "factory",
        "id": "f7122878eb49effb"
      },
      "id": "f7122878eb49effb",
      "type": "add",
      "date": 1673854416582
    },
    {
      "type": "edit",
      "id": "f7122878eb49effb",
      "item": {
        "type": "paragraph",
        "id": "f7122878eb49effb",
        "text": "Used most notably in the Transformer, Attention has helped deep learning to arguably approach human level performance in various tasks with larger models continuing to boost performance [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, the heuristic motivations that produced Attention leave open the question of why it performs so well [1, 10]. Insights into why Attention is so effective would not only make it more interpretable but also guide future improvements."
      },
      "date": 1673854418500
    },
    {
      "type": "edit",
      "id": "f7122878eb49effb",
      "item": {
        "type": "paragraph",
        "id": "f7122878eb49effb",
        "text": "Used most notably in the Transformer, [[Attention]] has helped deep learning to arguably approach human level performance in various tasks with larger models continuing to boost performance [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, the heuristic motivations that produced Attention leave open the question of why it performs so well [1, 10]. Insights into why Attention is so effective would not only make it more interpretable but also guide future improvements."
      },
      "date": 1673854465547
    },
    {
      "type": "edit",
      "id": "f7122878eb49effb",
      "item": {
        "type": "paragraph",
        "id": "f7122878eb49effb",
        "text": "Used most notably in the Transformer, [[Attention]] has helped [[Deep Learning]] to arguably approach human level performance in various tasks with larger models continuing to boost performance [1, 2, 3, 4, 5, 6, 7, 8, 9]. However, the heuristic motivations that produced Attention leave open the question of why it performs so well [1, 10]. Insights into why Attention is so effective would not only make it more interpretable but also guide future improvements."
      },
      "date": 1673854480743
    },
    {
      "item": {
        "type": "factory",
        "id": "be1dc1ca3576704c"
      },
      "id": "be1dc1ca3576704c",
      "type": "add",
      "after": "f7122878eb49effb",
      "date": 1673854553213
    },
    {
      "type": "edit",
      "id": "be1dc1ca3576704c",
      "item": {
        "type": "paragraph",
        "id": "be1dc1ca3576704c",
        "text": "Much has been done to try and explain Attention’s success, including work showing that Transformer representations map more closely to human brain recordings and inductive biases than other models [11, 12]. Our work takes another step in this direction by showing the potential relationship between Attention and biologically plausible neural processing at the level of neuronal wiring, providing a novel mechanistic perspective behind the Attention operation. This potential relationship is created by showing mathematically that Attention closely approximates Sparse Distributed Memory (SDM)."
      },
      "date": 1673854554891
    },
    {
      "item": {
        "type": "factory",
        "id": "b2753b9b544b26e4"
      },
      "id": "b2753b9b544b26e4",
      "type": "add",
      "after": "be1dc1ca3576704c",
      "date": 1673854592412
    },
    {
      "type": "edit",
      "id": "b2753b9b544b26e4",
      "item": {
        "type": "paragraph",
        "id": "b2753b9b544b26e4",
        "text": "\nBRICKEN, Trenton and PEHLEVAN, Cengiz, 2021. Attention Approximates Sparse Distributed Memory. In: Advances in Neural Information Processing Systems. Online. Curran Associates, Inc. 2021. p. 15301–15315. [Accessed 16 January 2023]. Available from: https://proceedings.neurips.cc/paper/2021/hash/8171ac2c5544a5cb54ac0f38bf477af4-Abstract.html\n"
      },
      "date": 1673854605842
    },
    {
      "item": {
        "type": "factory",
        "id": "e3b8c5721d355464"
      },
      "id": "e3b8c5721d355464",
      "type": "add",
      "after": "b2753b9b544b26e4",
      "date": 1673854607456
    },
    {
      "id": "e3b8c5721d355464",
      "type": "move",
      "order": [
        "f7122878eb49effb",
        "be1dc1ca3576704c",
        "e3b8c5721d355464",
        "b2753b9b544b26e4"
      ],
      "date": 1673854609219
    },
    {
      "type": "edit",
      "id": "e3b8c5721d355464",
      "item": {
        "type": "pagefold",
        "id": "e3b8c5721d355464",
        "text": "~"
      },
      "date": 1673854612501
    },
    {
      "type": "edit",
      "id": "be1dc1ca3576704c",
      "item": {
        "type": "paragraph",
        "id": "be1dc1ca3576704c",
        "text": "Much has been done to try and explain Attention’s success, including work showing that Transformer representations map more closely to human brain recordings and inductive biases than other models [11, 12]. Our work takes another step in this direction by showing the potential relationship between Attention and biologically plausible neural processing at the level of neuronal wiring, providing a novel mechanistic perspective behind the Attention operation. This potential relationship is created by showing mathematically that Attention closely approximates [[Sparse Distributed Memory]] (SDM)."
      },
      "date": 1673854629301
    }
  ]
}