{
  "title": "Sentence Transformer",
  "story": [
    {
      "type": "paragraph",
      "id": "85f9b0539a4180e5",
      "text": "There are a lot of sentence transformers to choose from! [https://www.leebutterman.com/2023/06/01/offline-realtime-embedding-search.html page] There is a leaderboard of sentence embeddings: MTEB: Massive Text Embedding Benchmark [https://huggingface.co/blog/mteb page], [https://github.com/embeddings-benchmark/mteb#leaderboard github]"
    },
    {
      "type": "paragraph",
      "id": "0f207099359fba4b",
      "text": "Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual [[Word Alignment]] induction. However, these methods usually start from mBERT or XLM-R."
    },
    {
      "type": "paragraph",
      "id": "6e64cfc5e0bb954b",
      "text": "In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process.arXiv:2301.12140 [cs]"
    },
    {
      "type": "pagefold",
      "id": "834b24ff61a6cc84",
      "text": "~"
    },
    {
      "type": "paragraph",
      "id": "de976e43bfb1a72a",
      "text": "WANG, Weikang, CHEN, Guanhua, WANG, Hanqing, HAN, Yue and CHEN, Yun, 2023. Multilingual Sentence Transformer as A Multilingual Word Aligner. Online. 28 January 2023. arXiv. arXiv:2301.12140. [Accessed 5 September 2023]."
    }
  ],
  "journal": [
    {
      "type": "create",
      "item": {
        "title": "Sentence Transformer",
        "story": []
      },
      "date": 1693908395605
    },
    {
      "item": {
        "type": "factory",
        "id": "de976e43bfb1a72a"
      },
      "id": "de976e43bfb1a72a",
      "type": "add",
      "date": 1693908487752
    },
    {
      "type": "edit",
      "id": "de976e43bfb1a72a",
      "item": {
        "type": "paragraph",
        "id": "de976e43bfb1a72a",
        "text": "\nWANG, Weikang, CHEN, Guanhua, WANG, Hanqing, HAN, Yue and CHEN, Yun, 2023. Multilingual Sentence Transformer as A Multilingual Word Aligner. Online. 28 January 2023. arXiv. arXiv:2301.12140. [Accessed 5 September 2023]. Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual word alignment induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process.arXiv:2301.12140 [cs]\n"
      },
      "date": 1693908489185
    },
    {
      "type": "edit",
      "id": "de976e43bfb1a72a",
      "item": {
        "type": "paragraph",
        "id": "de976e43bfb1a72a",
        "text": "WANG, Weikang, CHEN, Guanhua, WANG, Hanqing, HAN, Yue and CHEN, Yun, 2023. Multilingual Sentence Transformer as A Multilingual Word Aligner. Online. 28 January 2023. arXiv. arXiv:2301.12140. [Accessed 5 September 2023]. Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual word alignment induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process.arXiv:2301.12140 [cs]\n"
      },
      "date": 1693908606315
    },
    {
      "type": "edit",
      "id": "de976e43bfb1a72a",
      "item": {
        "type": "paragraph",
        "id": "de976e43bfb1a72a",
        "text": "WANG, Weikang, CHEN, Guanhua, WANG, Hanqing, HAN, Yue and CHEN, Yun, 2023. Multilingual Sentence Transformer as A Multilingual Word Aligner. Online. 28 January 2023. arXiv. arXiv:2301.12140. [Accessed 5 September 2023]. Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual [[Word Alignment]] induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process.arXiv:2301.12140 [cs]\n"
      },
      "date": 1693908625082
    },
    {
      "type": "edit",
      "id": "de976e43bfb1a72a",
      "item": {
        "type": "paragraph",
        "id": "de976e43bfb1a72a",
        "text": "WANG, Weikang, CHEN, Guanhua, WANG, Hanqing, HAN, Yue and CHEN, Yun, 2023. Multilingual Sentence Transformer as A Multilingual Word Aligner. Online. 28 January 2023. arXiv. arXiv:2301.12140. [Accessed 5 September 2023]."
      },
      "date": 1693908650528
    },
    {
      "type": "add",
      "id": "0f207099359fba4b",
      "item": {
        "type": "paragraph",
        "id": "0f207099359fba4b",
        "text": "Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual [[Word Alignment]] induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process.arXiv:2301.12140 [cs]"
      },
      "after": "de976e43bfb1a72a",
      "date": 1693908651439
    },
    {
      "item": {
        "type": "factory",
        "id": "834b24ff61a6cc84"
      },
      "id": "834b24ff61a6cc84",
      "type": "add",
      "after": "0f207099359fba4b",
      "date": 1693908653338
    },
    {
      "type": "edit",
      "id": "834b24ff61a6cc84",
      "item": {
        "type": "pagefold",
        "id": "834b24ff61a6cc84",
        "text": "~"
      },
      "date": 1693908656063
    },
    {
      "id": "de976e43bfb1a72a",
      "type": "move",
      "order": [
        "0f207099359fba4b",
        "834b24ff61a6cc84",
        "de976e43bfb1a72a"
      ],
      "date": 1693908674523
    },
    {
      "type": "edit",
      "id": "0f207099359fba4b",
      "item": {
        "type": "paragraph",
        "id": "0f207099359fba4b",
        "text": "Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual [[Word Alignment]] induction. However, these methods usually start from mBERT or XLM-R."
      },
      "date": 1693908690324
    },
    {
      "type": "add",
      "id": "6e64cfc5e0bb954b",
      "item": {
        "type": "paragraph",
        "id": "6e64cfc5e0bb954b",
        "text": "In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process.arXiv:2301.12140 [cs]"
      },
      "after": "0f207099359fba4b",
      "date": 1693908690791
    },
    {
      "item": {
        "type": "factory",
        "id": "85f9b0539a4180e5"
      },
      "id": "85f9b0539a4180e5",
      "type": "add",
      "after": "de976e43bfb1a72a",
      "date": 1693909263077
    },
    {
      "id": "85f9b0539a4180e5",
      "type": "move",
      "order": [
        "0f207099359fba4b",
        "85f9b0539a4180e5",
        "6e64cfc5e0bb954b",
        "834b24ff61a6cc84",
        "de976e43bfb1a72a"
      ],
      "date": 1693909266646
    },
    {
      "id": "85f9b0539a4180e5",
      "type": "move",
      "order": [
        "85f9b0539a4180e5",
        "0f207099359fba4b",
        "6e64cfc5e0bb954b",
        "834b24ff61a6cc84",
        "de976e43bfb1a72a"
      ],
      "date": 1693909269458
    },
    {
      "type": "edit",
      "id": "85f9b0539a4180e5",
      "item": {
        "type": "paragraph",
        "id": "85f9b0539a4180e5",
        "text": "There are a lot of sentence transformers to choose from! There is a leaderboard of sentence embeddings: https://huggingface.co/blog/mteb"
      },
      "date": 1693909270753
    },
    {
      "type": "edit",
      "id": "85f9b0539a4180e5",
      "item": {
        "type": "paragraph",
        "id": "85f9b0539a4180e5",
        "text": "There are a lot of sentence transformers to choose from! There is a leaderboard of sentence embeddings: MTEB: Massive Text Embedding Benchmark [https://huggingface.co/blog/mteb page]"
      },
      "date": 1693909304036
    },
    {
      "type": "edit",
      "id": "85f9b0539a4180e5",
      "item": {
        "type": "paragraph",
        "id": "85f9b0539a4180e5",
        "text": "There are a lot of sentence transformers to choose from! There is a leaderboard of sentence embeddings: MTEB: Massive Text Embedding Benchmark [https://huggingface.co/blog/mteb page], [https://github.com/embeddings-benchmark/mteb#leaderboard github]"
      },
      "date": 1693909354421
    },
    {
      "type": "edit",
      "id": "85f9b0539a4180e5",
      "item": {
        "type": "paragraph",
        "id": "85f9b0539a4180e5",
        "text": "There are a lot of sentence transformers to choose from! [https://www.leebutterman.com/2023/06/01/offline-realtime-embedding-search.html page ]There is a leaderboard of sentence embeddings: MTEB: Massive Text Embedding Benchmark [https://huggingface.co/blog/mteb page], [https://github.com/embeddings-benchmark/mteb#leaderboard github]"
      },
      "date": 1693909388587
    },
    {
      "type": "edit",
      "id": "85f9b0539a4180e5",
      "item": {
        "type": "paragraph",
        "id": "85f9b0539a4180e5",
        "text": "There are a lot of sentence transformers to choose from! [https://www.leebutterman.com/2023/06/01/offline-realtime-embedding-search.html page] There is a leaderboard of sentence embeddings: MTEB: Massive Text Embedding Benchmark [https://huggingface.co/blog/mteb page], [https://github.com/embeddings-benchmark/mteb#leaderboard github]"
      },
      "date": 1693909397226
    }
  ]
}